"""
éšæœºæœç´¢ç®—æ³•
é€šè¿‡éšæœºé‡‡æ ·æœç´¢ç©ºé—´æ¥å¯»æ‰¾æœ€ä¼˜ Prompt ç»„åˆ
"""
import time
import random
from config.models import SearchSpace, SearchResult
from metrics import MetricsCalculator


class RandomSearchAlgorithm:
    """éšæœºæœç´¢ç®—æ³•"""
    
    def __init__(self, llm):
        """
        åˆå§‹åŒ–ç®—æ³•
        
        Args:
            llm: LangChain LLM å®ä¾‹
        """
        self.llm = llm
    
    def run(
        self, 
        task_description: str,
        task_type: str,
        test_dataset: list[dict],
        search_space: SearchSpace,
        iterations: int = 5,
        progress_callback=None
    ) -> tuple[list[SearchResult], SearchResult]:
        """
        æ‰§è¡Œéšæœºæœç´¢ä¼˜åŒ–
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            task_type: ä»»åŠ¡ç±»å‹ (classification/summarization/translation)
            test_dataset: æµ‹è¯•æ•°æ®é›† [{'input': '...', 'ground_truth': '...'}]
            search_space: æœç´¢ç©ºé—´
            iterations: æœç´¢è¿­ä»£æ¬¡æ•°
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° callback(current, total, message)
            
        Returns:
            (æ‰€æœ‰ç»“æœåˆ—è¡¨, æœ€ä½³ç»“æœ)
        """
        results_log = []
        calc = MetricsCalculator()
        
        print(f"\n{'='*60}")
        print(f"å¼€å§‹éšæœºæœç´¢ä¼˜åŒ– - {iterations} æ¬¡è¿­ä»£")
        print(f"{'='*60}\n")
        
        for i in range(iterations):
            # 1. éšæœºé‡‡æ ·ï¼šæ‘‡éª°å­ç»„åˆ Prompt
            chosen_role = random.choice(search_space.roles)
            chosen_style = random.choice(search_space.styles)
            chosen_tech = random.choice(search_space.techniques)
            
            print(f"è¿­ä»£ {i+1}/{iterations}")
            print(f"  è§’è‰²: {chosen_role}")
            print(f"  é£æ ¼: {chosen_style}")
            print(f"  æŠ€å·§: {chosen_tech}")
            
            # 2. æ‹¼è£…å€™é€‰ Prompt
            candidate_prompt = self._build_prompt(
                task_type, task_description, chosen_role, chosen_style, chosen_tech
            )
            
            # 3. åœ¨æµ‹è¯•é›†ä¸Šè·‘åˆ†
            scores = []
            for case_idx, case in enumerate(test_dataset):
                try:
                    print(f"\n  ğŸ“ æµ‹è¯•æ ·æœ¬ {case_idx+1}/{len(test_dataset)}")
                    print(f"    è¾“å…¥: {case['input'][:50]}..." if len(case['input']) > 50 else f"    è¾“å…¥: {case['input']}")
                    print(f"    æ ‡å‡†ç­”æ¡ˆ: {case['ground_truth']}")
                    
                    # æ›¿æ¢å ä½ç¬¦
                    prompt_filled = self._fill_prompt(candidate_prompt, case['input'], task_type)
                    
                    # è°ƒç”¨ LLM
                    print(f"    ğŸ¤– è°ƒç”¨ LLM...")
                    response = self.llm.invoke(prompt_filled)
                    time.sleep(0.3)  # API è°ƒç”¨å»¶è¿Ÿ
                    prediction = response.content.strip()
                    print(f"    ğŸ’¬ LLM è¾“å‡º: {prediction[:80]}..." if len(prediction) > 80 else f"    ğŸ’¬ LLM è¾“å‡º: {prediction}")
                    
                    # è®¡ç®—åˆ†æ•°
                    score = self._calculate_score(prediction, case['ground_truth'], task_type, calc)
                    scores.append(score)
                    print(f"    âœ… å¾—åˆ†: {score:.1f}")
                    
                except Exception as e:
                    print(f"    âŒ è¯„ä¼°å¤±è´¥ï¼")
                    print(f"    é”™è¯¯ç±»å‹: {type(e).__name__}")
                    print(f"    é”™è¯¯ä¿¡æ¯: {e}")
                    scores.append(0.0)
            
            # è®¡ç®—å¹³å‡åˆ†
            avg_score = sum(scores) / len(scores) if scores else 0.0
            print(f"  å¹³å‡å¾—åˆ†: {avg_score:.2f}\n")
            
            # 4. è®°å½•ç»“æœ
            result = SearchResult(
                iteration_id=i+1,
                role=chosen_role,
                style=chosen_style,
                technique=chosen_tech,
                full_prompt=candidate_prompt,
                avg_score=avg_score,
                task_type=task_type
            )
            results_log.append(result)
            
            # è°ƒç”¨è¿›åº¦å›è°ƒ
            if progress_callback:
                progress_callback(i+1, iterations, f"å®Œæˆè¿­ä»£ {i+1}/{iterations}ï¼Œå¾—åˆ†: {avg_score:.2f}")
        
        # æ‰¾å‡ºæœ€ä½³ç»“æœ
        best_result = max(results_log, key=lambda x: x.avg_score)
        
        print(f"{'='*60}")
        print(f"æœç´¢å®Œæˆï¼æœ€ä½³å¾—åˆ†: {best_result.avg_score:.2f}")
        print(f"æœ€ä½³ç»„åˆ: {best_result.role} + {best_result.style} + {best_result.technique}")
        print(f"{'='*60}\n")
        
        return results_log, best_result
    
    def _build_prompt(self, task_type: str, task_description: str, 
                     role: str, style: str, technique: str) -> str:
        """æ„å»ºå€™é€‰ Prompt"""
        if task_type == "classification":
            return f"""ä½ æ˜¯ä¸€ä½{role}ã€‚

ä»»åŠ¡ï¼š{task_description}

é£æ ¼è¦æ±‚ï¼š{style}

æŒ‡ä»¤ï¼š{technique}

è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼š
[å¾…åˆ†ç±»æ–‡æœ¬]

åªè¾“å‡ºåˆ†ç±»æ ‡ç­¾ï¼Œä¸è¦é¢å¤–è§£é‡Šã€‚
"""
        elif task_type == "summarization":
            return f"""ä½ æ˜¯ä¸€ä½{role}ã€‚

ä»»åŠ¡ï¼š{task_description}

é£æ ¼è¦æ±‚ï¼š{style}

æŒ‡ä»¤ï¼š{technique}

è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œæ‘˜è¦ï¼š
[å¾…æ‘˜è¦æ–‡æœ¬]

è¯·æŒ‰ç…§è¦æ±‚è¾“å‡ºæ‘˜è¦ã€‚
"""
        elif task_type == "translation":
            return f"""ä½ æ˜¯ä¸€ä½{role}ã€‚

ä»»åŠ¡ï¼š{task_description}

é£æ ¼è¦æ±‚ï¼š{style}

æŒ‡ä»¤ï¼š{technique}

è¯·ç¿»è¯‘ä»¥ä¸‹æ–‡æœ¬ï¼š
[å¾…ç¿»è¯‘æ–‡æœ¬]

åªè¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦é¢å¤–è¯´æ˜ã€‚
"""
        else:
            return f"""è§’è‰²: {role}
é£æ ¼: {style}
ä»»åŠ¡: {task_description}
æŒ‡ä»¤: {technique}

è¾“å…¥: {{input}}
"""
    
    def _fill_prompt(self, prompt: str, input_text: str, task_type: str) -> str:
        """å¡«å…… Prompt ä¸­çš„å ä½ç¬¦"""
        if task_type == "classification":
            return prompt.replace("[å¾…åˆ†ç±»æ–‡æœ¬]", input_text)
        elif task_type == "summarization":
            return prompt.replace("[å¾…æ‘˜è¦æ–‡æœ¬]", input_text)
        elif task_type == "translation":
            return prompt.replace("[å¾…ç¿»è¯‘æ–‡æœ¬]", input_text)
        else:
            return prompt.replace("{{input}}", input_text)
    
    def _calculate_score(self, prediction: str, ground_truth: str, 
                        task_type: str, calc: MetricsCalculator) -> float:
        """è®¡ç®—é¢„æµ‹ç»“æœçš„åˆ†æ•°"""
        if task_type == "classification":
            # åˆ†ç±»ä»»åŠ¡ï¼šç®€å•åŒ¹é…
            score = 100.0 if prediction == ground_truth else 0.0
            print(f"    ğŸ“Š åŒ¹é…ç»“æœ: {'âœ… æ­£ç¡®' if score == 100.0 else 'âŒ é”™è¯¯'}")
            return score
        elif task_type == "summarization":
            # æ‘˜è¦ä»»åŠ¡ï¼šROUGE
            print(f"    ğŸ“Š è®¡ç®— ROUGE åˆ†æ•°...")
            rouge_scores = calc.calculate_rouge(prediction, ground_truth)
            score = rouge_scores['rouge1']
            print(f"    ğŸ“Š ROUGE-1: {score:.2f}")
            return score
        elif task_type == "translation":
            # ç¿»è¯‘ä»»åŠ¡ï¼šBLEU
            print(f"    ğŸ“Š è®¡ç®— BLEU åˆ†æ•°...")
            score = calc.calculate_bleu(prediction, ground_truth)
            print(f"    ğŸ“Š BLEU: {score:.2f}")
            return score
        else:
            return 50.0  # é»˜è®¤åˆ†æ•°
