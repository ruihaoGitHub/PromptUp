"""
éšæœºæœç´¢ç®—æ³•
é€šè¿‡éšæœºé‡‡æ ·æœç´¢ç©ºé—´æ¥å¯»æ‰¾æœ€ä¼˜ Prompt ç»„åˆ
"""
import time
import random
from config.models import SearchSpace, SearchResult
from metrics import MetricsCalculator


class RandomSearchAlgorithm:
    """éšæœºæœç´¢ç®—æ³•"""
    
    def __init__(self, llm):
        """
        åˆå§‹åŒ–ç®—æ³•
        
        Args:
            llm: LangChain LLM å®ä¾‹
        """
        self.llm = llm
    
    def run(
        self, 
        task_description: str,
        task_type: str,
        test_dataset: list[dict],
        search_space: SearchSpace,
        iterations: int = 5,
        progress_callback=None,
        labels: list[str] = None
    ) -> tuple[list[SearchResult], SearchResult]:
        """
        æ‰§è¡Œéšæœºæœç´¢ä¼˜åŒ–
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            task_type: ä»»åŠ¡ç±»å‹ (classification/summarization/translation)
            test_dataset: æµ‹è¯•æ•°æ®é›† [{'input': '...', 'ground_truth': '...'}]
            search_space: æœç´¢ç©ºé—´
            iterations: æœç´¢è¿­ä»£æ¬¡æ•°
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° callback(current, total, message)
            labels: åˆ†ç±»ä»»åŠ¡çš„æ ‡ç­¾åˆ—è¡¨ï¼ˆä»…åˆ†ç±»ä»»åŠ¡éœ€è¦ï¼‰
            
        Returns:
            (æ‰€æœ‰ç»“æœåˆ—è¡¨, æœ€ä½³ç»“æœ)
        """
        results_log = []
        calc = MetricsCalculator()

        # é¢„ç”Ÿæˆæ‰€æœ‰ç»„åˆï¼Œç¡®ä¿ä¸é‡å¤
        all_combinations = [
            (role, style, tech)
            for role in search_space.roles
            for style in search_space.styles
            for tech in search_space.techniques
        ]
        total_combinations = len(all_combinations)
        if total_combinations == 0:
            raise ValueError("æœç´¢ç©ºé—´ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œéšæœºæœç´¢ã€‚")

        if iterations > total_combinations:
            print(f"âš ï¸ è¿­ä»£æ¬¡æ•° {iterations} è¶…è¿‡æœç´¢ç©ºé—´ç»„åˆæ•° {total_combinations}ï¼Œå°†è‡ªåŠ¨è°ƒæ•´ä¸º {total_combinations} æ¬¡ä»¥é¿å…é‡å¤ã€‚")
            iterations = total_combinations

        random.shuffle(all_combinations)
        
        print(f"\n{'='*60}")
        print(f"å¼€å§‹éšæœºæœç´¢ä¼˜åŒ– - {iterations} æ¬¡è¿­ä»£")
        print(f"{'='*60}\n")
        
        for i in range(iterations):
            # 1. éšæœºé‡‡æ ·ï¼šæ— é‡å¤ç»„åˆ
            chosen_role, chosen_style, chosen_tech = all_combinations[i]
            
            print(f"è¿­ä»£ {i+1}/{iterations}")
            print(f"  è§’è‰²: {chosen_role}")
            print(f"  é£æ ¼: {chosen_style}")
            print(f"  æŠ€å·§: {chosen_tech}")
            
            # 2. æ‹¼è£…å€™é€‰ Prompt
            candidate_prompt = self._build_prompt(
                task_type, task_description, chosen_role, chosen_style, chosen_tech, labels
            )
            
            # 3. åœ¨æµ‹è¯•é›†ä¸Šè·‘åˆ†
            scores = []
            for case_idx, case in enumerate(test_dataset):
                try:
                    print(f"\n  ğŸ“ æµ‹è¯•æ ·æœ¬ {case_idx+1}/{len(test_dataset)}")
                    print(f"    è¾“å…¥: {case['input'][:50]}..." if len(case['input']) > 50 else f"    è¾“å…¥: {case['input']}")
                    print(f"    æ ‡å‡†ç­”æ¡ˆ: {case['ground_truth']}")
                    
                    # æ›¿æ¢å ä½ç¬¦
                    prompt_filled = self._fill_prompt(candidate_prompt, case['input'], task_type)
                    
                    # è°ƒç”¨ LLM
                    print(f"    ğŸ¤– è°ƒç”¨ LLM...")
                    response = self.llm.invoke(prompt_filled)
                    time.sleep(0.3)  # API è°ƒç”¨å»¶è¿Ÿ
                    prediction = response.content.strip()
                    print(f"    ğŸ’¬ LLM è¾“å‡º: {prediction[:80]}..." if len(prediction) > 80 else f"    ğŸ’¬ LLM è¾“å‡º: {prediction}")
                    
                    # è®¡ç®—åˆ†æ•°
                    score = self._calculate_score(prediction, case['ground_truth'], task_type, calc)
                    scores.append(score)
                    print(f"    âœ… å¾—åˆ†: {score:.1f}")
                    
                except Exception as e:
                    print(f"    âŒ è¯„ä¼°å¤±è´¥ï¼")
                    print(f"    é”™è¯¯ç±»å‹: {type(e).__name__}")
                    print(f"    é”™è¯¯ä¿¡æ¯: {e}")
                    scores.append(0.0)
            
            # è®¡ç®—å¹³å‡åˆ†
            avg_score = sum(scores) / len(scores) if scores else 0.0
            print(f"  å¹³å‡å¾—åˆ†: {avg_score:.2f}\n")
            
            # 4. è®°å½•ç»“æœ
            result = SearchResult(
                iteration_id=i+1,
                role=chosen_role,
                style=chosen_style,
                technique=chosen_tech,
                full_prompt=candidate_prompt,
                avg_score=avg_score,
                task_type=task_type
            )
            results_log.append(result)
            
            # è°ƒç”¨è¿›åº¦å›è°ƒ
            if progress_callback:
                progress_callback(i+1, iterations, f"å®Œæˆè¿­ä»£ {i+1}/{iterations}ï¼Œå¾—åˆ†: {avg_score:.2f}")
        
        # æ‰¾å‡ºæœ€ä½³ç»“æœ
        best_result = max(results_log, key=lambda x: x.avg_score)
        
        print(f"{'='*60}")
        print(f"æœç´¢å®Œæˆï¼æœ€ä½³å¾—åˆ†: {best_result.avg_score:.2f}")
        print(f"æœ€ä½³ç»„åˆ: {best_result.role} + {best_result.style} + {best_result.technique}")
        print(f"{'='*60}\n")
        
        return results_log, best_result
    
    def _build_prompt(self, task_type: str, task_description: str, 
                     role: str, style: str, technique: str, labels: list[str] = None) -> str:
        """æ„å»ºå€™é€‰ Prompt"""
        if task_type == "classification":
            # åŠ¨æ€ç”Ÿæˆæ ‡ç­¾åˆ—è¡¨
            if labels:
                labels_str = ", ".join(labels)
                output_instruction = f"åªè¾“å‡ºä»¥ä¸‹æ ‡ç­¾ä¹‹ä¸€ï¼š{labels_str}ã€‚ä¸è¦é¢å¤–è§£é‡Šã€‚"
            else:
                output_instruction = "åªè¾“å‡ºåˆ†ç±»æ ‡ç­¾ï¼Œä¸è¦é¢å¤–è§£é‡Šã€‚"
                
            return f"""ä½ æ˜¯ä¸€ä½{role}ã€‚

ä»»åŠ¡ï¼š{task_description}

é£æ ¼è¦æ±‚ï¼š{style}

æŒ‡ä»¤ï¼š{technique}

è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼š
[å¾…åˆ†ç±»æ–‡æœ¬]

{output_instruction}
"""
        elif task_type == "summarization":
            return f"""ä½ æ˜¯ä¸€ä½{role}ã€‚

ä»»åŠ¡ï¼š{task_description}

é£æ ¼è¦æ±‚ï¼š{style}

æŒ‡ä»¤ï¼š{technique}

è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œæ‘˜è¦ï¼š
[å¾…æ‘˜è¦æ–‡æœ¬]

è¯·æŒ‰ç…§è¦æ±‚è¾“å‡ºæ‘˜è¦ã€‚
"""
        elif task_type == "translation":
            return f"""ä½ æ˜¯ä¸€ä½{role}ã€‚

ä»»åŠ¡ï¼š{task_description}

é£æ ¼è¦æ±‚ï¼š{style}

æŒ‡ä»¤ï¼š{technique}

è¯·ç¿»è¯‘ä»¥ä¸‹æ–‡æœ¬ï¼š
[å¾…ç¿»è¯‘æ–‡æœ¬]

åªè¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦é¢å¤–è¯´æ˜ã€‚
"""
        else:
            return f"""è§’è‰²: {role}
é£æ ¼: {style}
ä»»åŠ¡: {task_description}
æŒ‡ä»¤: {technique}

è¾“å…¥: {{input}}
"""
    
    def _fill_prompt(self, prompt: str, input_text: str, task_type: str) -> str:
        """å¡«å…… Prompt ä¸­çš„å ä½ç¬¦"""
        if task_type == "classification":
            return prompt.replace("[å¾…åˆ†ç±»æ–‡æœ¬]", input_text)
        elif task_type == "summarization":
            return prompt.replace("[å¾…æ‘˜è¦æ–‡æœ¬]", input_text)
        elif task_type == "translation":
            return prompt.replace("[å¾…ç¿»è¯‘æ–‡æœ¬]", input_text)
        else:
            return prompt.replace("{{input}}", input_text)
    
    def _calculate_score(self, prediction: str, ground_truth: str, 
                        task_type: str, calc: MetricsCalculator) -> float:
        """è®¡ç®—é¢„æµ‹ç»“æœçš„åˆ†æ•°"""
        if task_type == "classification":
            # åˆ†ç±»ä»»åŠ¡ï¼šç®€å•åŒ¹é…
            score = 100.0 if prediction == ground_truth else 0.0
            print(f"    ğŸ“Š åŒ¹é…ç»“æœ: {'âœ… æ­£ç¡®' if score == 100.0 else 'âŒ é”™è¯¯'}")
            return score
        elif task_type == "summarization":
            # æ‘˜è¦ä»»åŠ¡ï¼šROUGE
            print(f"    ğŸ“Š è®¡ç®— ROUGE åˆ†æ•°...")
            rouge_scores = calc.calculate_rouge(prediction, ground_truth)
            score = rouge_scores['rouge1']
            print(f"    ğŸ“Š ROUGE-1: {score:.2f}")
            return score
        elif task_type == "translation":
            # ç¿»è¯‘ä»»åŠ¡ï¼šBLEU
            print(f"    ğŸ“Š è®¡ç®— BLEU åˆ†æ•°...")
            score = calc.calculate_bleu(prediction, ground_truth)
            print(f"    ğŸ“Š BLEU: {score:.2f}")
            return score
        else:
            return 50.0  # é»˜è®¤åˆ†æ•°
