"""
è´å¶æ–¯ä¼˜åŒ–æ¨¡å—
ä½¿ç”¨æ¦‚ç‡æ¨¡å‹æ™ºèƒ½ä¼˜åŒ– Prompt ç»„åˆ
"""
import time
from typing import Optional, Callable
from config.models import SearchSpace, SearchResult
from metrics import MetricsCalculator

try:
    import optuna
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False


class BayesianOptimization:
    """è´å¶æ–¯ä¼˜åŒ–å™¨"""
    
    def __init__(self, llm):
        """
        åˆå§‹åŒ–è´å¶æ–¯ä¼˜åŒ–
        
        Args:
            llm: LLM å®ä¾‹
        """
        self.llm = llm
    
    def run(
        self,
        task_description: str,
        task_type: str,
        test_dataset: list,
        search_space: SearchSpace,
        n_trials: int = 20,
        progress_callback: Optional[Callable] = None
    ) -> tuple[list, SearchResult, list]:
        """
        è´å¶æ–¯ä¼˜åŒ– Prompt
        
        æ ¸å¿ƒæ€æƒ³ï¼šåˆ©ç”¨æ¦‚ç‡æ¨¡å‹ï¼ˆTPEï¼‰æ™ºèƒ½é€‰æ‹©ä¸‹ä¸€ä¸ªå°è¯•çš„å‚æ•°ç»„åˆï¼Œ
        ç”¨æœ€å°‘çš„å°è¯•æ¬¡æ•°æ‰¾åˆ°æœ€ä¼˜è§£ï¼Œæ¯”éšæœºæœç´¢æ•ˆç‡é«˜å¾—å¤šã€‚
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            task_type: ä»»åŠ¡ç±»å‹ (classification/summarization/translation)
            test_dataset: æµ‹è¯•æ•°æ®é›† [{"input": "...", "ground_truth": "..."}, ...]
            search_space: æœç´¢ç©ºé—´
            n_trials: å°è¯•æ¬¡æ•°ï¼ˆè´å¶æ–¯ä¼˜åŒ–é€šå¸¸20-50æ¬¡å°±èƒ½æ‰¾åˆ°å¥½ç»“æœï¼‰
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° callback(trial, total_trials, best_score)
        
        Returns:
            (all_results, best_result, trial_history)
            - all_results: æ‰€æœ‰è¯•éªŒçš„ç»“æœ
            - best_result: æœ€ä½³ç»“æœ
            - trial_history: è¯•éªŒå†å² [{"trial": 1, "score": 85.0, "params": {...}}, ...]
        """
        if not OPTUNA_AVAILABLE:
            raise ImportError("è´å¶æ–¯ä¼˜åŒ–éœ€è¦ optuna åº“ã€‚è¯·è¿è¡Œ: pip install optuna")
        
        print(f"\n{'='*60}")
        print(f"ğŸ§ è´å¶æ–¯ä¼˜åŒ–å¼€å§‹")
        print(f"{'='*60}")
        print(f"ğŸ“‹ ä»»åŠ¡ç±»å‹: {task_type}")
        print(f"ğŸ”¬ ä½¿ç”¨ç®—æ³•: TPE (Tree-structured Parzen Estimator)")
        print(f"ğŸ¯ å°è¯•æ¬¡æ•°: {n_trials}")
        print(f"ğŸ“ æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_dataset)}")
        print(f"ğŸ’° é¢„è®¡ API è°ƒç”¨: {n_trials * len(test_dataset)} æ¬¡")
        print(f"ğŸ’¡ è´å¶æ–¯ä¼˜åŒ–ä¼šæ ¹æ®å†å²ç»“æœæ™ºèƒ½é€‰æ‹©ä¸‹ä¸€ä¸ªå‚æ•°ç»„åˆ")
        print(f"{'='*60}\n")
        
        all_results = []
        trial_history = []
        best_score_so_far = 0.0
        
        def objective(trial):
            """Optuna çš„ç›®æ ‡å‡½æ•°"""
            nonlocal best_score_so_far
            
            # è®© Optuna å»ºè®®å‚æ•°
            role = trial.suggest_categorical('role', search_space.roles)
            style = trial.suggest_categorical('style', search_space.styles)
            technique = trial.suggest_categorical('technique', search_space.techniques)
            
            print(f"\n{'='*60}")
            print(f"ğŸ” è¯•éªŒ {trial.number + 1}/{n_trials}")
            print(f"{'='*60}")
            
            # æ˜¾ç¤ºç­–ç•¥æç¤º
            if trial.number < 5:
                print(f"  ğŸ“ ç­–ç•¥: éšæœºæ¢ç´¢ï¼ˆå»ºç«‹åˆå§‹æ¨¡å‹ï¼‰")
            else:
                # è®¡ç®—ä¸æœ€ä½³ç»“æœçš„ç›¸ä¼¼åº¦ï¼ˆç®€å•å¯å‘å¼ï¼‰
                best_trials = sorted(trial_history, key=lambda x: x['score'], reverse=True)[:3]
                if best_trials and any(
                    (role == t['role'] or style == t['style'] or technique == t['technique'])
                    for t in best_trials
                ):
                    print(f"  ğŸ“ ç­–ç•¥: å¼€å‘é«˜åˆ†åŒºåŸŸï¼ˆåŸºäºå†å²æœ€ä½³ï¼‰")
                else:
                    print(f"  ğŸ“ ç­–ç•¥: æ¢ç´¢æ–°åŒºåŸŸï¼ˆé¿å…å±€éƒ¨æœ€ä¼˜ï¼‰")
            
            print(f"  å‚æ•°ç»„åˆ: {role} + {style} + {technique}")
            
            # æ„å»º Prompt
            if task_type == "classification":
                prompt_template = f"""ä½ æ˜¯ä¸€ä½{role}ã€‚

è¯·ä»¥{style}çš„é£æ ¼å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
{task_description}

ç­–ç•¥æç¤ºï¼š{technique}

**é‡è¦ï¼šä½ å¿…é¡»åªè¾“å‡ºåˆ†ç±»æ ‡ç­¾ï¼ˆå¦‚ï¼šç§¯æã€æ¶ˆæã€ä¸­ç«‹ï¼‰ï¼Œä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šã€åˆ†ææˆ–å…¶ä»–å†…å®¹ã€‚**

è¾“å…¥ï¼š{{{{text}}}}
è¾“å‡ºï¼ˆåªè¾“å‡ºæ ‡ç­¾ï¼‰ï¼š"""
            else:
                prompt_template = f"""ä½ æ˜¯ä¸€ä½{role}ã€‚

è¯·ä»¥{style}çš„é£æ ¼å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
{task_description}

ç­–ç•¥æç¤ºï¼š{technique}

è¾“å…¥ï¼š{{{{text}}}}
"""
            
            # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
            predictions = []
            ground_truths = []
            
            for idx, sample in enumerate(test_dataset, 1):
                test_input = sample.get("input", "")
                ground_truth = sample.get("ground_truth", "")
                
                final_prompt = prompt_template.replace("{{text}}", test_input)
                
                # æ˜¾ç¤ºå½“å‰è¿›åº¦
                print(f"    ğŸ“ è¯„ä¼°æ ·æœ¬ {idx}/{len(test_dataset)}...", end="", flush=True)
                
                # è°ƒç”¨ LLMï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
                prediction = ""
                max_retries = 3
                retry_delay = 2.0
                
                for retry in range(max_retries):
                    try:
                        response = self.llm.invoke(final_prompt)
                        time.sleep(1.2)  # å¢åŠ å»¶è¿Ÿåˆ° 1.2s
                        prediction = response.content.strip()
                        print(" âœ“")  # æˆåŠŸæ ‡è®°
                        break
                        
                    except Exception as e:
                        error_msg = str(e)
                        if "429" in error_msg or "Too Many Requests" in error_msg:
                            if retry < max_retries - 1:
                                wait_time = retry_delay * (2 ** retry)
                                print(f" âš ï¸ é™æµï¼Œç­‰å¾… {wait_time:.0f}s...")
                                time.sleep(wait_time)
                                continue
                            else:
                                print(" âœ— (è¾¾åˆ°é‡è¯•ä¸Šé™)")
                                prediction = ""
                                break
                        else:
                            print(f" âœ— ({str(e)[:30]})")
                            prediction = ""
                            break
                
                # æ¸…ç†é¢„æµ‹ç»“æœ
                if prediction and task_type == "classification":
                    prediction = prediction.split('\n')[0].strip()
                    for prefix in ["è¾“å‡ºï¼š", "è¾“å‡º:", "ç»“æœï¼š", "ç»“æœ:", "åˆ†ç±»ï¼š", "åˆ†ç±»:", "æ ‡ç­¾ï¼š", "æ ‡ç­¾:"]:
                        if prediction.startswith(prefix):
                            prediction = prediction[len(prefix):].strip()
                    if len(prediction) > 10:
                        for label in ["ç§¯æ", "æ¶ˆæ", "ä¸­ç«‹", "æ­£é¢", "è´Ÿé¢", "ä¸­æ€§"]:
                            if label in prediction:
                                prediction = label
                                break
                
                predictions.append(prediction)
                ground_truths.append(ground_truth)
            
            # è®¡ç®—åˆ†æ•°
            calc = MetricsCalculator()
            valid_pairs = [(p, g) for p, g in zip(predictions, ground_truths) if p]
            
            if not valid_pairs:
                score = 0.0
            else:
                valid_predictions = [p for p, g in valid_pairs]
                valid_ground_truths = [g for p, g in valid_pairs]
                
                if task_type == "classification":
                    score = calc.calculate_accuracy(valid_predictions, valid_ground_truths)
                elif task_type == "summarization":
                    scores = [calc.calculate_rouge(p, g)['rougeL'] for p, g in valid_pairs]
                    score = sum(scores) / len(scores) if scores else 0
                elif task_type == "translation":
                    scores = [calc.calculate_bleu(p, g) for p, g in valid_pairs]
                    score = sum(scores) / len(scores) if scores else 0
                else:
                    score = 0.0
            
            # è®°å½•ç»“æœ
            result = SearchResult(
                iteration_id=trial.number + 1,
                role=role,
                style=style,
                technique=technique,
                full_prompt=prompt_template,
                avg_score=score,
                task_type=task_type
            )
            all_results.append(result)
            
            # æ›´æ–°æœ€ä½³åˆ†æ•°
            if score > best_score_so_far:
                best_score_so_far = score
                print(f"  â†’ å¾—åˆ†: {score:.2f} ğŸ‰ æ–°çºªå½•ï¼")
            else:
                print(f"  â†’ å¾—åˆ†: {score:.2f}")
            
            # è®°å½•è¯•éªŒå†å²
            trial_history.append({
                "trial": trial.number + 1,
                "score": score,
                "best_score": best_score_so_far,
                "role": role,
                "style": style,
                "technique": technique
            })
            
            # è¿›åº¦å›è°ƒ
            if progress_callback:
                progress_callback(trial.number + 1, n_trials, best_score_so_far)
            
            # è¯•éªŒé—´å¢åŠ å»¶è¿Ÿï¼Œé¿å…è¿ç»­è°ƒç”¨è§¦å‘é™æµ
            if trial.number < n_trials - 1:  # ä¸æ˜¯æœ€åä¸€æ¬¡
                print(f"  â¸ï¸  è¯•éªŒé—´å†·å´ 2ç§’...")
                time.sleep(2.0)
            
            return score
        
        # åˆ›å»º Optuna Studyï¼ˆä¼˜åŒ– TPE å‚æ•°ï¼‰
        optuna.logging.set_verbosity(optuna.logging.WARNING)
        study = optuna.create_study(
            direction="maximize",
            sampler=optuna.samplers.TPESampler(
                n_startup_trials=min(5, n_trials // 3),  # å‰5æ¬¡éšæœºæ¢ç´¢
                n_ei_candidates=24,  # é»˜è®¤24ï¼Œå¢åŠ å€™é€‰æ•°é‡
                multivariate=True,  # è€ƒè™‘å‚æ•°é—´çš„ç›¸å…³æ€§
                warn_independent_sampling=False,
                seed=None  # ç§»é™¤å›ºå®šç§å­ï¼Œå¢åŠ éšæœºæ€§
            )
        )
        
        # æ‰§è¡Œä¼˜åŒ–
        study.optimize(objective, n_trials=n_trials, show_progress_bar=False)
        
        # è·å–æœ€ä½³ç»“æœ
        best_trial = study.best_trial
        best_result = next(r for r in all_results if 
                          r.role == best_trial.params['role'] and
                          r.style == best_trial.params['style'] and
                          r.technique == best_trial.params['technique'])
        
        print(f"\n{'='*60}")
        print(f"ğŸ† è´å¶æ–¯ä¼˜åŒ–å®Œæˆï¼")
        print(f"{'='*60}")
        print(f"ğŸ¥‡ æœ€ä½³å¾—åˆ†: {best_result.avg_score:.2f}")
        print(f"ğŸ§¬ æœ€ä½³ç»„åˆ: {best_result.role} + {best_result.style} + {best_result.technique}")
        print(f"ğŸ“Š æ”¶æ•›é€Ÿåº¦: åœ¨ç¬¬ {best_trial.number + 1} æ¬¡è¯•éªŒä¸­æ‰¾åˆ°æœ€ä½³ç»“æœ")
        
        # åˆ†æä¼˜åŒ–æ•ˆæœ
        scores = [h['score'] for h in trial_history]
        first_5_avg = sum(scores[:5]) / 5 if len(scores) >= 5 else sum(scores) / len(scores)
        last_5_avg = sum(scores[-5:]) / 5 if len(scores) >= 5 else sum(scores) / len(scores)
        
        print(f"\nğŸ“ˆ ä¼˜åŒ–åˆ†æ:")
        print(f"  å‰5æ¬¡å¹³å‡: {first_5_avg:.2f}")
        print(f"  å5æ¬¡å¹³å‡: {last_5_avg:.2f}")
        if last_5_avg >= first_5_avg:
            print(f"  âœ… åæœŸè¡¨ç°æå‡ {last_5_avg - first_5_avg:.2f} åˆ†ï¼ˆæ™ºèƒ½ä¼˜åŒ–ç”Ÿæ•ˆï¼‰")
        else:
            print(f"  âš ï¸ åæœŸæ¢ç´¢å…¶ä»–åŒºåŸŸï¼ˆé˜²æ­¢é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼‰")
        
        print(f"{'='*60}\n")
        
        return all_results, best_result, trial_history
