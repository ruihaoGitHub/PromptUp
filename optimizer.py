"""
Prompt ä¼˜åŒ–æ ¸å¿ƒæ¨¡å—
å®ç°è‡ªåŠ¨åŒ–çš„ Prompt ç”Ÿæˆã€ä¼˜åŒ–å’Œè¯„ä¼°
"""
import os
from typing import Optional, Literal
from langchain_openai import ChatOpenAI
from langchain_nvidia_ai_endpoints import ChatNVIDIA
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
import json
from templates import get_strategy_by_scene, OPTIMIZATION_PRINCIPLES


class OptimizedPrompt(BaseModel):
    """ä¼˜åŒ–åçš„ Prompt ç»“æ„ï¼ˆç”Ÿæˆä»»åŠ¡ï¼‰"""
    thinking_process: str = Field(description="ä¼˜åŒ–æ—¶çš„æ€è€ƒè¿‡ç¨‹ï¼Œåˆ†æåŸå§‹ Prompt çš„é—®é¢˜å’Œæ”¹è¿›æ–¹å‘")
    improved_prompt: str = Field(description="ä¼˜åŒ–åçš„å®Œæ•´ Promptï¼Œå¯ç›´æ¥ä½¿ç”¨")
    enhancement_techniques: list[str] = Field(description="ä½¿ç”¨çš„ä¼˜åŒ–æŠ€æœ¯ï¼Œå¦‚ï¼šå¢åŠ è§’è‰²è®¾å®šã€æ˜ç¡®è¾“å‡ºæ ¼å¼ç­‰")
    keywords_added: list[str] = Field(description="æ–°å¢çš„å…³é”®è¯å’Œä¸“ä¸šæœ¯è¯­")
    structure_applied: str = Field(description="åº”ç”¨çš„ Prompt æ¡†æ¶åç§°ï¼Œå¦‚ CO-STARã€BROKE ç­‰")


class ClassificationPrompt(BaseModel):
    """ä¼˜åŒ–åçš„åˆ†ç±»ä»»åŠ¡ Prompt ç»“æ„"""
    thinking_process: str = Field(description="ä¼˜åŒ–åˆ†æè¿‡ç¨‹")
    role_definition: str = Field(description="è§’è‰²è®¾å®šï¼Œä¾‹å¦‚ï¼šä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„æƒ…æ„Ÿåˆ†æä¸“å®¶")
    label_definitions: dict[str, str] = Field(description="æ ‡ç­¾è¯¦ç»†å®šä¹‰å­—å…¸ï¼ŒKeyæ˜¯æ ‡ç­¾åï¼ŒValueæ˜¯è¯¦ç»†åˆ¤æ–­æ ‡å‡†")
    few_shot_examples: list[dict[str, str]] = Field(description="è‡ªåŠ¨åˆæˆçš„3-5ä¸ªé«˜è´¨é‡å°‘æ ·æœ¬ç¤ºä¾‹")
    reasoning_guidance: str = Field(description="æ€ç»´é“¾å¼•å¯¼è¯­ï¼Œå¸®åŠ©æ¨¡å‹é€æ­¥åˆ†æ")
    output_format: str = Field(description="ä¸¥æ ¼çš„è¾“å‡ºæ ¼å¼è¦æ±‚")
    final_prompt: str = Field(description="ç»„åˆå¥½çš„æœ€ç»ˆå¯ç”¨çš„å®Œæ•´ Prompt")

class SummarizationPrompt(BaseModel):
    """ä¼˜åŒ–åçš„æ‘˜è¦ä»»åŠ¡ Prompt ç»“æ„"""
    thinking_process: str = Field(description="ä¼˜åŒ–åˆ†æè¿‡ç¨‹")
    role_setting: str = Field(description="è§’è‰²è®¾å®šï¼Œå¦‚ï¼šä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æŠ€æœ¯æ–‡æ¡£ç¼–å†™ä¸“å®¶")
    extraction_rules: list[str] = Field(description="å…·ä½“çš„æå–è§„åˆ™ï¼Œå¦‚ï¼šå¿…é¡»ä¿ç•™æ‰€æœ‰æ•°å­—ã€æ—¥æœŸå’Œè´£ä»»äºº")
    negative_constraints: list[str] = Field(description="è´Ÿé¢çº¦æŸï¼Œæ˜ç¡®å‘Šè¯‰æ¨¡å‹ä¸è¦åšä»€ä¹ˆ")
    format_template: str = Field(description="ä¸¥æ ¼çš„è¾“å‡ºæ ¼å¼æ¨¡æ¿ï¼Œé€šå¸¸åŒ…å«Markdownç»“æ„")
    step_by_step_guide: str = Field(description="ç»™æ¨¡å‹çš„æ€è€ƒæ­¥éª¤ï¼Œå¦‚ï¼šé€šè¯»å…¨æ–‡ -> æ ‡è®°é‡ç‚¹ -> æ’°å†™åˆç¨¿")
    focus_areas: list[str] = Field(description="æ ¸å¿ƒå…³æ³¨ç‚¹ï¼Œé’ˆå¯¹ç”¨æˆ·éœ€æ±‚å¼ºè°ƒçš„ä¿¡æ¯")
    final_prompt: str = Field(description="ç»„åˆå¥½çš„æœ€ç»ˆå¯ç”¨çš„æ‘˜è¦ Promptï¼Œ{{text}}å ä½ç¬¦")


class TranslationPrompt(BaseModel):
    """ä¼˜åŒ–åçš„ç¿»è¯‘ä»»åŠ¡ Prompt ç»“æ„"""
    thinking_process: str = Field(description="ä¼˜åŒ–åˆ†æè¿‡ç¨‹")
    role_definition: str = Field(description="è§’è‰²è®¾å®šï¼Œä¾‹å¦‚ï¼šä½ æ˜¯ç²¾é€šä¸­è‹±åŒè¯­çš„ã€Šè‡ªç„¶ã€‹æ‚å¿—ç¼–è¾‘")
    style_guidelines: list[str] = Field(description="é£æ ¼æŒ‡å—åˆ—è¡¨ï¼Œä¾‹å¦‚ï¼š['ä¿æŒå­¦æœ¯ä¸¥è°¨', 'é¿å…å£è¯­åŒ–', 'ä¿ç•™è¢«åŠ¨è¯­æ€']")
    glossary_section: str = Field(description="æ„å»ºçš„æœ¯è¯­å¯¹ç…§è¡¨éƒ¨åˆ†ï¼Œå¦‚æœæ²¡æœ‰åˆ™ç•™ç©º")
    workflow_steps: str = Field(description="ç¿»è¯‘çš„å·¥ä½œæµæŒ‡ä»¤ï¼Œæ¨èä½¿ç”¨'ç›´è¯‘-åæ€-æ¶¦è‰²'ä¸‰æ­¥æ³•")
    final_prompt: str = Field(description="æœ€ç»ˆç»„åˆå¥½çš„ Prompt æ¨¡æ¿ï¼Œå¾…ç¿»è¯‘æ–‡æœ¬ç”¨ {{text}} å ä½")


class PromptOptimizer:
    """Prompt è‡ªåŠ¨ä¼˜åŒ–å™¨"""
    
    def __init__(self, 
                 api_key: Optional[str] = None, 
                 model: str = "meta/llama-3.1-405b-instruct", 
                 base_url: Optional[str] = None,
                 provider: Literal["openai", "nvidia"] = "nvidia",
                 temperature: float = 0.7,
                 top_p: float = 0.7,
                 max_tokens: int = 2048):
        """
        åˆå§‹åŒ–ä¼˜åŒ–å™¨
        
        Args:
            api_key: API Keyï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            base_url: API base URL
            provider: API æä¾›å•† ("openai" æˆ– "nvidia")
            temperature: æ¸©åº¦å‚æ•°
            top_p: Top-p é‡‡æ ·å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
        """
        self.provider = provider
        self.model = model
        
        # æ ¹æ®æä¾›å•†åˆå§‹åŒ– LLM
        if provider == "nvidia":
            if api_key:
                os.environ["NVIDIA_API_KEY"] = api_key
            
            llm_params = {
                "model": model,
                "temperature": temperature,
                "top_p": top_p,
                "max_tokens": max_tokens
            }
            if base_url:
                llm_params["base_url"] = base_url
            
            self.llm = ChatNVIDIA(**llm_params)
            
        else:  # openai
            if api_key:
                os.environ["OPENAI_API_KEY"] = api_key
            
            llm_params = {
                "model": model,
                "temperature": temperature,
                "max_tokens": max_tokens
            }
            if base_url:
                llm_params["base_url"] = base_url
            
            self.llm = ChatOpenAI(**llm_params)
    
    def optimize(self, 
                 user_prompt: str, 
                 scene_desc: str = "é€šç”¨",
                 optimization_mode: str = "é€šç”¨å¢å¼º (General)") -> OptimizedPrompt:
        """
        æ ¸å¿ƒä¼˜åŒ–å‡½æ•°
        
        Args:
            user_prompt: ç”¨æˆ·è¾“å…¥çš„åŸå§‹ Prompt
            scene_desc: åœºæ™¯æè¿°
            optimization_mode: ä¼˜åŒ–æ¨¡å¼
            
        Returns:
            OptimizedPrompt: ä¼˜åŒ–åçš„ç»“æ„åŒ– Prompt
        """
        # æ‰“å°ä¼˜åŒ–å¼€å§‹ä¿¡æ¯
        print(f"\n{'='*60}")
        print(f"âš™ï¸  å¼€å§‹ Prompt ä¼˜åŒ–")
        print(f"{'='*60}")
        print(f"ğŸ”Œ API æä¾›å•†: {self.provider.upper()}")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {self.model}")
        print(f"ğŸ¯ ä¼˜åŒ–æ¨¡å¼: {optimization_mode}")
        print(f"ğŸ“ åŸå§‹ Prompt: {user_prompt[:50]}{'...' if len(user_prompt) > 50 else ''}")
        if scene_desc:
            print(f"ğŸ¬ åœºæ™¯æè¿°: {scene_desc[:50]}{'...' if len(scene_desc) > 50 else ''}")
        print(f"{'='*60}\n")
        
        # è·å–åœºæ™¯å¯¹åº”çš„ä¼˜åŒ–ç­–ç•¥
        strategy = get_strategy_by_scene(optimization_mode)
        
        # æ„å»º Meta-Prompt
        system_prompt = self._build_meta_prompt(strategy, scene_desc)
        
        # æ„å»ºæ¶ˆæ¯é“¾
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "ç”¨æˆ·åŸå§‹ Promptï¼š{input}\n\nåœºæ™¯è¡¥å……è¯´æ˜ï¼š{scene}")
        ])
        
        # æ‰§è¡Œä¼˜åŒ–
        try:
            print("ğŸ“¤ æ­£åœ¨è°ƒç”¨ API...")
            
            # æ„å»ºå®Œæ•´æç¤º
            messages = prompt_template.format_messages(
                input=user_prompt,
                scene=scene_desc if scene_desc else "æ— ç‰¹æ®Šè¯´æ˜"
            )
            
            print(f"ğŸ’¬ æ¶ˆæ¯é•¿åº¦: {len(str(messages))} å­—ç¬¦")
            
            # è°ƒç”¨ LLM
            if self.provider == "openai":
                # OpenAI æ”¯æŒ JSON mode
                print("ğŸ”§ ä½¿ç”¨ OpenAI JSON mode")
                response = self.llm.invoke(
                    messages,
                    response_format={"type": "json_object"}
                )
            else:
                # NVIDIA ä½¿ç”¨æ™®é€šè°ƒç”¨
                print("ğŸ”§ ä½¿ç”¨ NVIDIA æ ‡å‡†è°ƒç”¨")
                response = self.llm.invoke(messages)
            
            # è§£æç»“æœ
            content = response.content
            print(f"ğŸ“¥ æ”¶åˆ°å“åº”ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
            print(f"ğŸ“„ å“åº”å‰100å­—ç¬¦: {content[:100]}...")
            
            # å°è¯•æå– JSONï¼ˆå¯èƒ½åŒ…å«åœ¨ markdown ä»£ç å—ä¸­ï¼‰
            if "```json" in content:
                print("ğŸ” æ£€æµ‹åˆ° JSON ä»£ç å—ï¼Œæ­£åœ¨æå–...")
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                print("ğŸ” æ£€æµ‹åˆ°ä»£ç å—ï¼Œæ­£åœ¨æå–...")
                content = content.split("```")[1].split("```")[0].strip()
            
            print("âš™ï¸ æ­£åœ¨è§£æ JSON...")
            result_dict = json.loads(content)
            
            print("âœ… JSON è§£ææˆåŠŸ")
            print("ğŸ”¨ æ­£åœ¨éªŒè¯æ•°æ®ç»“æ„...")
            optimized = OptimizedPrompt(**result_dict)
            
            print("âœ… ä¼˜åŒ–å®Œæˆï¼")
            print(f"{'='*60}\n")
            
            return optimized
            
        except Exception as e:
            # é”™è¯¯å¤„ç†ï¼šè¯¦ç»†è®°å½•åˆ°ç»ˆç«¯
            print(f"\nâŒ ä¼˜åŒ–å¤±è´¥ï¼")
            print(f"{'='*60}")
            
            error_msg = str(e)
            print(f"ğŸ› é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"ğŸ“ é”™è¯¯è¯¦æƒ…: {error_msg[:500]}")
            
            # å¦‚æœæ˜¯ Pydantic éªŒè¯é”™è¯¯ï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯
            if "validation" in error_msg.lower() or "Field required" in error_msg:
                print("\nâš ï¸ è¿™æ˜¯æ•°æ®ç»“æ„éªŒè¯é”™è¯¯ï¼Œå¯èƒ½åŸå› ï¼š")
                print("   1. æ¨¡å‹è¿”å›çš„ JSON æ ¼å¼ä¸ç¬¦åˆè¦æ±‚")
                print("   2. ç¼ºå°‘å¿…éœ€çš„å­—æ®µï¼ˆthinking_process, improved_prompt ç­‰ï¼‰")
                print("   3. æ¨¡å‹å¯èƒ½ä¸æ”¯æŒ JSON æ ¼å¼è¾“å‡º")
                print("\nğŸ’¡ å»ºè®®ï¼šå°è¯•æ›´æ¢æ¨¡å‹ï¼Œæ¨èä½¿ç”¨ meta/llama-3.1-405b-instruct")
            
            # æ‰“å°å®Œæ•´å †æ ˆ
            import traceback
            print(f"\nğŸ“„ å®Œæ•´å †æ ˆä¿¡æ¯ï¼š")
            traceback.print_exc()
            print(f"{'='*60}\n")
            
            # æ ¹æ®é”™è¯¯ç±»å‹æŠ›å‡ºæ˜ç¡®çš„å¼‚å¸¸
            if "404" in error_msg:
                raise Exception(f"API è°ƒç”¨å¤±è´¥ (404): è¯·æ£€æŸ¥ API Key æ˜¯å¦æœ‰æ•ˆï¼Œæˆ–æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®ã€‚è¯¦ç»†ä¿¡æ¯ï¼š{error_msg[:200]}")
            elif "401" in error_msg or "Unauthorized" in error_msg:
                raise Exception(f"API Key æ— æ•ˆæˆ–å·²è¿‡æœŸã€‚è¯·æ£€æŸ¥æ‚¨çš„ API Key é…ç½®ã€‚")
            elif "rate_limit" in error_msg.lower():
                raise Exception(f"API è¯·æ±‚é¢‘ç‡è¶…é™ï¼Œè¯·ç¨åå†è¯•ã€‚")
            else:
                raise Exception(f"ä¼˜åŒ–å¤±è´¥: {error_msg[:300]}")
    
    def optimize_classification(self,
                               task_description: str,
                               labels: list[str],
                               example_texts: Optional[list[str]] = None) -> ClassificationPrompt:
        """
        é’ˆå¯¹åˆ†ç±»ä»»åŠ¡çš„ä¼˜åŒ–å‡½æ•°
        
        Args:
            task_description: åˆ†ç±»ä»»åŠ¡æè¿°ï¼Œå¦‚ "åˆ¤æ–­ç”¨æˆ·è¯„è®ºçš„æƒ…æ„Ÿå€¾å‘"
            labels: ç›®æ ‡æ ‡ç­¾åˆ—è¡¨ï¼Œå¦‚ ["Positive", "Negative", "Neutral"]
            example_texts: å¯é€‰çš„ç¤ºä¾‹æ–‡æœ¬ï¼Œç”¨äºç”Ÿæˆ Few-Shot æ ·æœ¬
            
        Returns:
            ClassificationPrompt: ä¼˜åŒ–åçš„åˆ†ç±» Prompt
        """
        print(f"\n{'='*60}")
        print(f"ğŸ·ï¸  å¼€å§‹åˆ†ç±»ä»»åŠ¡ Prompt ä¼˜åŒ–")
        print(f"{'='*60}")
        print(f"ğŸ”Œ API æä¾›å•†: {self.provider.upper()}")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {self.model}")
        print(f"ğŸ“ ä»»åŠ¡æè¿°: {task_description[:50]}...")
        print(f"ğŸ·ï¸  ç›®æ ‡æ ‡ç­¾: {', '.join(labels)}")
        print(f"{'='*60}\n")
        
        # æ„å»ºåˆ†ç±»ä»»åŠ¡ä¸“ç”¨çš„ Meta-Prompt
        # ä¸ä½¿ç”¨ f-stringï¼Œé¿å…èŠ±æ‹¬å·å†²çª
        system_prompt = """
ä½ æ˜¯ä¸€ä¸ªä¸“é—¨æ„å»º AI æ–‡æœ¬åˆ†ç±»å™¨çš„ä¸“å®¶ã€‚ä½ çš„ç›®æ ‡æ˜¯ç¼–å†™ä¸€ä¸ª**é«˜ç²¾åº¦**çš„åˆ†ç±» Promptã€‚

**ä»»åŠ¡æè¿°**ï¼šTASK_DESCRIPTION
**ç›®æ ‡æ ‡ç­¾**ï¼šTARGET_LABELS

**ä½ çš„ä»»åŠ¡**ï¼š

1. **æ ‡ç­¾æ¶ˆæ­§ (Label Disambiguation)**
   - ä¸ºæ¯ä¸ªæ ‡ç­¾ç¼–å†™æ¸…æ™°ã€å…·ä½“çš„å®šä¹‰
   - æ˜ç¡®è¾¹ç•Œæƒ…å†µï¼ˆEdge Casesï¼‰å’Œåˆ¤æ–­æ ‡å‡†
   - è¯´æ˜ä»€ä¹ˆæ ·çš„æ–‡æœ¬å±äºè¯¥æ ‡ç­¾ï¼Œä»€ä¹ˆä¸å±äº

2. **æ ·æœ¬åˆæˆ (Few-Shot Generation)**
   - æ ¹æ®æ ‡ç­¾å®šä¹‰ï¼Œåˆ›ä½œ 3-5 ä¸ªå…¸å‹çš„é«˜è´¨é‡ç¤ºä¾‹
   - ç¤ºä¾‹å¿…é¡»è¦†ç›–ä¸åŒæ ‡ç­¾ï¼Œå…·æœ‰ä»£è¡¨æ€§
   - æ¯ä¸ªç¤ºä¾‹åŒ…å« inputï¼ˆè¾“å…¥æ–‡æœ¬ï¼‰å’Œ labelï¼ˆå¯¹åº”æ ‡ç­¾ï¼‰

3. **æ€ç»´é“¾è®¾è®¡ (Chain of Thought)**
   - è®¾è®¡å¼•å¯¼è¯­ï¼Œè®©æ¨¡å‹å…ˆåˆ†æç‰¹å¾ï¼Œå†ç»™å‡ºåˆ†ç±»ç»“æœ
   - å¯¹äºå¤æ‚åˆ†ç±»ä»»åŠ¡ï¼Œä½¿ç”¨ "Let's think step by step"

4. **æ ¼å¼é”å®š (Output Format)**
   - æ˜ç¡®è¦æ±‚æ¨¡å‹åªè¾“å‡ºç‰¹å®šæ ¼å¼ï¼ˆå¦‚ JSONï¼‰
   - ç¦æ­¢æ¨¡å‹è¾“å‡ºå¤šä½™çš„è§£é‡Šæˆ–åºŸè¯
   - ç¡®ä¿è¾“å‡ºå¯ä»¥è¢«ä»£ç è½»æ¾è§£æ

5. **è§’è‰²è®¾å®š**
   - ä¸ºåˆ†ç±»å™¨è®¾å®šä¸€ä¸ªä¸“ä¸šçš„è§’è‰²èº«ä»½
   - å¢å¼ºæ¨¡å‹å¯¹ä»»åŠ¡çš„ç†è§£å’Œæ‰§è¡Œå‡†ç¡®åº¦

**è¾“å‡ºè¦æ±‚**ï¼š
è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- thinking_process: ä½ çš„ä¼˜åŒ–æ€è€ƒè¿‡ç¨‹
- role_definition: è§’è‰²è®¾å®šæè¿°
- label_definitions: æ ‡ç­¾å®šä¹‰å­—å…¸ï¼ˆé”®ä¸ºæ ‡ç­¾åï¼Œå€¼ä¸ºè¯¦ç»†å®šä¹‰ï¼‰
- few_shot_examples: ç¤ºä¾‹åˆ—è¡¨ï¼ˆæ¯ä¸ªåŒ…å« input å’Œ label å­—æ®µï¼‰
- reasoning_guidance: æ€ç»´é“¾å¼•å¯¼è¯­
- output_format: è¾“å‡ºæ ¼å¼è¦æ±‚è¯´æ˜
- final_prompt: å®Œæ•´çš„ã€å¯ç›´æ¥ä½¿ç”¨çš„åˆ†ç±» Prompt
- enhancement_techniques: ä½¿ç”¨çš„ä¼˜åŒ–æŠ€æœ¯åˆ—è¡¨

**å…³é”®è¦æ±‚ - final_prompt å¿…é¡»åŒ…å«å ä½ç¬¦**ï¼š
- final_prompt å¿…é¡»æ˜¯ä¸€ä¸ªå®Œæ•´çš„ã€ç»“æ„æ¸…æ™°çš„ã€å¯ä»¥ç›´æ¥å¤åˆ¶ä½¿ç”¨çš„åˆ†ç±» Prompt
- **å¿…é¡»åœ¨ Prompt ä¸­æ˜ç¡®æ ‡æ³¨å¾…åˆ†ç±»æ–‡æœ¬çš„ä½ç½®**ï¼Œä½¿ç”¨ä»¥ä¸‹ä»»ä¸€å ä½ç¬¦æ ¼å¼ï¼š
  * [å¾…åˆ†ç±»æ–‡æœ¬] ï¼ˆæ¨èï¼‰
  * {{text}} ï¼ˆä¸¤ä¸ªèŠ±æ‹¬å·ï¼‰
  * [è¾“å…¥è¯„è®º]
  * [å¾…å¤„ç†æ–‡æœ¬]
- å ä½ç¬¦åº”è¯¥æ”¾åœ¨åˆç†çš„ä½ç½®ï¼Œæ¯”å¦‚ï¼š
  * "è¯„è®ºå†…å®¹ï¼š[å¾…åˆ†ç±»æ–‡æœ¬]"
  * "è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼š[å¾…åˆ†ç±»æ–‡æœ¬]"
  * "æ–‡æœ¬ï¼š{{text}}"
- **ä¸è¦**åªè¯´"åˆ†æè¿™ä¸ªè¯„è®º"æˆ–"åˆ¤æ–­æƒ…æ„Ÿ"è€Œä¸æä¾›å…·ä½“çš„æ’å…¥ä½ç½®
- final_prompt å¿…é¡»æ˜¯å¯ä»¥é€šè¿‡ç®€å•çš„å­—ç¬¦ä¸²æ›¿æ¢å°±èƒ½ä½¿ç”¨çš„æ¨¡æ¿

**ç¤ºä¾‹æ­£ç¡®æ ¼å¼**ï¼š
```
ä½ æ˜¯ä¸“ä¸šçš„æƒ…æ„Ÿåˆ†æå¸ˆã€‚
æ ‡ç­¾å®šä¹‰ï¼š...
ç¤ºä¾‹ï¼š...
ç°åœ¨è¯·åˆ†æä»¥ä¸‹è¯„è®ºçš„æƒ…æ„Ÿå€¾å‘ï¼š
[å¾…åˆ†ç±»æ–‡æœ¬]
è¯·è¾“å‡ºæ ‡ç­¾åç§°å³å¯ã€‚
```

**ç¤ºä¾‹é”™è¯¯æ ¼å¼ï¼ˆä¸è¦ç”Ÿæˆè¿™æ ·çš„ï¼‰**ï¼š
```
ä½ æ˜¯ä¸“ä¸šçš„æƒ…æ„Ÿåˆ†æå¸ˆã€‚
æ ‡ç­¾å®šä¹‰ï¼š...
ç¤ºä¾‹ï¼š...
è®©æˆ‘ä»¬åˆ†æè¯„è®ºçš„æƒ…æ„Ÿå€¾å‘ã€‚ï¼ˆâŒ ç¼ºå°‘æ˜ç¡®çš„æ–‡æœ¬æ’å…¥ä½ç½®ï¼‰
```
"""
        
        # æ‰‹åŠ¨æ›¿æ¢å˜é‡
        system_prompt = system_prompt.replace("TASK_DESCRIPTION", task_description)
        system_prompt = system_prompt.replace("TARGET_LABELS", ', '.join(labels))
        
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "è¯·ä¸ºè¿™ä¸ªåˆ†ç±»ä»»åŠ¡ç”Ÿæˆä¼˜åŒ–çš„ Promptã€‚")
        ])
        
        try:
            print("ğŸ“¤ æ­£åœ¨è°ƒç”¨ API...")
            
            messages = prompt_template.format_messages()
            print(f"ğŸ’¬ æ¶ˆæ¯é•¿åº¦: {len(str(messages))} å­—ç¬¦")
            
            # è°ƒç”¨ LLM
            if self.provider == "openai":
                print("ğŸ”§ ä½¿ç”¨ OpenAI JSON mode")
                response = self.llm.invoke(
                    messages,
                    response_format={"type": "json_object"}
                )
            else:
                print("ğŸ”§ ä½¿ç”¨ NVIDIA æ ‡å‡†è°ƒç”¨")
                response = self.llm.invoke(messages)
            
            # è§£æç»“æœ
            content = response.content
            print(f"ğŸ“¥ æ”¶åˆ°å“åº”ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
            
            # æå– JSON
            if "```json" in content:
                print("ğŸ” æ£€æµ‹åˆ° JSON ä»£ç å—ï¼Œæ­£åœ¨æå–...")
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                print("ğŸ” æ£€æµ‹åˆ°ä»£ç å—ï¼Œæ­£åœ¨æå–...")
                content = content.split("```")[1].split("```")[0].strip()
            
            print("âš™ï¸ æ­£åœ¨è§£æ JSON...")
            result_dict = json.loads(content)
            
            print("âœ… JSON è§£ææˆåŠŸ")
            print("ğŸ”¨ æ­£åœ¨éªŒè¯æ•°æ®ç»“æ„...")
            optimized = ClassificationPrompt(**result_dict)
            
            print("âœ… åˆ†ç±» Prompt ä¼˜åŒ–å®Œæˆï¼")
            print(f"{'='*60}\n")
            
            return optimized
            
        except Exception as e:
            # é”™è¯¯å¤„ç†
            print(f"\nâŒ åˆ†ç±»ä¼˜åŒ–å¤±è´¥ï¼")
            print(f"{'='*60}")
            
            error_msg = str(e)
            print(f"ğŸ› é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"ğŸ“ é”™è¯¯è¯¦æƒ…: {error_msg[:500]}")
            
            import traceback
            print(f"\nğŸ“„ å®Œæ•´å †æ ˆä¿¡æ¯ï¼š")
            traceback.print_exc()
            print(f"{'='*60}\n")
            
            # æŠ›å‡ºå¼‚å¸¸
            if "404" in error_msg:
                raise Exception(f"API è°ƒç”¨å¤±è´¥ (404): è¯·æ£€æŸ¥ API Key æ˜¯å¦æœ‰æ•ˆï¼Œæˆ–æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®ã€‚")
            elif "401" in error_msg or "Unauthorized" in error_msg:
                raise Exception(f"API Key æ— æ•ˆæˆ–å·²è¿‡æœŸã€‚")
            else:
                raise Exception(f"åˆ†ç±»ä¼˜åŒ–å¤±è´¥: {error_msg[:300]}")
    
    def optimize_summarization(self,
                              task_description: str,
                              source_type: str,
                              target_audience: str,
                              focus_points: str,
                              length_constraint: Optional[str] = None) -> SummarizationPrompt:
        """
        é’ˆå¯¹æ‘˜è¦ä»»åŠ¡çš„ä¼˜åŒ–å‡½æ•°
        
        Args:
            task_description: æ‘˜è¦ä»»åŠ¡æè¿°ï¼Œå¦‚ "æ€»ç»“æŠ€æœ¯ä¼šè®®çš„æ ¸å¿ƒå†³ç­–"
            source_type: æºæ–‡æœ¬ç±»å‹ï¼Œå¦‚ "ä¼šè®®è®°å½•"ã€"å­¦æœ¯è®ºæ–‡"ã€"æ–°é—»æŠ¥é“"
            target_audience: ç›®æ ‡è¯»è€…ï¼Œå¦‚ "æŠ€æœ¯ç»ç†"ã€"æ™®é€šç”¨æˆ·"
            focus_points: æ ¸å¿ƒå…³æ³¨ç‚¹ï¼Œå¦‚ "è¡ŒåŠ¨è®¡åˆ’å’Œè´Ÿè´£äºº"
            length_constraint: ç¯‡å¹…é™åˆ¶ï¼Œå¦‚ "100å­—ä»¥å†…"ã€"3-5ä¸ªè¦ç‚¹"
            
        Returns:
            SummarizationPrompt: ä¼˜åŒ–åçš„æ‘˜è¦ Prompt
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“ å¼€å§‹æ‘˜è¦ä»»åŠ¡ Prompt ä¼˜åŒ–")
        print(f"{'='*60}")
        print(f"ğŸ”Œ API æä¾›å•†: {self.provider.upper()}")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {self.model}")
        print(f"ğŸ“ ä»»åŠ¡æè¿°: {task_description[:50]}...")
        print(f"ğŸ“„ æºæ–‡æœ¬ç±»å‹: {source_type}")
        print(f"ğŸ‘¥ ç›®æ ‡å—ä¼—: {target_audience}")
        print(f"ğŸ¯ å…³æ³¨ç‚¹: {focus_points[:50]}...")
        if length_constraint:
            print(f"ğŸ“ ç¯‡å¹…é™åˆ¶: {length_constraint}")
        print(f"{'='*60}\n")
        
        # æ„å»ºæ‘˜è¦ä»»åŠ¡ä¸“ç”¨çš„ Meta-Prompt
        length_text = f"\n**ç¯‡å¹…é™åˆ¶**ï¼š{length_constraint}" if length_constraint else ""
        
        system_prompt = f"""
ä½ æ˜¯ä¸€ä½ç²¾é€šä¿¡æ¯å‹ç¼©å’Œæ‘˜è¦æ’°å†™çš„ Prompt Engineering ä¸“å®¶ã€‚
ç”¨æˆ·çš„ç›®æ ‡æ˜¯é’ˆå¯¹ç‰¹å®šåœºæ™¯ç”Ÿæˆä¸€ä¸ª**é«˜è´¨é‡çš„æ‘˜è¦ Prompt**ã€‚

**ä»»åŠ¡ä¿¡æ¯**ï¼š
- ä»»åŠ¡æè¿°ï¼š{task_description}
- æºæ–‡æœ¬ç±»å‹ï¼š{source_type}
- ç›®æ ‡å—ä¼—ï¼š{target_audience}
- æ ¸å¿ƒå…³æ³¨ç‚¹ï¼š{focus_points}{length_text}

**ä½ çš„ä»»åŠ¡**ï¼š

1. **è§’è‰²æ²‰æµ¸ (Role Immersion)**
   - æ ¹æ®æºæ–‡æœ¬ç±»å‹å’Œç›®æ ‡å—ä¼—ï¼Œè®¾å®šæœ€åˆé€‚çš„ä¸“å®¶è§’è‰²
   - ä¾‹å¦‚ï¼šä¼šè®®è®°å½• â†’ "ä¸“ä¸šçš„ä¼šè®®çºªè¦ç§˜ä¹¦"ï¼›å­¦æœ¯è®ºæ–‡ â†’ "èµ„æ·±çš„ç§‘ç ”ç¼–è¾‘"

2. **æå–è§„åˆ™åˆ¶å®š (Extraction Rules)**
   - æ˜ç¡®å‘Šè¯‰æ¨¡å‹å¿…é¡»ä¿ç•™ä»€ä¹ˆä¿¡æ¯ï¼ˆå¦‚ï¼šæ•°å­—ã€æ—¥æœŸã€äººåã€å…³é”®å†³ç­–ï¼‰
   - é’ˆå¯¹ç”¨æˆ·çš„æ ¸å¿ƒå…³æ³¨ç‚¹ï¼Œå¼ºè°ƒç›¸å…³ä¿¡æ¯çš„é‡è¦æ€§
   - è‡³å°‘æä¾› 3-5 æ¡å…·ä½“çš„æå–è§„åˆ™

3. **è´Ÿé¢çº¦æŸ (Negative Constraints)**
   - æ˜ç¡®å‘Šè¯‰æ¨¡å‹"ä¸è¦"åšä»€ä¹ˆ
   - ä¾‹å¦‚ï¼šä¸è¦ä½¿ç”¨æ¨¡ç³Šè¯æ±‡ã€ä¸è¦é—æ¼æ•°æ®ã€ä¸è¦æ·»åŠ åŸæ–‡æ²¡æœ‰çš„ä¿¡æ¯
   - é˜²æ­¢æ¨¡å‹"å¹»è§‰"ï¼ˆç¼–é€ ç»†èŠ‚ï¼‰

4. **ç»“æ„åŒ–è¾“å‡º (Structured Format)**
   - æ ¹æ®æºæ–‡æœ¬ç±»å‹è®¾è®¡åˆé€‚çš„è¾“å‡ºæ ¼å¼
   - ä¼šè®®è®°å½• â†’ è¡¨æ ¼æˆ–åˆ†å±‚ç»“æ„ï¼ˆèƒŒæ™¯ã€å†³ç­–ã€è¡ŒåŠ¨è®¡åˆ’ï¼‰
   - æ–°é—»æŠ¥é“ â†’ TL;DR + å…³é”®äº‹å®
   - å­¦æœ¯è®ºæ–‡ â†’ ç ”ç©¶ç›®çš„ã€æ–¹æ³•ã€ç»“è®ºã€æ„ä¹‰

5. **æ€è€ƒæ­¥éª¤è®¾è®¡ (Step-by-Step Guide)**
   - ç»™æ¨¡å‹æ˜ç¡®çš„å¤„ç†æµç¨‹ï¼Œå¦‚ï¼š
     Step 1: é€šè¯»å…¨æ–‡ï¼Œæ ‡è®°å…³é”®ä¿¡æ¯
     Step 2: æ ¹æ®å…³æ³¨ç‚¹ç­›é€‰å†…å®¹
     Step 3: æŒ‰ç»“æ„ç»„ç»‡ä¿¡æ¯
     Step 4: ç²¾ç®€è¡¨è¾¾ï¼Œç¡®ä¿å‡†ç¡®

6. **å…³æ³¨ç‚¹é”šå®š (Focus Areas)**
   - å°†ç”¨æˆ·çš„æ ¸å¿ƒå…³æ³¨ç‚¹è½¬åŒ–ä¸ºå…·ä½“çš„ä¿¡æ¯ç±»åˆ«
   - åœ¨ Prompt ä¸­å¤šæ¬¡å¼ºè°ƒè¿™äº›å…³æ³¨ç‚¹çš„ä¼˜å…ˆçº§

**è¾“å‡ºè¦æ±‚**ï¼š
è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- thinking_process: ä½ çš„ä¼˜åŒ–æ€è€ƒè¿‡ç¨‹
- role_setting: è§’è‰²è®¾å®šæè¿°
- extraction_rules: æå–è§„åˆ™åˆ—è¡¨ï¼ˆè‡³å°‘3-5æ¡ï¼‰
- negative_constraints: è´Ÿé¢çº¦æŸåˆ—è¡¨ï¼ˆè‡³å°‘3æ¡ï¼‰
- format_template: è¾“å‡ºæ ¼å¼æ¨¡æ¿ï¼ˆä½¿ç”¨ Markdownï¼‰
- step_by_step_guide: å¤„ç†æ­¥éª¤è¯´æ˜
- focus_areas: æ ¸å¿ƒå…³æ³¨ç‚¹åˆ—è¡¨
- final_prompt: å®Œæ•´çš„ã€å¯ç›´æ¥ä½¿ç”¨çš„æ‘˜è¦ Promptï¼ˆç”¨ {{{{text}}}} ä½œä¸ºå¾…æ‘˜è¦æ–‡æœ¬çš„å ä½ç¬¦ï¼‰

**é‡è¦**ï¼š
- final_prompt å¿…é¡»æ˜¯ä¸€ä¸ªå®Œæ•´çš„ã€ç»“æ„æ¸…æ™°çš„ã€å¯ä»¥ç›´æ¥å¤åˆ¶ä½¿ç”¨çš„æ‘˜è¦ Prompt
- å…¶ä¸­å¾…æ‘˜è¦çš„æ–‡æœ¬ç”¨ {{{{text}}}} å ä½ç¬¦è¡¨ç¤º
- æ‰€æœ‰è§„åˆ™å’Œçº¦æŸéƒ½è¦æ•´åˆè¿› final_prompt ä¸­
"""
        
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "è¯·ä¸ºè¿™ä¸ªæ‘˜è¦ä»»åŠ¡ç”Ÿæˆä¼˜åŒ–çš„ Promptã€‚")
        ])
        
        try:
            print("ğŸ“¤ æ­£åœ¨è°ƒç”¨ API...")
            
            messages = prompt_template.format_messages()
            print(f"ğŸ’¬ æ¶ˆæ¯é•¿åº¦: {len(str(messages))} å­—ç¬¦")
            
            # è°ƒç”¨ LLM
            if self.provider == "openai":
                print("ğŸ”§ ä½¿ç”¨ OpenAI JSON mode")
                response = self.llm.invoke(
                    messages,
                    response_format={"type": "json_object"}
                )
            else:
                print("ğŸ”§ ä½¿ç”¨ NVIDIA æ ‡å‡†è°ƒç”¨")
                response = self.llm.invoke(messages)
            
            # è§£æç»“æœ
            content = response.content
            print(f"ğŸ“¥ æ”¶åˆ°å“åº”ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
            
            # æå– JSON
            if "```json" in content:
                print("ğŸ” æ£€æµ‹åˆ° JSON ä»£ç å—ï¼Œæ­£åœ¨æå–...")
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                print("ğŸ” æ£€æµ‹åˆ°ä»£ç å—ï¼Œæ­£åœ¨æå–...")
                content = content.split("```")[1].split("```")[0].strip()
            
            print("âš™ï¸ æ­£åœ¨è§£æ JSON...")
            result_dict = json.loads(content)
            
            print("âœ… JSON è§£ææˆåŠŸ")
            print("ğŸ”¨ æ­£åœ¨éªŒè¯æ•°æ®ç»“æ„...")
            optimized = SummarizationPrompt(**result_dict)
            
            print("âœ… æ‘˜è¦ Prompt ä¼˜åŒ–å®Œæˆï¼")
            print(f"{'='*60}\n")
            
            return optimized
            
        except Exception as e:
            # é”™è¯¯å¤„ç†
            print(f"\nâŒ æ‘˜è¦ä¼˜åŒ–å¤±è´¥ï¼")
            print(f"{'='*60}")
            
            error_msg = str(e)
            print(f"ğŸ› é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"ğŸ“ é”™è¯¯è¯¦æƒ…: {error_msg[:500]}")
            
            import traceback
            print(f"\nğŸ“„ å®Œæ•´å †æ ˆä¿¡æ¯ï¼š")
            traceback.print_exc()
            print(f"{'='*60}\n")
            
            # æŠ›å‡ºå¼‚å¸¸
            if "404" in error_msg:
                raise Exception(f"API è°ƒç”¨å¤±è´¥ (404): è¯·æ£€æŸ¥ API Key æ˜¯å¦æœ‰æ•ˆï¼Œæˆ–æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®ã€‚")
            elif "401" in error_msg or "Unauthorized" in error_msg:
                raise Exception(f"API Key æ— æ•ˆæˆ–å·²è¿‡æœŸã€‚")
            else:
                raise Exception(f"æ‘˜è¦ä¼˜åŒ–å¤±è´¥: {error_msg[:300]}")
    
    def optimize_translation(self,
                           source_lang: str,
                           target_lang: str,
                           domain: str,
                           tone: str,
                           user_glossary: str = "") -> TranslationPrompt:
        """
        é’ˆå¯¹ç¿»è¯‘ä»»åŠ¡çš„ä¼˜åŒ–å‡½æ•°
        
        Args:
            source_lang: æºè¯­è¨€ï¼Œå¦‚ "ä¸­æ–‡"ã€"è‹±æ–‡"
            target_lang: ç›®æ ‡è¯­è¨€
            domain: åº”ç”¨é¢†åŸŸï¼Œå¦‚ "é€šç”¨æ—¥å¸¸"ã€"IT/æŠ€æœ¯æ–‡æ¡£"ã€"æ³•å¾‹åˆåŒ"ç­‰
            tone: æœŸæœ›é£æ ¼ï¼Œå¦‚ "æ ‡å‡†/å‡†ç¡®"ã€"åœ°é“/å£è¯­åŒ–"
            user_glossary: ç”¨æˆ·æä¾›çš„æœ¯è¯­è¡¨ï¼Œæ ¼å¼å¦‚ "Prompt=æç¤ºè¯\nLLM=å¤§è¯­è¨€æ¨¡å‹"
            
        Returns:
            TranslationPrompt: ä¼˜åŒ–åçš„ç¿»è¯‘ Prompt
        """
        print(f"\n{'='*60}")
        print(f"ğŸŒ å¼€å§‹ç¿»è¯‘ä»»åŠ¡ Prompt ä¼˜åŒ–")
        print(f"{'='*60}")
        print(f"ğŸ”Œ API æä¾›å•†: {self.provider.upper()}")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {self.model}")
        print(f"ğŸ”„ ç¿»è¯‘æ–¹å‘: {source_lang} â†’ {target_lang}")
        print(f"ğŸ“š åº”ç”¨é¢†åŸŸ: {domain}")
        print(f"ğŸ¨ æœŸæœ›é£æ ¼: {tone}")
        if user_glossary:
            print(f"ğŸ“– æœ¯è¯­è¡¨: {len(user_glossary.split(chr(10)))} æ¡")
        print(f"{'='*60}\n")
        
        # å¤„ç†æœ¯è¯­è¡¨
        glossary_text = ""
        if user_glossary.strip():
            glossary_text = f"""
**ç”¨æˆ·æŒ‡å®šæœ¯è¯­è¡¨**ï¼š
ç”¨æˆ·å¼ºåˆ¶æŒ‡å®šäº†ä»¥ä¸‹æœ¯è¯­å¯¹åº”å…³ç³»ï¼Œå¿…é¡»åœ¨ Prompt ä¸­åˆ›å»ºä¸€ä¸ªæ˜ç¡®çš„ Glossary Section æ¥é”å®šè¿™äº›ç¿»è¯‘ï¼š
{user_glossary}
"""
        
        # æ„å»ºç¿»è¯‘ä»»åŠ¡ä¸“ç”¨çš„ Meta-Prompt
        system_prompt = f"""
ä½ æ˜¯ä¸€ä½ç²¾é€šå¤šè¯­è¨€è½¬æ¢çš„ Prompt Engineering ä¸“å®¶ã€‚
ä½ çš„ä»»åŠ¡æ˜¯æ„å»ºä¸€ä¸ª**ä¸“å®¶çº§çš„ç¿»è¯‘ Prompt**ï¼Œä»¥è§£å†³æœºå™¨ç¿»è¯‘ç”Ÿç¡¬ã€ç¼ºä¹è¯­å¢ƒã€é£æ ¼ä¸ä¸€è‡´çš„é—®é¢˜ã€‚

**ä»»åŠ¡ä¿¡æ¯**ï¼š
- è¯­è¨€æ–¹å‘ï¼š{source_lang} â†’ {target_lang}
- åº”ç”¨é¢†åŸŸï¼š{domain}
- æœŸæœ›é£æ ¼ï¼š{tone}{glossary_text}

**ç¿»è¯‘ä»»åŠ¡çš„æ ¸å¿ƒæŒ‘æˆ˜**ï¼š
1. **è¯­å¢ƒåå·®ï¼ˆContext Nuanceï¼‰**ï¼šåŒä¸€ä¸ªè¯åœ¨ä¸åŒåœºæ™¯æœ‰ä¸åŒå«ä¹‰ï¼ˆå¦‚ "Bank" æ˜¯"é“¶è¡Œ"è¿˜æ˜¯"æ²³å²¸"ï¼Ÿï¼‰
2. **é£æ ¼ä¸€è‡´æ€§ï¼ˆTone & Styleï¼‰**ï¼šæ˜¯"ä¿¡è¾¾é›…"çš„æ–‡å­¦ç¿»è¯‘ï¼Œè¿˜æ˜¯"ç²¾å‡†ç›´ç™½"çš„æŠ€æœ¯ç¿»è¯‘ï¼Ÿ
3. **æœ¯è¯­ä¸€è‡´æ€§ï¼ˆGlossary Consistencyï¼‰**ï¼šç‰¹å®šçš„ä¸“æœ‰åè¯ä¸èƒ½ä¹±ç¿»ï¼Œéœ€è¦ç»Ÿä¸€æ ‡å‡†

**ä½ çš„ä»»åŠ¡**ï¼š
æ„å»ºä¸€ä¸ªåŒ…å«ä»¥ä¸‹é«˜çº§ç­–ç•¥çš„ç¿»è¯‘ Promptï¼š

1. **é¢†åŸŸæ²‰æµ¸ï¼ˆDomain Immersionï¼‰**
   - æ ¹æ®é¢†åŸŸè®¾å®šæœ€æƒå¨çš„ä¸“å®¶è§’è‰²
   - ITæ–‡æ¡£ â†’ "ç²¾é€šä¸­è‹±åŒè¯­çš„èµ„æ·±è½¯ä»¶å·¥ç¨‹å¸ˆå’ŒæŠ€æœ¯æ–‡æ¡£ç¼–è¾‘"
   - æ³•å¾‹åˆåŒ â†’ "èµ„æ·±å›½é™…æ³•å¾‹ç¿»è¯‘ä¸“å®¶ï¼Œç†Ÿæ‚‰ä¸­è‹±æ³•å¾‹æœ¯è¯­ä½“ç³»"
   - æ–‡å­¦ä½œå“ â†’ "ä¸“ä¸šæ–‡å­¦è¯‘è€…ï¼Œæ›¾ç¿»è¯‘å¤šéƒ¨è·å¥–ä½œå“"
   - å­¦æœ¯è®ºæ–‡ â†’ "ã€Šè‡ªç„¶ã€‹æ‚å¿—ç¼–è¾‘ï¼Œç²¾é€šå­¦æœ¯è§„èŒƒå’Œç§‘ç ”è¡¨è¾¾"

2. **æœ¯è¯­é”å®šï¼ˆGlossary Lockingï¼‰**
   - å¦‚æœç”¨æˆ·æä¾›äº†æœ¯è¯­è¡¨ï¼Œå¿…é¡»åœ¨ Prompt ä¸­ç”Ÿæˆä¸€ä¸ªæ¸…æ™°çš„ Mapping Table
   - è¦æ±‚æ¨¡å‹"ä¸¥æ ¼éµå®ˆ"ï¼ˆStrictly Adhereï¼‰è¿™äº›æœ¯è¯­å¯¹åº”å…³ç³»
   - æ ¼å¼ç¤ºä¾‹ï¼š
     ```
     **æœ¯è¯­è¡¨ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰**ï¼š
     - Apple â†’ è‹¹æœå…¬å¸ï¼ˆè€Œé"è‹¹æœ"æ°´æœï¼‰
     - Prompt â†’ æç¤ºè¯ï¼ˆæŠ€æœ¯æœ¯è¯­ï¼Œä¸ç¿»è¯‘ä¸º"æç¤º"ï¼‰
     ```

3. **ä¸‰æ­¥ç¿»è¯‘æ³•ï¼ˆThree-Step Translationï¼‰**
   - åœ¨ Prompt ä¸­è¦æ±‚æ¨¡å‹æŒ‰ä»¥ä¸‹æµç¨‹å¤„ç†ï¼š
     Step 1: åˆ†æä¸Šä¸‹æ–‡å’Œä¸“ä¸šæœ¯è¯­ï¼Œè¿›è¡Œåˆæ­¥ç›´è¯‘
     Step 2: æ ¹æ®è¯­å¢ƒå’Œé¢†åŸŸç‰¹ç‚¹ï¼Œè°ƒæ•´è¡¨è¾¾æ–¹å¼ï¼Œç¡®ä¿è¯­ä¹‰å‡†ç¡®
     Step 3: æ¶¦è‰²é£æ ¼ï¼Œä½¿è¯‘æ–‡ç¬¦åˆç›®æ ‡è¯­è¨€çš„è¡¨è¾¾ä¹ æƒ¯å’ŒæœŸæœ›é£æ ¼
   - è¿™ç§"æ…¢æ€è€ƒ"æ¨¡å¼èƒ½æ˜¾è‘—æå‡è´¨é‡

4. **é£æ ¼æŒ‡å—ï¼ˆStyle Guidelinesï¼‰**
   - æ ¹æ®æœŸæœ›é£æ ¼ç»™å‡ºå…·ä½“æŒ‡å¯¼ï¼š
   - "æ ‡å‡†/å‡†ç¡®"ï¼šä¿æŒå®¢è§‚ã€ä¸¥è°¨ï¼Œé¿å…æ·»åŠ ä¸»è§‚è‰²å½©
   - "åœ°é“/å£è¯­åŒ–"ï¼šä½¿ç”¨ç›®æ ‡è¯­è¨€çš„è‡ªç„¶è¡¨è¾¾ï¼Œé¿å…"ç¿»è¯‘è…”"
   - "ä¼˜ç¾/æ–‡å­¦æ€§"ï¼šæ³¨é‡éŸµå¾‹å’Œç¾æ„Ÿï¼Œå¯é€‚å½“æ„è¯‘
   - "æç®€/æ‘˜è¦å¼"ï¼šç®€æ´æ˜äº†ï¼Œå»é™¤å†—ä½™

5. **ä¿ç•™è§„åˆ™ï¼ˆPreservation Rulesï¼‰**
   - å¯¹äºä»¥ä¸‹å†…å®¹ï¼Œæ˜ç¡®è¦æ±‚ä¿ç•™åŸæ–‡ï¼š
   - ä»£ç å—ã€å‘½ä»¤è¡Œã€æ–‡ä»¶è·¯å¾„
   - ä¸“æœ‰åè¯ï¼ˆäººåã€åœ°åã€å“ç‰Œåï¼‰
   - æ— æ³•ç¿»è¯‘æˆ–ä¸å®œç¿»è¯‘çš„æœ¯è¯­ï¼ˆç”¨æ‹¬å·æ³¨é‡ŠåŸæ–‡ï¼‰

6. **æ ¼å¼è§„èŒƒï¼ˆFormat Requirementsï¼‰**
   - ä¿æŒåŸæ–‡çš„æ®µè½ç»“æ„å’Œæ ¼å¼
   - æ•°å­—ã€æ ‡ç‚¹ç¬¦å·çš„è§„èŒƒï¼ˆå¦‚ï¼šä¸­æ–‡ç”¨å…¨è§’ï¼Œè‹±æ–‡ç”¨åŠè§’ï¼‰

**è¾“å‡ºè¦æ±‚**ï¼š
è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- thinking_process: ä½ çš„ä¼˜åŒ–æ€è€ƒè¿‡ç¨‹ï¼Œåˆ†æè¿™ä¸ªç¿»è¯‘ä»»åŠ¡çš„ç‰¹ç‚¹å’Œéš¾ç‚¹
- role_definition: è§’è‰²è®¾å®šæè¿°ï¼Œè¦å…·ä½“åˆ°è¯¥é¢†åŸŸæœ€æƒå¨çš„ä¸“å®¶
- style_guidelines: é£æ ¼æŒ‡å—åˆ—è¡¨ï¼ˆlistï¼‰ï¼Œé’ˆå¯¹æœŸæœ›é£æ ¼çš„å…·ä½“è¦æ±‚ï¼ˆ3-5æ¡ï¼‰
- glossary_section: æœ¯è¯­å¯¹ç…§è¡¨éƒ¨åˆ†çš„æ–‡æœ¬ï¼ˆå¦‚æœç”¨æˆ·æä¾›äº†æœ¯è¯­è¡¨ï¼‰ã€‚å¦‚æœæ²¡æœ‰åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
- workflow_steps: ç¿»è¯‘å·¥ä½œæµæŒ‡ä»¤ï¼Œæ¨èä½¿ç”¨"ä¸‰æ­¥ç¿»è¯‘æ³•"çš„è¯¦ç»†æè¿°
- final_prompt: å®Œæ•´çš„ã€å¯ç›´æ¥ä½¿ç”¨çš„ç¿»è¯‘ Promptï¼ˆç”¨ {{{{text}}}} ä½œä¸ºå¾…ç¿»è¯‘æ–‡æœ¬çš„å ä½ç¬¦ï¼‰

**é‡è¦**ï¼š
- final_prompt å¿…é¡»æ˜¯ä¸€ä¸ªå®Œæ•´çš„ã€ç»“æ„æ¸…æ™°çš„ã€å¯ä»¥ç›´æ¥å¤åˆ¶ä½¿ç”¨çš„ç¿»è¯‘ Prompt
- å…¶ä¸­å¾…ç¿»è¯‘çš„æ–‡æœ¬ç”¨ {{{{text}}}} å ä½ç¬¦è¡¨ç¤º
- æ‰€æœ‰è§„åˆ™ã€æœ¯è¯­è¡¨ã€é£æ ¼æŒ‡å—éƒ½è¦æ•´åˆè¿› final_prompt ä¸­
- åŠ¡å¿…ä½“ç°"é¢†åŸŸä¸“å®¶ + æœ¯è¯­é”å®š + ä¸‰æ­¥ç¿»è¯‘æ³•"çš„æ ¸å¿ƒç­–ç•¥
"""
        
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "è¯·ä¸ºè¿™ä¸ªç¿»è¯‘ä»»åŠ¡ç”Ÿæˆä¼˜åŒ–çš„ Promptã€‚")
        ])
        
        try:
            print("ğŸ“¤ æ­£åœ¨è°ƒç”¨ API...")
            
            messages = prompt_template.format_messages()
            print(f"ğŸ’¬ æ¶ˆæ¯é•¿åº¦: {len(str(messages))} å­—ç¬¦")
            
            # è°ƒç”¨ LLM
            if self.provider == "openai":
                print("ğŸ”§ ä½¿ç”¨ OpenAI JSON mode")
                response = self.llm.invoke(
                    messages,
                    response_format={"type": "json_object"}
                )
            else:
                print("ğŸ”§ ä½¿ç”¨ NVIDIA æ ‡å‡†è°ƒç”¨")
                response = self.llm.invoke(messages)
            
            # è§£æç»“æœ
            content = response.content
            print(f"ğŸ“¥ æ”¶åˆ°å“åº”ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
            
            # æå– JSON
            if "```json" in content:
                print("ğŸ” æ£€æµ‹åˆ° JSON ä»£ç å—ï¼Œæ­£åœ¨æå–...")
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                print("ğŸ” æ£€æµ‹åˆ°ä»£ç å—ï¼Œæ­£åœ¨æå–...")
                content = content.split("```")[1].split("```")[0].strip()
            
            print("âš™ï¸ æ­£åœ¨è§£æ JSON...")
            result_dict = json.loads(content)
            
            print("âœ… JSON è§£ææˆåŠŸ")
            print("ğŸ”¨ æ­£åœ¨éªŒè¯æ•°æ®ç»“æ„...")
            optimized = TranslationPrompt(**result_dict)
            
            print("âœ… ç¿»è¯‘ Prompt ä¼˜åŒ–å®Œæˆï¼")
            print(f"{'='*60}\n")
            
            return optimized
            
        except Exception as e:
            # é”™è¯¯å¤„ç†
            print(f"\nâŒ ç¿»è¯‘ä¼˜åŒ–å¤±è´¥ï¼")
            print(f"{'='*60}")
            
            error_msg = str(e)
            print(f"ğŸ› é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"ğŸ“ é”™è¯¯è¯¦æƒ…: {error_msg[:500]}")
            
            import traceback
            print(f"\nğŸ“„ å®Œæ•´å †æ ˆä¿¡æ¯ï¼š")
            traceback.print_exc()
            print(f"{'='*60}\n")
            
            # æŠ›å‡ºå¼‚å¸¸
            if "404" in error_msg:
                raise Exception(f"API è°ƒç”¨å¤±è´¥ (404): è¯·æ£€æŸ¥ API Key æ˜¯å¦æœ‰æ•ˆï¼Œæˆ–æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®ã€‚")
            elif "401" in error_msg or "Unauthorized" in error_msg:
                raise Exception(f"API Key æ— æ•ˆæˆ–å·²è¿‡æœŸã€‚")
            else:
                raise Exception(f"ç¿»è¯‘ä¼˜åŒ–å¤±è´¥: {error_msg[:300]}")
    
    def _build_meta_prompt(self, strategy: dict, scene_desc: str) -> str:
        """æ„å»º Meta-Promptï¼ˆæ•™ LLM å¦‚ä½•ä¼˜åŒ– Prompt çš„æç¤ºè¯ï¼‰"""
        
        template_name = strategy.get("template", "CO-STAR")
        focus_principles = strategy.get("focus", ["clarity", "structure"])
        extra_requirements = strategy.get("extra_requirements", [])
        
        # è·å–ç„¦ç‚¹åŸåˆ™çš„è¯¦ç»†è¯´æ˜
        principles_text = "\n".join([
            f"   - {OPTIMIZATION_PRINCIPLES.get(p, p)}"
            for p in focus_principles
        ])
        
        # æ„å»ºé¢å¤–è¦æ±‚æ–‡æœ¬
        extra_text = ""
        if extra_requirements:
            extra_text = "\n\n**åœºæ™¯ç‰¹å®šè¦æ±‚**ï¼š\n" + "\n".join([
                f"   - {req}" for req in extra_requirements
            ])
        
        meta_prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸–ç•Œçº§çš„ Prompt Engineering ä¸“å®¶ï¼Œæ“…é•¿å°†ç®€å•çš„æŒ‡ä»¤è½¬åŒ–ä¸ºç»“æ„åŒ–ã€é«˜æ€§èƒ½çš„ä¸“å®¶çº§ Promptã€‚

**ä½ çš„ä»»åŠ¡æµç¨‹**ï¼š

1. **æ·±åº¦ç†è§£**ï¼šä»”ç»†åˆ†æç”¨æˆ·çš„åŸå§‹ Promptï¼Œè¯†åˆ«å…¶æ ¸å¿ƒæ„å›¾å’Œéšå«éœ€æ±‚

2. **ä¸‰å¤§ä¼˜åŒ–ç­–ç•¥**ï¼š
   
   a) **è¯­ä¹‰æ‰©å±• (Semantic Expansion)**
      - è¡¥å……ç¼ºå¤±çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
      - æ˜ç¡®éšå«çš„çº¦æŸæ¡ä»¶
      - è§„èŒƒè¾“å‡ºæ ¼å¼è¦æ±‚
   
   b) **å…³é”®è¯å¢å¼º (Keywords Enhancement)**
      - è¯†åˆ«ä»»åŠ¡æ‰€å±çš„ä¸“ä¸šé¢†åŸŸ
      - åŠ å…¥è¯¥é¢†åŸŸçš„ä¸“ä¸šæœ¯è¯­å’Œè¡Œä¸šæ¦‚å¿µ
      - ç”¨ç²¾ç¡®çš„è¯æ±‡æ›¿æ¢æ¨¡ç³Šè¡¨è¾¾
   
   c) **ç»“æ„åŒ–é‡å†™ (Template Application)**
      - å¿…é¡»ä½¿ç”¨ **{template_name}** æ¡†æ¶è¿›è¡Œé‡å†™
      - ç¡®ä¿ Prompt é€»è¾‘æ¸…æ™°ã€å±‚æ¬¡åˆ†æ˜

3. **ä¼˜åŒ–åŸåˆ™**ï¼ˆæœ¬æ¬¡ä¼˜åŒ–é‡ç‚¹å…³æ³¨ï¼‰ï¼š
{principles_text}
{extra_text}

**åœºæ™¯ä¸Šä¸‹æ–‡**ï¼š{scene_desc if scene_desc else "é€šç”¨åœºæ™¯"}

**è¾“å‡ºè¦æ±‚**ï¼š
è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- thinking_process: ä½ çš„ä¼˜åŒ–æ€è€ƒè¿‡ç¨‹ï¼ˆ200å­—å·¦å³ï¼‰
- improved_prompt: ä¼˜åŒ–åçš„å®Œæ•´ Promptï¼ˆå¯ç›´æ¥ä½¿ç”¨ï¼‰
- enhancement_techniques: ä½¿ç”¨çš„ä¼˜åŒ–æŠ€æœ¯åˆ—è¡¨
- keywords_added: æ–°å¢çš„å…³é”®è¯åˆ—è¡¨
- structure_applied: åº”ç”¨çš„æ¡†æ¶åç§°

**é‡è¦**ï¼šimproved_prompt åº”è¯¥æ˜¯ä¸€ä¸ªå®Œæ•´çš„ã€å¯ä»¥ç›´æ¥å¤åˆ¶ä½¿ç”¨çš„é«˜è´¨é‡ Promptï¼Œä¸è¦åŒ…å«ä»»ä½•å…ƒä¿¡æ¯æˆ–è¯´æ˜ã€‚
"""
        return meta_prompt
    
    def _fallback_optimization(self, original_prompt: str, error: str) -> OptimizedPrompt:
        """å½“ä¼˜åŒ–å¤±è´¥æ—¶çš„å¤‡ç”¨æ–¹æ¡ˆ"""
        return OptimizedPrompt(
            thinking_process=f"ä¼˜åŒ–è¿‡ç¨‹ä¸­é‡åˆ°é”™è¯¯ï¼š{error}ã€‚ä»¥ä¸‹æ˜¯åŸºç¡€ä¼˜åŒ–ç‰ˆæœ¬ã€‚",
            improved_prompt=f"""
è¯·ä»¥ä¸“ä¸šçš„æ€åº¦å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š

{original_prompt}

è¦æ±‚ï¼š
1. è¾“å‡ºå†…å®¹åº”è¯¥æ¸…æ™°ã€å‡†ç¡®ã€å®Œæ•´
2. ä½¿ç”¨æ°å½“çš„æ ¼å¼ç»„ç»‡ä¿¡æ¯
3. æ³¨é‡ç»†èŠ‚å’Œä¸“ä¸šæ€§
4. å¦‚æœ‰éœ€è¦ï¼Œè¯·å±•ç¤ºä½ çš„æ€è€ƒè¿‡ç¨‹
""",
            enhancement_techniques=["åŸºç¡€ç»“æ„åŒ–", "æ·»åŠ é€šç”¨è¦æ±‚"],
            keywords_added=[],
            structure_applied="ç®€å•ä¼˜åŒ–"
        )
    
    def compare_results(self, original_prompt: str, optimized_prompt: str, 
                       test_query: Optional[str] = None) -> tuple[str, str]:
        """
        A/B å¯¹æ¯”æµ‹è¯•ï¼šåˆ†åˆ«è¿è¡ŒåŸå§‹å’Œä¼˜åŒ–åçš„ Prompt
        
        Args:
            original_prompt: åŸå§‹ Prompt
            optimized_prompt: ä¼˜åŒ–åçš„ Prompt
            test_query: å¯é€‰çš„æµ‹è¯•æŸ¥è¯¢ï¼ˆå¦‚æœ Prompt æœ¬èº«ä¸æ˜¯ç›´æ¥çš„é—®é¢˜ï¼‰
            
        Returns:
            (åŸå§‹ç»“æœ, ä¼˜åŒ–åç»“æœ)
        """
        try:
            # è¿è¡ŒåŸå§‹ Prompt
            response_original = self.llm.invoke(original_prompt)
            result_original = response_original.content
            
            # è¿è¡Œä¼˜åŒ–åçš„ Prompt
            response_optimized = self.llm.invoke(optimized_prompt)
            result_optimized = response_optimized.content
            
            return result_original, result_optimized
            
        except Exception as e:
            return f"è¿è¡Œå¤±è´¥: {str(e)}", f"è¿è¡Œå¤±è´¥: {str(e)}"
    
    def batch_optimize(self, prompts: list[str], 
                       scene_desc: str = "é€šç”¨",
                       optimization_mode: str = "é€šç”¨å¢å¼º (General)") -> list[OptimizedPrompt]:
        """
        æ‰¹é‡ä¼˜åŒ–å¤šä¸ª Prompt
        
        Args:
            prompts: Prompt åˆ—è¡¨
            scene_desc: åœºæ™¯æè¿°
            optimization_mode: ä¼˜åŒ–æ¨¡å¼
            
        Returns:
            ä¼˜åŒ–ç»“æœåˆ—è¡¨
        """
        results = []
        for prompt in prompts:
            result = self.optimize(prompt, scene_desc, optimization_mode)
            results.append(result)
        return results


# ä¾¿æ·å‡½æ•°
def quick_optimize(user_prompt: str, 
                   api_key: Optional[str] = None,
                   scene: str = "é€šç”¨",
                   mode: str = "é€šç”¨å¢å¼º (General)",
                   provider: str = "nvidia") -> OptimizedPrompt:
    """
    å¿«é€Ÿä¼˜åŒ–å‡½æ•°ï¼Œé€‚åˆç®€å•è°ƒç”¨
    
    Example:
        result = quick_optimize("å†™ä¸ªè´ªåƒè›‡", scene="Pythonåˆå­¦è€…", mode="ä»£ç ç”Ÿæˆ (Coding)")
        print(result.improved_prompt)
    """
    optimizer = PromptOptimizer(api_key=api_key, provider=provider)
    return optimizer.optimize(user_prompt, scene, mode)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    from dotenv import load_dotenv
    load_dotenv()
    
    # ç¤ºä¾‹æµ‹è¯•
    test_prompt = "å†™ä¸ªè´ªåƒè›‡æ¸¸æˆ"
    
    optimizer = PromptOptimizer()
    result = optimizer.optimize(
        test_prompt, 
        scene_desc="Python, ç»™å°å­©å­¦ç¼–ç¨‹ç”¨",
        optimization_mode="ä»£ç ç”Ÿæˆ (Coding)"
    )
    
    print("=" * 50)
    print("ä¼˜åŒ–æ€è€ƒè¿‡ç¨‹ï¼š")
    print(result.thinking_process)
    print("\n" + "=" * 50)
    print("ä¼˜åŒ–åçš„ Promptï¼š")
    print(result.improved_prompt)
    print("\n" + "=" * 50)
    print("ä½¿ç”¨çš„æŠ€æœ¯ï¼š", result.enhancement_techniques)
    print("æ–°å¢å…³é”®è¯ï¼š", result.keywords_added)
