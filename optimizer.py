"""
Prompt ä¼˜åŒ–æ ¸å¿ƒæ¨¡å—
å®ç°è‡ªåŠ¨åŒ–çš„ Prompt ç”Ÿæˆã€ä¼˜åŒ–å’Œè¯„ä¼°
"""
import os
import time
from typing import Optional, Literal
from langchain_openai import ChatOpenAI
from langchain_nvidia_ai_endpoints import ChatNVIDIA
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
import json
import random
from templates import get_strategy_by_scene, OPTIMIZATION_PRINCIPLES
from metrics import MetricsCalculator


class OptimizedPrompt(BaseModel):
    """ä¼˜åŒ–åçš„ Prompt ç»“æ„ï¼ˆç”Ÿæˆä»»åŠ¡ï¼‰"""
    thinking_process: str = Field(description="ä¼˜åŒ–æ—¶çš„æ€è€ƒè¿‡ç¨‹ï¼Œåˆ†æåŸå§‹ Prompt çš„é—®é¢˜å’Œæ”¹è¿›æ–¹å‘")
    improved_prompt: str = Field(description="ä¼˜åŒ–åçš„å®Œæ•´ Promptï¼Œå¯ç›´æ¥ä½¿ç”¨")
    enhancement_techniques: list[str] = Field(description="ä½¿ç”¨çš„ä¼˜åŒ–æŠ€æœ¯ï¼Œå¦‚ï¼šå¢åŠ è§’è‰²è®¾å®šã€æ˜ç¡®è¾“å‡ºæ ¼å¼ç­‰")
    keywords_added: list[str] = Field(description="æ–°å¢çš„å…³é”®è¯å’Œä¸“ä¸šæœ¯è¯­")
    structure_applied: str = Field(description="åº”ç”¨çš„ Prompt æ¡†æ¶åç§°ï¼Œå¦‚ CO-STARã€BROKE ç­‰")


class ClassificationPrompt(BaseModel):
    """ä¼˜åŒ–åçš„åˆ†ç±»ä»»åŠ¡ Prompt ç»“æ„"""
    thinking_process: str = Field(description="ä¼˜åŒ–åˆ†æè¿‡ç¨‹")
    role_definition: str = Field(description="è§’è‰²è®¾å®šï¼Œä¾‹å¦‚ï¼šä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„æƒ…æ„Ÿåˆ†æä¸“å®¶")
    label_definitions: dict[str, str] = Field(description="æ ‡ç­¾è¯¦ç»†å®šä¹‰å­—å…¸ï¼ŒKeyæ˜¯æ ‡ç­¾åï¼ŒValueæ˜¯è¯¦ç»†åˆ¤æ–­æ ‡å‡†")
    few_shot_examples: list[dict[str, str]] = Field(description="è‡ªåŠ¨åˆæˆçš„3-5ä¸ªé«˜è´¨é‡å°‘æ ·æœ¬ç¤ºä¾‹")
    reasoning_guidance: str = Field(description="æ€ç»´é“¾å¼•å¯¼è¯­ï¼Œå¸®åŠ©æ¨¡å‹é€æ­¥åˆ†æ")
    output_format: str = Field(description="ä¸¥æ ¼çš„è¾“å‡ºæ ¼å¼è¦æ±‚")
    final_prompt: str = Field(description="ç»„åˆå¥½çš„æœ€ç»ˆå¯ç”¨çš„å®Œæ•´ Prompt")

class SummarizationPrompt(BaseModel):
    """ä¼˜åŒ–åçš„æ‘˜è¦ä»»åŠ¡ Prompt ç»“æ„"""
    thinking_process: str = Field(description="ä¼˜åŒ–åˆ†æè¿‡ç¨‹")
    role_setting: str = Field(description="è§’è‰²è®¾å®šï¼Œå¦‚ï¼šä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æŠ€æœ¯æ–‡æ¡£ç¼–å†™ä¸“å®¶")
    extraction_rules: list[str] = Field(description="å…·ä½“çš„æå–è§„åˆ™ï¼Œå¦‚ï¼šå¿…é¡»ä¿ç•™æ‰€æœ‰æ•°å­—ã€æ—¥æœŸå’Œè´£ä»»äºº")
    negative_constraints: list[str] = Field(description="è´Ÿé¢çº¦æŸï¼Œæ˜ç¡®å‘Šè¯‰æ¨¡å‹ä¸è¦åšä»€ä¹ˆ")
    format_template: str = Field(description="ä¸¥æ ¼çš„è¾“å‡ºæ ¼å¼æ¨¡æ¿ï¼Œé€šå¸¸åŒ…å«Markdownç»“æ„")
    step_by_step_guide: str = Field(description="ç»™æ¨¡å‹çš„æ€è€ƒæ­¥éª¤ï¼Œå¦‚ï¼šé€šè¯»å…¨æ–‡ -> æ ‡è®°é‡ç‚¹ -> æ’°å†™åˆç¨¿")
    focus_areas: list[str] = Field(description="æ ¸å¿ƒå…³æ³¨ç‚¹ï¼Œé’ˆå¯¹ç”¨æˆ·éœ€æ±‚å¼ºè°ƒçš„ä¿¡æ¯")
    final_prompt: str = Field(description="ç»„åˆå¥½çš„æœ€ç»ˆå¯ç”¨çš„æ‘˜è¦ Promptï¼Œ{{text}}å ä½ç¬¦")


class TranslationPrompt(BaseModel):
    """ä¼˜åŒ–åçš„ç¿»è¯‘ä»»åŠ¡ Prompt ç»“æ„"""
    thinking_process: str = Field(description="ä¼˜åŒ–åˆ†æè¿‡ç¨‹")
    role_definition: str = Field(description="è§’è‰²è®¾å®šï¼Œä¾‹å¦‚ï¼šä½ æ˜¯ç²¾é€šä¸­è‹±åŒè¯­çš„ã€Šè‡ªç„¶ã€‹æ‚å¿—ç¼–è¾‘")
    style_guidelines: list[str] = Field(description="é£æ ¼æŒ‡å—åˆ—è¡¨ï¼Œä¾‹å¦‚ï¼š['ä¿æŒå­¦æœ¯ä¸¥è°¨', 'é¿å…å£è¯­åŒ–', 'ä¿ç•™è¢«åŠ¨è¯­æ€']")
    glossary_section: str = Field(description="æ„å»ºçš„æœ¯è¯­å¯¹ç…§è¡¨éƒ¨åˆ†ï¼Œå¦‚æœæ²¡æœ‰åˆ™ç•™ç©º")
    workflow_steps: str = Field(description="ç¿»è¯‘çš„å·¥ä½œæµæŒ‡ä»¤ï¼Œæ¨èä½¿ç”¨'ç›´è¯‘-åæ€-æ¶¦è‰²'ä¸‰æ­¥æ³•")
    final_prompt: str = Field(description="æœ€ç»ˆç»„åˆå¥½çš„ Prompt æ¨¡æ¿ï¼Œå¾…ç¿»è¯‘æ–‡æœ¬ç”¨ {{text}} å ä½")


class SearchSpace(BaseModel):
    """éšæœºæœç´¢çš„å˜é‡ç©ºé—´"""
    roles: list[str] = Field(description="5ä¸ªé€‚åˆè¯¥ä»»åŠ¡çš„ä¸åŒè§’è‰²è®¾å®š")
    styles: list[str] = Field(description="5ç§ä¸åŒçš„å›ç­”é£æ ¼")
    techniques: list[str] = Field(description="3ç§ä¸åŒçš„æç¤ºæŠ€å·§ï¼Œå¦‚: step-by-step, few-shot, direct")


class SearchResult(BaseModel):
    """å•æ¬¡æœç´¢çš„ç»“æœ"""
    iteration_id: int = Field(description="è¿­ä»£ç¼–å·")
    role: str = Field(description="ä½¿ç”¨çš„è§’è‰²")
    style: str = Field(description="ä½¿ç”¨çš„é£æ ¼")
    technique: str = Field(description="ä½¿ç”¨çš„æŠ€å·§")
    full_prompt: str = Field(description="å®Œæ•´çš„ç»„åˆ Prompt")
    avg_score: float = Field(description="åœ¨éªŒè¯é›†ä¸Šçš„å¹³å‡å¾—åˆ†")
    task_type: str = Field(description="ä»»åŠ¡ç±»å‹ï¼šclassification/summarization/translation")


class PromptOptimizer:
    """Prompt è‡ªåŠ¨ä¼˜åŒ–å™¨"""
    
    def __init__(self, 
                 api_key: Optional[str] = None, 
                 model: str = "meta/llama-3.1-405b-instruct", 
                 base_url: Optional[str] = None,
                 provider: Literal["openai", "nvidia"] = "nvidia",
                 temperature: float = 0.7,
                 top_p: float = 0.7,
                 max_tokens: int = 2048):
        """
        åˆå§‹åŒ–ä¼˜åŒ–å™¨
        
        Args:
            api_key: API Keyï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            base_url: API base URL
            provider: API æä¾›å•† ("openai" æˆ– "nvidia")
            temperature: æ¸©åº¦å‚æ•°
            top_p: Top-p é‡‡æ ·å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
        """
        self.provider = provider
        self.model = model
        
        # æ ¹æ®æä¾›å•†åˆå§‹åŒ– LLM
        if provider == "nvidia":
            if api_key:
                os.environ["NVIDIA_API_KEY"] = api_key
            
            llm_params = {
                "model": model,
                "temperature": temperature,
                "top_p": top_p,
                "max_tokens": max_tokens
            }
            if base_url:
                llm_params["base_url"] = base_url
            
            self.llm = ChatNVIDIA(**llm_params)
            
        else:  # openai
            if api_key:
                os.environ["OPENAI_API_KEY"] = api_key
            
            llm_params = {
                "model": model,
                "temperature": temperature,
                "max_tokens": max_tokens
            }
            if base_url:
                llm_params["base_url"] = base_url
            
            self.llm = ChatOpenAI(**llm_params)
    
    def optimize(self, 
                 user_prompt: str, 
                 scene_desc: str = "é€šç”¨",
                 optimization_mode: str = "é€šç”¨å¢å¼º (General)") -> OptimizedPrompt:
        """
        æ ¸å¿ƒä¼˜åŒ–å‡½æ•°
        
        Args:
            user_prompt: ç”¨æˆ·è¾“å…¥çš„åŸå§‹ Prompt
            scene_desc: åœºæ™¯æè¿°
            optimization_mode: ä¼˜åŒ–æ¨¡å¼
            
        Returns:
            OptimizedPrompt: ä¼˜åŒ–åçš„ç»“æ„åŒ– Prompt
        """
        # æ‰“å°ä¼˜åŒ–å¼€å§‹ä¿¡æ¯
        print(f"\n{'='*60}")
        print(f"âš™ï¸  å¼€å§‹ Prompt ä¼˜åŒ–")
        print(f"{'='*60}")
        print(f"ğŸ”Œ API æä¾›å•†: {self.provider.upper()}")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {self.model}")
        print(f"ğŸ¯ ä¼˜åŒ–æ¨¡å¼: {optimization_mode}")
        print(f"ğŸ“ åŸå§‹ Prompt: {user_prompt[:50]}{'...' if len(user_prompt) > 50 else ''}")
        if scene_desc:
            print(f"ğŸ¬ åœºæ™¯æè¿°: {scene_desc[:50]}{'...' if len(scene_desc) > 50 else ''}")
        print(f"{'='*60}\n")
        
        # è·å–åœºæ™¯å¯¹åº”çš„ä¼˜åŒ–ç­–ç•¥
        strategy = get_strategy_by_scene(optimization_mode)
        
        # æ„å»º Meta-Prompt
        system_prompt = self._build_meta_prompt(strategy, scene_desc)
        
        # æ„å»ºæ¶ˆæ¯é“¾
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "ç”¨æˆ·åŸå§‹ Promptï¼š{input}\n\nåœºæ™¯è¡¥å……è¯´æ˜ï¼š{scene}")
        ])
        
        # æ‰§è¡Œä¼˜åŒ–
        try:
            print("ğŸ“¤ æ­£åœ¨è°ƒç”¨ API...")
            
            # æ„å»ºå®Œæ•´æç¤º
            messages = prompt_template.format_messages(
                input=user_prompt,
                scene=scene_desc if scene_desc else "æ— ç‰¹æ®Šè¯´æ˜"
            )
            
            print(f"ğŸ’¬ æ¶ˆæ¯é•¿åº¦: {len(str(messages))} å­—ç¬¦")
            
            # è°ƒç”¨ LLM
            if self.provider == "openai":
                # OpenAI æ”¯æŒ JSON mode
                print("ğŸ”§ ä½¿ç”¨ OpenAI JSON mode")
                response = self.llm.invoke(
                    messages,
                    response_format={"type": "json_object"}
                )
                time.sleep(0.5)  # API è°ƒç”¨å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡è¿‡å¿«
            else:
                # NVIDIA ä½¿ç”¨æ™®é€šè°ƒç”¨
                print("ğŸ”§ ä½¿ç”¨ NVIDIA æ ‡å‡†è°ƒç”¨")
                response = self.llm.invoke(messages)
                time.sleep(0.5)  # API è°ƒç”¨å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡è¿‡å¿«
            
            # è§£æç»“æœ
            content = response.content
            print(f"ğŸ“¥ æ”¶åˆ°å“åº”ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
            print(f"ğŸ“„ å“åº”å‰100å­—ç¬¦: {content[:100]}...")
            
            # å°è¯•æå– JSONï¼ˆå¯èƒ½åŒ…å«åœ¨ markdown ä»£ç å—ä¸­ï¼‰
            if "```json" in content:
                print("ğŸ” æ£€æµ‹åˆ° JSON ä»£ç å—ï¼Œæ­£åœ¨æå–...")
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                print("ğŸ” æ£€æµ‹åˆ°ä»£ç å—ï¼Œæ­£åœ¨æå–...")
                content = content.split("```")[1].split("```")[0].strip()
            
            print("âš™ï¸ æ­£åœ¨è§£æ JSON...")
            result_dict = json.loads(content)
            
            print("âœ… JSON è§£ææˆåŠŸ")
            print("ğŸ”¨ æ­£åœ¨éªŒè¯æ•°æ®ç»“æ„...")
            optimized = OptimizedPrompt(**result_dict)
            
            print("âœ… ä¼˜åŒ–å®Œæˆï¼")
            print(f"{'='*60}\n")
            
            return optimized
            
        except Exception as e:
            # é”™è¯¯å¤„ç†ï¼šè¯¦ç»†è®°å½•åˆ°ç»ˆç«¯
            print(f"\nâŒ ä¼˜åŒ–å¤±è´¥ï¼")
            print(f"{'='*60}")
            
            error_msg = str(e)
            print(f"ğŸ› é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"ğŸ“ é”™è¯¯è¯¦æƒ…: {error_msg[:500]}")
            
            # å¦‚æœæ˜¯ Pydantic éªŒè¯é”™è¯¯ï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯
            if "validation" in error_msg.lower() or "Field required" in error_msg:
                print("\nâš ï¸ è¿™æ˜¯æ•°æ®ç»“æ„éªŒè¯é”™è¯¯ï¼Œå¯èƒ½åŸå› ï¼š")
                print("   1. æ¨¡å‹è¿”å›çš„ JSON æ ¼å¼ä¸ç¬¦åˆè¦æ±‚")
                print("   2. ç¼ºå°‘å¿…éœ€çš„å­—æ®µï¼ˆthinking_process, improved_prompt ç­‰ï¼‰")
                print("   3. æ¨¡å‹å¯èƒ½ä¸æ”¯æŒ JSON æ ¼å¼è¾“å‡º")
                print("\nğŸ’¡ å»ºè®®ï¼šå°è¯•æ›´æ¢æ¨¡å‹ï¼Œæ¨èä½¿ç”¨ meta/llama-3.1-405b-instruct")
            
            # æ‰“å°å®Œæ•´å †æ ˆ
            import traceback
            print(f"\nğŸ“„ å®Œæ•´å †æ ˆä¿¡æ¯ï¼š")
            traceback.print_exc()
            print(f"{'='*60}\n")
            
            # æ ¹æ®é”™è¯¯ç±»å‹æŠ›å‡ºæ˜ç¡®çš„å¼‚å¸¸
            if "404" in error_msg:
                raise Exception(f"API è°ƒç”¨å¤±è´¥ (404): è¯·æ£€æŸ¥ API Key æ˜¯å¦æœ‰æ•ˆï¼Œæˆ–æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®ã€‚è¯¦ç»†ä¿¡æ¯ï¼š{error_msg[:200]}")
            elif "401" in error_msg or "Unauthorized" in error_msg:
                raise Exception(f"API Key æ— æ•ˆæˆ–å·²è¿‡æœŸã€‚è¯·æ£€æŸ¥æ‚¨çš„ API Key é…ç½®ã€‚")
            elif "rate_limit" in error_msg.lower():
                raise Exception(f"API è¯·æ±‚é¢‘ç‡è¶…é™ï¼Œè¯·ç¨åå†è¯•ã€‚")
            else:
                raise Exception(f"ä¼˜åŒ–å¤±è´¥: {error_msg[:300]}")
    
    def optimize_classification(self,
                               task_description: str,
                               labels: list[str],
                               example_texts: Optional[list[str]] = None) -> ClassificationPrompt:
        """
        é’ˆå¯¹åˆ†ç±»ä»»åŠ¡çš„ä¼˜åŒ–å‡½æ•°
        
        Args:
            task_description: åˆ†ç±»ä»»åŠ¡æè¿°ï¼Œå¦‚ "åˆ¤æ–­ç”¨æˆ·è¯„è®ºçš„æƒ…æ„Ÿå€¾å‘"
            labels: ç›®æ ‡æ ‡ç­¾åˆ—è¡¨ï¼Œå¦‚ ["Positive", "Negative", "Neutral"]
            example_texts: å¯é€‰çš„ç¤ºä¾‹æ–‡æœ¬ï¼Œç”¨äºç”Ÿæˆ Few-Shot æ ·æœ¬
            
        Returns:
            ClassificationPrompt: ä¼˜åŒ–åçš„åˆ†ç±» Prompt
        """
        print(f"\n{'='*60}")
        print(f"ğŸ·ï¸  å¼€å§‹åˆ†ç±»ä»»åŠ¡ Prompt ä¼˜åŒ–")
        print(f"{'='*60}")
        print(f"ğŸ”Œ API æä¾›å•†: {self.provider.upper()}")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {self.model}")
        print(f"ğŸ“ ä»»åŠ¡æè¿°: {task_description[:50]}...")
        print(f"ğŸ·ï¸  ç›®æ ‡æ ‡ç­¾: {', '.join(labels)}")
        print(f"{'='*60}\n")
        
        # æ„å»ºåˆ†ç±»ä»»åŠ¡ä¸“ç”¨çš„ Meta-Prompt
        # ä¸ä½¿ç”¨ f-stringï¼Œé¿å…èŠ±æ‹¬å·å†²çª
        system_prompt = """
ä½ æ˜¯ä¸€ä¸ªä¸“é—¨æ„å»º AI æ–‡æœ¬åˆ†ç±»å™¨çš„ä¸“å®¶ã€‚ä½ çš„ç›®æ ‡æ˜¯ç¼–å†™ä¸€ä¸ª**é«˜ç²¾åº¦**çš„åˆ†ç±» Promptã€‚

**ä»»åŠ¡æè¿°**ï¼šTASK_DESCRIPTION
**ç›®æ ‡æ ‡ç­¾**ï¼šTARGET_LABELS

**ä½ çš„ä»»åŠ¡**ï¼š

1. **æ ‡ç­¾æ¶ˆæ­§ (Label Disambiguation)**
   - ä¸ºæ¯ä¸ªæ ‡ç­¾ç¼–å†™æ¸…æ™°ã€å…·ä½“çš„å®šä¹‰
   - æ˜ç¡®è¾¹ç•Œæƒ…å†µï¼ˆEdge Casesï¼‰å’Œåˆ¤æ–­æ ‡å‡†
   - è¯´æ˜ä»€ä¹ˆæ ·çš„æ–‡æœ¬å±äºè¯¥æ ‡ç­¾ï¼Œä»€ä¹ˆä¸å±äº

2. **æ ·æœ¬åˆæˆ (Few-Shot Generation)**
   - æ ¹æ®æ ‡ç­¾å®šä¹‰ï¼Œåˆ›ä½œ 3-5 ä¸ªå…¸å‹çš„é«˜è´¨é‡ç¤ºä¾‹
   - ç¤ºä¾‹å¿…é¡»è¦†ç›–ä¸åŒæ ‡ç­¾ï¼Œå…·æœ‰ä»£è¡¨æ€§
   - æ¯ä¸ªç¤ºä¾‹åŒ…å« inputï¼ˆè¾“å…¥æ–‡æœ¬ï¼‰å’Œ labelï¼ˆå¯¹åº”æ ‡ç­¾ï¼‰

3. **æ€ç»´é“¾è®¾è®¡ (Chain of Thought)**
   - è®¾è®¡å¼•å¯¼è¯­ï¼Œè®©æ¨¡å‹å…ˆåˆ†æç‰¹å¾ï¼Œå†ç»™å‡ºåˆ†ç±»ç»“æœ
   - å¯¹äºå¤æ‚åˆ†ç±»ä»»åŠ¡ï¼Œä½¿ç”¨ "Let's think step by step"

4. **æ ¼å¼é”å®š (Output Format)**
   - æ˜ç¡®è¦æ±‚æ¨¡å‹åªè¾“å‡ºç‰¹å®šæ ¼å¼ï¼ˆå¦‚ JSONï¼‰
   - ç¦æ­¢æ¨¡å‹è¾“å‡ºå¤šä½™çš„è§£é‡Šæˆ–åºŸè¯
   - ç¡®ä¿è¾“å‡ºå¯ä»¥è¢«ä»£ç è½»æ¾è§£æ

5. **è§’è‰²è®¾å®š**
   - ä¸ºåˆ†ç±»å™¨è®¾å®šä¸€ä¸ªä¸“ä¸šçš„è§’è‰²èº«ä»½
   - å¢å¼ºæ¨¡å‹å¯¹ä»»åŠ¡çš„ç†è§£å’Œæ‰§è¡Œå‡†ç¡®åº¦

**è¾“å‡ºè¦æ±‚**ï¼š
è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- thinking_process: ä½ çš„ä¼˜åŒ–æ€è€ƒè¿‡ç¨‹
- role_definition: è§’è‰²è®¾å®šæè¿°
- label_definitions: æ ‡ç­¾å®šä¹‰å­—å…¸ï¼ˆé”®ä¸ºæ ‡ç­¾åï¼Œå€¼ä¸ºè¯¦ç»†å®šä¹‰ï¼‰
- few_shot_examples: ç¤ºä¾‹åˆ—è¡¨ï¼ˆæ¯ä¸ªåŒ…å« input å’Œ label å­—æ®µï¼‰
- reasoning_guidance: æ€ç»´é“¾å¼•å¯¼è¯­
- output_format: è¾“å‡ºæ ¼å¼è¦æ±‚è¯´æ˜
- final_prompt: å®Œæ•´çš„ã€å¯ç›´æ¥ä½¿ç”¨çš„åˆ†ç±» Prompt
- enhancement_techniques: ä½¿ç”¨çš„ä¼˜åŒ–æŠ€æœ¯åˆ—è¡¨

**å…³é”®è¦æ±‚ - final_prompt å¿…é¡»åŒ…å«å ä½ç¬¦**ï¼š
- final_prompt å¿…é¡»æ˜¯ä¸€ä¸ªå®Œæ•´çš„ã€ç»“æ„æ¸…æ™°çš„ã€å¯ä»¥ç›´æ¥å¤åˆ¶ä½¿ç”¨çš„åˆ†ç±» Prompt
- **å¿…é¡»åœ¨ Prompt ä¸­æ˜ç¡®æ ‡æ³¨å¾…åˆ†ç±»æ–‡æœ¬çš„ä½ç½®**ï¼Œä½¿ç”¨ä»¥ä¸‹ä»»ä¸€å ä½ç¬¦æ ¼å¼ï¼š
  * [å¾…åˆ†ç±»æ–‡æœ¬] ï¼ˆæ¨èï¼‰
  * {{text}} ï¼ˆä¸¤ä¸ªèŠ±æ‹¬å·ï¼‰
  * [è¾“å…¥è¯„è®º]
  * [å¾…å¤„ç†æ–‡æœ¬]
- å ä½ç¬¦åº”è¯¥æ”¾åœ¨åˆç†çš„ä½ç½®ï¼Œæ¯”å¦‚ï¼š
  * "è¯„è®ºå†…å®¹ï¼š[å¾…åˆ†ç±»æ–‡æœ¬]"
  * "è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼š[å¾…åˆ†ç±»æ–‡æœ¬]"
  * "æ–‡æœ¬ï¼š{{text}}"
- **ä¸è¦**åªè¯´"åˆ†æè¿™ä¸ªè¯„è®º"æˆ–"åˆ¤æ–­æƒ…æ„Ÿ"è€Œä¸æä¾›å…·ä½“çš„æ’å…¥ä½ç½®
- final_prompt å¿…é¡»æ˜¯å¯ä»¥é€šè¿‡ç®€å•çš„å­—ç¬¦ä¸²æ›¿æ¢å°±èƒ½ä½¿ç”¨çš„æ¨¡æ¿

**ç¤ºä¾‹æ­£ç¡®æ ¼å¼**ï¼š
```
ä½ æ˜¯ä¸“ä¸šçš„æƒ…æ„Ÿåˆ†æå¸ˆã€‚
æ ‡ç­¾å®šä¹‰ï¼š...
ç¤ºä¾‹ï¼š...
ç°åœ¨è¯·åˆ†æä»¥ä¸‹è¯„è®ºçš„æƒ…æ„Ÿå€¾å‘ï¼š
[å¾…åˆ†ç±»æ–‡æœ¬]
è¯·è¾“å‡ºæ ‡ç­¾åç§°å³å¯ã€‚
```

**ç¤ºä¾‹é”™è¯¯æ ¼å¼ï¼ˆä¸è¦ç”Ÿæˆè¿™æ ·çš„ï¼‰**ï¼š
```
ä½ æ˜¯ä¸“ä¸šçš„æƒ…æ„Ÿåˆ†æå¸ˆã€‚
æ ‡ç­¾å®šä¹‰ï¼š...
ç¤ºä¾‹ï¼š...
è®©æˆ‘ä»¬åˆ†æè¯„è®ºçš„æƒ…æ„Ÿå€¾å‘ã€‚ï¼ˆâŒ ç¼ºå°‘æ˜ç¡®çš„æ–‡æœ¬æ’å…¥ä½ç½®ï¼‰
```
"""
        
        # æ‰‹åŠ¨æ›¿æ¢å˜é‡
        system_prompt = system_prompt.replace("TASK_DESCRIPTION", task_description)
        system_prompt = system_prompt.replace("TARGET_LABELS", ', '.join(labels))
        
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "è¯·ä¸ºè¿™ä¸ªåˆ†ç±»ä»»åŠ¡ç”Ÿæˆä¼˜åŒ–çš„ Promptã€‚")
        ])
        
        try:
            print("ğŸ“¤ æ­£åœ¨è°ƒç”¨ API...")
            
            messages = prompt_template.format_messages()
            print(f"ğŸ’¬ æ¶ˆæ¯é•¿åº¦: {len(str(messages))} å­—ç¬¦")
            
            # è°ƒç”¨ LLM
            if self.provider == "openai":
                print("ğŸ”§ ä½¿ç”¨ OpenAI JSON mode")
                response = self.llm.invoke(
                    messages,
                    response_format={"type": "json_object"}
                )
                time.sleep(0.5)  # API è°ƒç”¨å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡è¿‡å¿«
            else:
                print("ğŸ”§ ä½¿ç”¨ NVIDIA æ ‡å‡†è°ƒç”¨")
                response = self.llm.invoke(messages)
                time.sleep(0.5)  # API è°ƒç”¨å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡è¿‡å¿«
            
            # è§£æç»“æœ
            content = response.content
            print(f"ğŸ“¥ æ”¶åˆ°å“åº”ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
            
            # æå– JSON
            if "```json" in content:
                print("ğŸ” æ£€æµ‹åˆ° JSON ä»£ç å—ï¼Œæ­£åœ¨æå–...")
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                print("ğŸ” æ£€æµ‹åˆ°ä»£ç å—ï¼Œæ­£åœ¨æå–...")
                content = content.split("```")[1].split("```")[0].strip()
            
            print("âš™ï¸ æ­£åœ¨è§£æ JSON...")
            result_dict = json.loads(content)
            
            print("âœ… JSON è§£ææˆåŠŸ")
            print("ğŸ”¨ æ­£åœ¨éªŒè¯æ•°æ®ç»“æ„...")
            optimized = ClassificationPrompt(**result_dict)
            
            print("âœ… åˆ†ç±» Prompt ä¼˜åŒ–å®Œæˆï¼")
            print(f"{'='*60}\n")
            
            return optimized
            
        except Exception as e:
            # é”™è¯¯å¤„ç†
            print(f"\nâŒ åˆ†ç±»ä¼˜åŒ–å¤±è´¥ï¼")
            print(f"{'='*60}")
            
            error_msg = str(e)
            print(f"ğŸ› é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"ğŸ“ é”™è¯¯è¯¦æƒ…: {error_msg[:500]}")
            
            import traceback
            print(f"\nğŸ“„ å®Œæ•´å †æ ˆä¿¡æ¯ï¼š")
            traceback.print_exc()
            print(f"{'='*60}\n")
            
            # æŠ›å‡ºå¼‚å¸¸
            if "404" in error_msg:
                raise Exception(f"API è°ƒç”¨å¤±è´¥ (404): è¯·æ£€æŸ¥ API Key æ˜¯å¦æœ‰æ•ˆï¼Œæˆ–æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®ã€‚")
            elif "401" in error_msg or "Unauthorized" in error_msg:
                raise Exception(f"API Key æ— æ•ˆæˆ–å·²è¿‡æœŸã€‚")
            else:
                raise Exception(f"åˆ†ç±»ä¼˜åŒ–å¤±è´¥: {error_msg[:300]}")
    
    def optimize_summarization(self,
                              task_description: str,
                              source_type: str,
                              target_audience: str,
                              focus_points: str,
                              length_constraint: Optional[str] = None) -> SummarizationPrompt:
        """
        é’ˆå¯¹æ‘˜è¦ä»»åŠ¡çš„ä¼˜åŒ–å‡½æ•°
        
        Args:
            task_description: æ‘˜è¦ä»»åŠ¡æè¿°ï¼Œå¦‚ "æ€»ç»“æŠ€æœ¯ä¼šè®®çš„æ ¸å¿ƒå†³ç­–"
            source_type: æºæ–‡æœ¬ç±»å‹ï¼Œå¦‚ "ä¼šè®®è®°å½•"ã€"å­¦æœ¯è®ºæ–‡"ã€"æ–°é—»æŠ¥é“"
            target_audience: ç›®æ ‡è¯»è€…ï¼Œå¦‚ "æŠ€æœ¯ç»ç†"ã€"æ™®é€šç”¨æˆ·"
            focus_points: æ ¸å¿ƒå…³æ³¨ç‚¹ï¼Œå¦‚ "è¡ŒåŠ¨è®¡åˆ’å’Œè´Ÿè´£äºº"
            length_constraint: ç¯‡å¹…é™åˆ¶ï¼Œå¦‚ "100å­—ä»¥å†…"ã€"3-5ä¸ªè¦ç‚¹"
            
        Returns:
            SummarizationPrompt: ä¼˜åŒ–åçš„æ‘˜è¦ Prompt
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“ å¼€å§‹æ‘˜è¦ä»»åŠ¡ Prompt ä¼˜åŒ–")
        print(f"{'='*60}")
        print(f"ğŸ”Œ API æä¾›å•†: {self.provider.upper()}")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {self.model}")
        print(f"ğŸ“ ä»»åŠ¡æè¿°: {task_description[:50]}...")
        print(f"ğŸ“„ æºæ–‡æœ¬ç±»å‹: {source_type}")
        print(f"ğŸ‘¥ ç›®æ ‡å—ä¼—: {target_audience}")
        print(f"ğŸ¯ å…³æ³¨ç‚¹: {focus_points[:50]}...")
        if length_constraint:
            print(f"ğŸ“ ç¯‡å¹…é™åˆ¶: {length_constraint}")
        print(f"{'='*60}\n")
        
        # æ„å»ºæ‘˜è¦ä»»åŠ¡ä¸“ç”¨çš„ Meta-Prompt
        length_text = f"\n**ç¯‡å¹…é™åˆ¶**ï¼š{length_constraint}" if length_constraint else ""
        
        system_prompt = f"""
ä½ æ˜¯ä¸€ä½ç²¾é€šä¿¡æ¯å‹ç¼©å’Œæ‘˜è¦æ’°å†™çš„ Prompt Engineering ä¸“å®¶ã€‚
ç”¨æˆ·çš„ç›®æ ‡æ˜¯é’ˆå¯¹ç‰¹å®šåœºæ™¯ç”Ÿæˆä¸€ä¸ª**é«˜è´¨é‡çš„æ‘˜è¦ Prompt**ã€‚

**ä»»åŠ¡ä¿¡æ¯**ï¼š
- ä»»åŠ¡æè¿°ï¼š{task_description}
- æºæ–‡æœ¬ç±»å‹ï¼š{source_type}
- ç›®æ ‡å—ä¼—ï¼š{target_audience}
- æ ¸å¿ƒå…³æ³¨ç‚¹ï¼š{focus_points}{length_text}

**ä½ çš„ä»»åŠ¡**ï¼š

1. **è§’è‰²æ²‰æµ¸ (Role Immersion)**
   - æ ¹æ®æºæ–‡æœ¬ç±»å‹å’Œç›®æ ‡å—ä¼—ï¼Œè®¾å®šæœ€åˆé€‚çš„ä¸“å®¶è§’è‰²
   - ä¾‹å¦‚ï¼šä¼šè®®è®°å½• â†’ "ä¸“ä¸šçš„ä¼šè®®çºªè¦ç§˜ä¹¦"ï¼›å­¦æœ¯è®ºæ–‡ â†’ "èµ„æ·±çš„ç§‘ç ”ç¼–è¾‘"

2. **æå–è§„åˆ™åˆ¶å®š (Extraction Rules)**
   - æ˜ç¡®å‘Šè¯‰æ¨¡å‹å¿…é¡»ä¿ç•™ä»€ä¹ˆä¿¡æ¯ï¼ˆå¦‚ï¼šæ•°å­—ã€æ—¥æœŸã€äººåã€å…³é”®å†³ç­–ï¼‰
   - é’ˆå¯¹ç”¨æˆ·çš„æ ¸å¿ƒå…³æ³¨ç‚¹ï¼Œå¼ºè°ƒç›¸å…³ä¿¡æ¯çš„é‡è¦æ€§
   - è‡³å°‘æä¾› 3-5 æ¡å…·ä½“çš„æå–è§„åˆ™

3. **è´Ÿé¢çº¦æŸ (Negative Constraints)**
   - æ˜ç¡®å‘Šè¯‰æ¨¡å‹"ä¸è¦"åšä»€ä¹ˆ
   - ä¾‹å¦‚ï¼šä¸è¦ä½¿ç”¨æ¨¡ç³Šè¯æ±‡ã€ä¸è¦é—æ¼æ•°æ®ã€ä¸è¦æ·»åŠ åŸæ–‡æ²¡æœ‰çš„ä¿¡æ¯
   - é˜²æ­¢æ¨¡å‹"å¹»è§‰"ï¼ˆç¼–é€ ç»†èŠ‚ï¼‰

4. **ç»“æ„åŒ–è¾“å‡º (Structured Format)**
   - æ ¹æ®æºæ–‡æœ¬ç±»å‹è®¾è®¡åˆé€‚çš„è¾“å‡ºæ ¼å¼
   - ä¼šè®®è®°å½• â†’ è¡¨æ ¼æˆ–åˆ†å±‚ç»“æ„ï¼ˆèƒŒæ™¯ã€å†³ç­–ã€è¡ŒåŠ¨è®¡åˆ’ï¼‰
   - æ–°é—»æŠ¥é“ â†’ TL;DR + å…³é”®äº‹å®
   - å­¦æœ¯è®ºæ–‡ â†’ ç ”ç©¶ç›®çš„ã€æ–¹æ³•ã€ç»“è®ºã€æ„ä¹‰

5. **æ€è€ƒæ­¥éª¤è®¾è®¡ (Step-by-Step Guide)**
   - ç»™æ¨¡å‹æ˜ç¡®çš„å¤„ç†æµç¨‹ï¼Œå¦‚ï¼š
     Step 1: é€šè¯»å…¨æ–‡ï¼Œæ ‡è®°å…³é”®ä¿¡æ¯
     Step 2: æ ¹æ®å…³æ³¨ç‚¹ç­›é€‰å†…å®¹
     Step 3: æŒ‰ç»“æ„ç»„ç»‡ä¿¡æ¯
     Step 4: ç²¾ç®€è¡¨è¾¾ï¼Œç¡®ä¿å‡†ç¡®

6. **å…³æ³¨ç‚¹é”šå®š (Focus Areas)**
   - å°†ç”¨æˆ·çš„æ ¸å¿ƒå…³æ³¨ç‚¹è½¬åŒ–ä¸ºå…·ä½“çš„ä¿¡æ¯ç±»åˆ«
   - åœ¨ Prompt ä¸­å¤šæ¬¡å¼ºè°ƒè¿™äº›å…³æ³¨ç‚¹çš„ä¼˜å…ˆçº§

**è¾“å‡ºè¦æ±‚**ï¼š
è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- thinking_process: ä½ çš„ä¼˜åŒ–æ€è€ƒè¿‡ç¨‹
- role_setting: è§’è‰²è®¾å®šæè¿°
- extraction_rules: æå–è§„åˆ™åˆ—è¡¨ï¼ˆè‡³å°‘3-5æ¡ï¼‰
- negative_constraints: è´Ÿé¢çº¦æŸåˆ—è¡¨ï¼ˆè‡³å°‘3æ¡ï¼‰
- format_template: è¾“å‡ºæ ¼å¼æ¨¡æ¿ï¼ˆä½¿ç”¨ Markdownï¼‰
- step_by_step_guide: å¤„ç†æ­¥éª¤è¯´æ˜
- focus_areas: æ ¸å¿ƒå…³æ³¨ç‚¹åˆ—è¡¨
- final_prompt: å®Œæ•´çš„ã€å¯ç›´æ¥ä½¿ç”¨çš„æ‘˜è¦ Promptï¼ˆç”¨ {{{{text}}}} ä½œä¸ºå¾…æ‘˜è¦æ–‡æœ¬çš„å ä½ç¬¦ï¼‰

**é‡è¦**ï¼š
- final_prompt å¿…é¡»æ˜¯ä¸€ä¸ªå®Œæ•´çš„ã€ç»“æ„æ¸…æ™°çš„ã€å¯ä»¥ç›´æ¥å¤åˆ¶ä½¿ç”¨çš„æ‘˜è¦ Prompt
- å…¶ä¸­å¾…æ‘˜è¦çš„æ–‡æœ¬ç”¨ {{{{text}}}} å ä½ç¬¦è¡¨ç¤º
- æ‰€æœ‰è§„åˆ™å’Œçº¦æŸéƒ½è¦æ•´åˆè¿› final_prompt ä¸­
"""
        
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "è¯·ä¸ºè¿™ä¸ªæ‘˜è¦ä»»åŠ¡ç”Ÿæˆä¼˜åŒ–çš„ Promptã€‚")
        ])
        
        try:
            print("ğŸ“¤ æ­£åœ¨è°ƒç”¨ API...")
            
            messages = prompt_template.format_messages()
            print(f"ğŸ’¬ æ¶ˆæ¯é•¿åº¦: {len(str(messages))} å­—ç¬¦")
            
            # è°ƒç”¨ LLM
            if self.provider == "openai":
                print("ğŸ”§ ä½¿ç”¨ OpenAI JSON mode")
                response = self.llm.invoke(
                    messages,
                    response_format={"type": "json_object"}
                )
                time.sleep(0.5)  # API è°ƒç”¨å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡è¿‡å¿«
            else:
                print("ğŸ”§ ä½¿ç”¨ NVIDIA æ ‡å‡†è°ƒç”¨")
                response = self.llm.invoke(messages)
                time.sleep(0.5)  # API è°ƒç”¨å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡è¿‡å¿«
            
            # è§£æç»“æœ
            content = response.content
            print(f"ğŸ“¥ æ”¶åˆ°å“åº”ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
            
            # æå– JSON
            if "```json" in content:
                print("ğŸ” æ£€æµ‹åˆ° JSON ä»£ç å—ï¼Œæ­£åœ¨æå–...")
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                print("ğŸ” æ£€æµ‹åˆ°ä»£ç å—ï¼Œæ­£åœ¨æå–...")
                content = content.split("```")[1].split("```")[0].strip()
            
            print("âš™ï¸ æ­£åœ¨è§£æ JSON...")
            result_dict = json.loads(content)
            
            print("âœ… JSON è§£ææˆåŠŸ")
            print("ğŸ”¨ æ­£åœ¨éªŒè¯æ•°æ®ç»“æ„...")
            optimized = SummarizationPrompt(**result_dict)
            
            print("âœ… æ‘˜è¦ Prompt ä¼˜åŒ–å®Œæˆï¼")
            print(f"{'='*60}\n")
            
            return optimized
            
        except Exception as e:
            # é”™è¯¯å¤„ç†
            print(f"\nâŒ æ‘˜è¦ä¼˜åŒ–å¤±è´¥ï¼")
            print(f"{'='*60}")
            
            error_msg = str(e)
            print(f"ğŸ› é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"ğŸ“ é”™è¯¯è¯¦æƒ…: {error_msg[:500]}")
            
            import traceback
            print(f"\nğŸ“„ å®Œæ•´å †æ ˆä¿¡æ¯ï¼š")
            traceback.print_exc()
            print(f"{'='*60}\n")
            
            # æŠ›å‡ºå¼‚å¸¸
            if "404" in error_msg:
                raise Exception(f"API è°ƒç”¨å¤±è´¥ (404): è¯·æ£€æŸ¥ API Key æ˜¯å¦æœ‰æ•ˆï¼Œæˆ–æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®ã€‚")
            elif "401" in error_msg or "Unauthorized" in error_msg:
                raise Exception(f"API Key æ— æ•ˆæˆ–å·²è¿‡æœŸã€‚")
            else:
                raise Exception(f"æ‘˜è¦ä¼˜åŒ–å¤±è´¥: {error_msg[:300]}")
    
    def optimize_translation(self,
                           source_lang: str,
                           target_lang: str,
                           domain: str,
                           tone: str,
                           user_glossary: str = "") -> TranslationPrompt:
        """
        é’ˆå¯¹ç¿»è¯‘ä»»åŠ¡çš„ä¼˜åŒ–å‡½æ•°
        
        Args:
            source_lang: æºè¯­è¨€ï¼Œå¦‚ "ä¸­æ–‡"ã€"è‹±æ–‡"
            target_lang: ç›®æ ‡è¯­è¨€
            domain: åº”ç”¨é¢†åŸŸï¼Œå¦‚ "é€šç”¨æ—¥å¸¸"ã€"IT/æŠ€æœ¯æ–‡æ¡£"ã€"æ³•å¾‹åˆåŒ"ç­‰
            tone: æœŸæœ›é£æ ¼ï¼Œå¦‚ "æ ‡å‡†/å‡†ç¡®"ã€"åœ°é“/å£è¯­åŒ–"
            user_glossary: ç”¨æˆ·æä¾›çš„æœ¯è¯­è¡¨ï¼Œæ ¼å¼å¦‚ "Prompt=æç¤ºè¯\nLLM=å¤§è¯­è¨€æ¨¡å‹"
            
        Returns:
            TranslationPrompt: ä¼˜åŒ–åçš„ç¿»è¯‘ Prompt
        """
        print(f"\n{'='*60}")
        print(f"ğŸŒ å¼€å§‹ç¿»è¯‘ä»»åŠ¡ Prompt ä¼˜åŒ–")
        print(f"{'='*60}")
        print(f"ğŸ”Œ API æä¾›å•†: {self.provider.upper()}")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {self.model}")
        print(f"ğŸ”„ ç¿»è¯‘æ–¹å‘: {source_lang} â†’ {target_lang}")
        print(f"ğŸ“š åº”ç”¨é¢†åŸŸ: {domain}")
        print(f"ğŸ¨ æœŸæœ›é£æ ¼: {tone}")
        if user_glossary:
            print(f"ğŸ“– æœ¯è¯­è¡¨: {len(user_glossary.split(chr(10)))} æ¡")
        print(f"{'='*60}\n")
        
        # å¤„ç†æœ¯è¯­è¡¨
        glossary_text = ""
        if user_glossary.strip():
            glossary_text = f"""
**ç”¨æˆ·æŒ‡å®šæœ¯è¯­è¡¨**ï¼š
ç”¨æˆ·å¼ºåˆ¶æŒ‡å®šäº†ä»¥ä¸‹æœ¯è¯­å¯¹åº”å…³ç³»ï¼Œå¿…é¡»åœ¨ Prompt ä¸­åˆ›å»ºä¸€ä¸ªæ˜ç¡®çš„ Glossary Section æ¥é”å®šè¿™äº›ç¿»è¯‘ï¼š
{user_glossary}
"""
        
        # æ„å»ºç¿»è¯‘ä»»åŠ¡ä¸“ç”¨çš„ Meta-Prompt
        system_prompt = f"""
ä½ æ˜¯ä¸€ä½ç²¾é€šå¤šè¯­è¨€è½¬æ¢çš„ Prompt Engineering ä¸“å®¶ã€‚
ä½ çš„ä»»åŠ¡æ˜¯æ„å»ºä¸€ä¸ª**ä¸“å®¶çº§çš„ç¿»è¯‘ Prompt**ï¼Œä»¥è§£å†³æœºå™¨ç¿»è¯‘ç”Ÿç¡¬ã€ç¼ºä¹è¯­å¢ƒã€é£æ ¼ä¸ä¸€è‡´çš„é—®é¢˜ã€‚

**ä»»åŠ¡ä¿¡æ¯**ï¼š
- è¯­è¨€æ–¹å‘ï¼š{source_lang} â†’ {target_lang}
- åº”ç”¨é¢†åŸŸï¼š{domain}
- æœŸæœ›é£æ ¼ï¼š{tone}{glossary_text}

**ç¿»è¯‘ä»»åŠ¡çš„æ ¸å¿ƒæŒ‘æˆ˜**ï¼š
1. **è¯­å¢ƒåå·®ï¼ˆContext Nuanceï¼‰**ï¼šåŒä¸€ä¸ªè¯åœ¨ä¸åŒåœºæ™¯æœ‰ä¸åŒå«ä¹‰ï¼ˆå¦‚ "Bank" æ˜¯"é“¶è¡Œ"è¿˜æ˜¯"æ²³å²¸"ï¼Ÿï¼‰
2. **é£æ ¼ä¸€è‡´æ€§ï¼ˆTone & Styleï¼‰**ï¼šæ˜¯"ä¿¡è¾¾é›…"çš„æ–‡å­¦ç¿»è¯‘ï¼Œè¿˜æ˜¯"ç²¾å‡†ç›´ç™½"çš„æŠ€æœ¯ç¿»è¯‘ï¼Ÿ
3. **æœ¯è¯­ä¸€è‡´æ€§ï¼ˆGlossary Consistencyï¼‰**ï¼šç‰¹å®šçš„ä¸“æœ‰åè¯ä¸èƒ½ä¹±ç¿»ï¼Œéœ€è¦ç»Ÿä¸€æ ‡å‡†

**ä½ çš„ä»»åŠ¡**ï¼š
æ„å»ºä¸€ä¸ªåŒ…å«ä»¥ä¸‹é«˜çº§ç­–ç•¥çš„ç¿»è¯‘ Promptï¼š

1. **é¢†åŸŸæ²‰æµ¸ï¼ˆDomain Immersionï¼‰**
   - æ ¹æ®é¢†åŸŸè®¾å®šæœ€æƒå¨çš„ä¸“å®¶è§’è‰²
   - ITæ–‡æ¡£ â†’ "ç²¾é€šä¸­è‹±åŒè¯­çš„èµ„æ·±è½¯ä»¶å·¥ç¨‹å¸ˆå’ŒæŠ€æœ¯æ–‡æ¡£ç¼–è¾‘"
   - æ³•å¾‹åˆåŒ â†’ "èµ„æ·±å›½é™…æ³•å¾‹ç¿»è¯‘ä¸“å®¶ï¼Œç†Ÿæ‚‰ä¸­è‹±æ³•å¾‹æœ¯è¯­ä½“ç³»"
   - æ–‡å­¦ä½œå“ â†’ "ä¸“ä¸šæ–‡å­¦è¯‘è€…ï¼Œæ›¾ç¿»è¯‘å¤šéƒ¨è·å¥–ä½œå“"
   - å­¦æœ¯è®ºæ–‡ â†’ "ã€Šè‡ªç„¶ã€‹æ‚å¿—ç¼–è¾‘ï¼Œç²¾é€šå­¦æœ¯è§„èŒƒå’Œç§‘ç ”è¡¨è¾¾"

2. **æœ¯è¯­é”å®šï¼ˆGlossary Lockingï¼‰**
   - å¦‚æœç”¨æˆ·æä¾›äº†æœ¯è¯­è¡¨ï¼Œå¿…é¡»åœ¨ Prompt ä¸­ç”Ÿæˆä¸€ä¸ªæ¸…æ™°çš„ Mapping Table
   - è¦æ±‚æ¨¡å‹"ä¸¥æ ¼éµå®ˆ"ï¼ˆStrictly Adhereï¼‰è¿™äº›æœ¯è¯­å¯¹åº”å…³ç³»
   - æ ¼å¼ç¤ºä¾‹ï¼š
     ```
     **æœ¯è¯­è¡¨ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰**ï¼š
     - Apple â†’ è‹¹æœå…¬å¸ï¼ˆè€Œé"è‹¹æœ"æ°´æœï¼‰
     - Prompt â†’ æç¤ºè¯ï¼ˆæŠ€æœ¯æœ¯è¯­ï¼Œä¸ç¿»è¯‘ä¸º"æç¤º"ï¼‰
     ```

3. **ä¸‰æ­¥ç¿»è¯‘æ³•ï¼ˆThree-Step Translationï¼‰**
   - åœ¨ Prompt ä¸­è¦æ±‚æ¨¡å‹æŒ‰ä»¥ä¸‹æµç¨‹å¤„ç†ï¼š
     Step 1: åˆ†æä¸Šä¸‹æ–‡å’Œä¸“ä¸šæœ¯è¯­ï¼Œè¿›è¡Œåˆæ­¥ç›´è¯‘
     Step 2: æ ¹æ®è¯­å¢ƒå’Œé¢†åŸŸç‰¹ç‚¹ï¼Œè°ƒæ•´è¡¨è¾¾æ–¹å¼ï¼Œç¡®ä¿è¯­ä¹‰å‡†ç¡®
     Step 3: æ¶¦è‰²é£æ ¼ï¼Œä½¿è¯‘æ–‡ç¬¦åˆç›®æ ‡è¯­è¨€çš„è¡¨è¾¾ä¹ æƒ¯å’ŒæœŸæœ›é£æ ¼
   - è¿™ç§"æ…¢æ€è€ƒ"æ¨¡å¼èƒ½æ˜¾è‘—æå‡è´¨é‡

4. **é£æ ¼æŒ‡å—ï¼ˆStyle Guidelinesï¼‰**
   - æ ¹æ®æœŸæœ›é£æ ¼ç»™å‡ºå…·ä½“æŒ‡å¯¼ï¼š
   - "æ ‡å‡†/å‡†ç¡®"ï¼šä¿æŒå®¢è§‚ã€ä¸¥è°¨ï¼Œé¿å…æ·»åŠ ä¸»è§‚è‰²å½©
   - "åœ°é“/å£è¯­åŒ–"ï¼šä½¿ç”¨ç›®æ ‡è¯­è¨€çš„è‡ªç„¶è¡¨è¾¾ï¼Œé¿å…"ç¿»è¯‘è…”"
   - "ä¼˜ç¾/æ–‡å­¦æ€§"ï¼šæ³¨é‡éŸµå¾‹å’Œç¾æ„Ÿï¼Œå¯é€‚å½“æ„è¯‘
   - "æç®€/æ‘˜è¦å¼"ï¼šç®€æ´æ˜äº†ï¼Œå»é™¤å†—ä½™

5. **ä¿ç•™è§„åˆ™ï¼ˆPreservation Rulesï¼‰**
   - å¯¹äºä»¥ä¸‹å†…å®¹ï¼Œæ˜ç¡®è¦æ±‚ä¿ç•™åŸæ–‡ï¼š
   - ä»£ç å—ã€å‘½ä»¤è¡Œã€æ–‡ä»¶è·¯å¾„
   - ä¸“æœ‰åè¯ï¼ˆäººåã€åœ°åã€å“ç‰Œåï¼‰
   - æ— æ³•ç¿»è¯‘æˆ–ä¸å®œç¿»è¯‘çš„æœ¯è¯­ï¼ˆç”¨æ‹¬å·æ³¨é‡ŠåŸæ–‡ï¼‰

6. **æ ¼å¼è§„èŒƒï¼ˆFormat Requirementsï¼‰**
   - ä¿æŒåŸæ–‡çš„æ®µè½ç»“æ„å’Œæ ¼å¼
   - æ•°å­—ã€æ ‡ç‚¹ç¬¦å·çš„è§„èŒƒï¼ˆå¦‚ï¼šä¸­æ–‡ç”¨å…¨è§’ï¼Œè‹±æ–‡ç”¨åŠè§’ï¼‰

**è¾“å‡ºè¦æ±‚**ï¼š
è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- thinking_process: ä½ çš„ä¼˜åŒ–æ€è€ƒè¿‡ç¨‹ï¼Œåˆ†æè¿™ä¸ªç¿»è¯‘ä»»åŠ¡çš„ç‰¹ç‚¹å’Œéš¾ç‚¹
- role_definition: è§’è‰²è®¾å®šæè¿°ï¼Œè¦å…·ä½“åˆ°è¯¥é¢†åŸŸæœ€æƒå¨çš„ä¸“å®¶
- style_guidelines: é£æ ¼æŒ‡å—åˆ—è¡¨ï¼ˆlistï¼‰ï¼Œé’ˆå¯¹æœŸæœ›é£æ ¼çš„å…·ä½“è¦æ±‚ï¼ˆ3-5æ¡ï¼‰
- glossary_section: æœ¯è¯­å¯¹ç…§è¡¨éƒ¨åˆ†çš„æ–‡æœ¬ï¼ˆå¦‚æœç”¨æˆ·æä¾›äº†æœ¯è¯­è¡¨ï¼‰ã€‚å¦‚æœæ²¡æœ‰åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
- workflow_steps: ç¿»è¯‘å·¥ä½œæµæŒ‡ä»¤ï¼Œæ¨èä½¿ç”¨"ä¸‰æ­¥ç¿»è¯‘æ³•"çš„è¯¦ç»†æè¿°
- final_prompt: å®Œæ•´çš„ã€å¯ç›´æ¥ä½¿ç”¨çš„ç¿»è¯‘ Promptï¼ˆç”¨ {{{{text}}}} ä½œä¸ºå¾…ç¿»è¯‘æ–‡æœ¬çš„å ä½ç¬¦ï¼‰

**é‡è¦**ï¼š
- final_prompt å¿…é¡»æ˜¯ä¸€ä¸ªå®Œæ•´çš„ã€ç»“æ„æ¸…æ™°çš„ã€å¯ä»¥ç›´æ¥å¤åˆ¶ä½¿ç”¨çš„ç¿»è¯‘ Prompt
- å…¶ä¸­å¾…ç¿»è¯‘çš„æ–‡æœ¬ç”¨ {{{{text}}}} å ä½ç¬¦è¡¨ç¤º
- æ‰€æœ‰è§„åˆ™ã€æœ¯è¯­è¡¨ã€é£æ ¼æŒ‡å—éƒ½è¦æ•´åˆè¿› final_prompt ä¸­
- åŠ¡å¿…ä½“ç°"é¢†åŸŸä¸“å®¶ + æœ¯è¯­é”å®š + ä¸‰æ­¥ç¿»è¯‘æ³•"çš„æ ¸å¿ƒç­–ç•¥
"""
        
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "è¯·ä¸ºè¿™ä¸ªç¿»è¯‘ä»»åŠ¡ç”Ÿæˆä¼˜åŒ–çš„ Promptã€‚")
        ])
        
        try:
            print("ğŸ“¤ æ­£åœ¨è°ƒç”¨ API...")
            
            messages = prompt_template.format_messages()
            print(f"ğŸ’¬ æ¶ˆæ¯é•¿åº¦: {len(str(messages))} å­—ç¬¦")
            
            # è°ƒç”¨ LLM
            if self.provider == "openai":
                print("ğŸ”§ ä½¿ç”¨ OpenAI JSON mode")
                response = self.llm.invoke(
                    messages,
                    response_format={"type": "json_object"}
                )
                time.sleep(0.5)  # API è°ƒç”¨å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡è¿‡å¿«
            else:
                print("ğŸ”§ ä½¿ç”¨ NVIDIA æ ‡å‡†è°ƒç”¨")
                response = self.llm.invoke(messages)
                time.sleep(0.5)  # API è°ƒç”¨å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡è¿‡å¿«
            
            # è§£æç»“æœ
            content = response.content
            print(f"ğŸ“¥ æ”¶åˆ°å“åº”ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
            
            # æå– JSON
            if "```json" in content:
                print("ğŸ” æ£€æµ‹åˆ° JSON ä»£ç å—ï¼Œæ­£åœ¨æå–...")
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                print("ğŸ” æ£€æµ‹åˆ°ä»£ç å—ï¼Œæ­£åœ¨æå–...")
                content = content.split("```")[1].split("```")[0].strip()
            
            print("âš™ï¸ æ­£åœ¨è§£æ JSON...")
            result_dict = json.loads(content)
            
            print("âœ… JSON è§£ææˆåŠŸ")
            print("ğŸ”¨ æ­£åœ¨éªŒè¯æ•°æ®ç»“æ„...")
            optimized = TranslationPrompt(**result_dict)
            
            print("âœ… ç¿»è¯‘ Prompt ä¼˜åŒ–å®Œæˆï¼")
            print(f"{'='*60}\n")
            
            return optimized
            
        except Exception as e:
            # é”™è¯¯å¤„ç†
            print(f"\nâŒ ç¿»è¯‘ä¼˜åŒ–å¤±è´¥ï¼")
            print(f"{'='*60}")
            
            error_msg = str(e)
            print(f"ğŸ› é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"ğŸ“ é”™è¯¯è¯¦æƒ…: {error_msg[:500]}")
            
            import traceback
            print(f"\nğŸ“„ å®Œæ•´å †æ ˆä¿¡æ¯ï¼š")
            traceback.print_exc()
            print(f"{'='*60}\n")
            
            # æŠ›å‡ºå¼‚å¸¸
            if "404" in error_msg:
                raise Exception(f"API è°ƒç”¨å¤±è´¥ (404): è¯·æ£€æŸ¥ API Key æ˜¯å¦æœ‰æ•ˆï¼Œæˆ–æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®ã€‚")
            elif "401" in error_msg or "Unauthorized" in error_msg:
                raise Exception(f"API Key æ— æ•ˆæˆ–å·²è¿‡æœŸã€‚")
            else:
                raise Exception(f"ç¿»è¯‘ä¼˜åŒ–å¤±è´¥: {error_msg[:300]}")
    
    def _build_meta_prompt(self, strategy: dict, scene_desc: str) -> str:
        """æ„å»º Meta-Promptï¼ˆæ•™ LLM å¦‚ä½•ä¼˜åŒ– Prompt çš„æç¤ºè¯ï¼‰"""
        
        template_name = strategy.get("template", "CO-STAR")
        focus_principles = strategy.get("focus", ["clarity", "structure"])
        extra_requirements = strategy.get("extra_requirements", [])
        
        # è·å–ç„¦ç‚¹åŸåˆ™çš„è¯¦ç»†è¯´æ˜
        principles_text = "\n".join([
            f"   - {OPTIMIZATION_PRINCIPLES.get(p, p)}"
            for p in focus_principles
        ])
        
        # æ„å»ºé¢å¤–è¦æ±‚æ–‡æœ¬
        extra_text = ""
        if extra_requirements:
            extra_text = "\n\n**åœºæ™¯ç‰¹å®šè¦æ±‚**ï¼š\n" + "\n".join([
                f"   - {req}" for req in extra_requirements
            ])
        
        meta_prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸–ç•Œçº§çš„ Prompt Engineering ä¸“å®¶ï¼Œæ“…é•¿å°†ç®€å•çš„æŒ‡ä»¤è½¬åŒ–ä¸ºç»“æ„åŒ–ã€é«˜æ€§èƒ½çš„ä¸“å®¶çº§ Promptã€‚

**ä½ çš„ä»»åŠ¡æµç¨‹**ï¼š

1. **æ·±åº¦ç†è§£**ï¼šä»”ç»†åˆ†æç”¨æˆ·çš„åŸå§‹ Promptï¼Œè¯†åˆ«å…¶æ ¸å¿ƒæ„å›¾å’Œéšå«éœ€æ±‚

2. **ä¸‰å¤§ä¼˜åŒ–ç­–ç•¥**ï¼š
   
   a) **è¯­ä¹‰æ‰©å±• (Semantic Expansion)**
      - è¡¥å……ç¼ºå¤±çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
      - æ˜ç¡®éšå«çš„çº¦æŸæ¡ä»¶
      - è§„èŒƒè¾“å‡ºæ ¼å¼è¦æ±‚
   
   b) **å…³é”®è¯å¢å¼º (Keywords Enhancement)**
      - è¯†åˆ«ä»»åŠ¡æ‰€å±çš„ä¸“ä¸šé¢†åŸŸ
      - åŠ å…¥è¯¥é¢†åŸŸçš„ä¸“ä¸šæœ¯è¯­å’Œè¡Œä¸šæ¦‚å¿µ
      - ç”¨ç²¾ç¡®çš„è¯æ±‡æ›¿æ¢æ¨¡ç³Šè¡¨è¾¾
   
   c) **ç»“æ„åŒ–é‡å†™ (Template Application)**
      - å¿…é¡»ä½¿ç”¨ **{template_name}** æ¡†æ¶è¿›è¡Œé‡å†™
      - ç¡®ä¿ Prompt é€»è¾‘æ¸…æ™°ã€å±‚æ¬¡åˆ†æ˜

3. **ä¼˜åŒ–åŸåˆ™**ï¼ˆæœ¬æ¬¡ä¼˜åŒ–é‡ç‚¹å…³æ³¨ï¼‰ï¼š
{principles_text}
{extra_text}

**åœºæ™¯ä¸Šä¸‹æ–‡**ï¼š{scene_desc if scene_desc else "é€šç”¨åœºæ™¯"}

**è¾“å‡ºè¦æ±‚**ï¼š
è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- thinking_process: ä½ çš„ä¼˜åŒ–æ€è€ƒè¿‡ç¨‹ï¼ˆ200å­—å·¦å³ï¼‰
- improved_prompt: ä¼˜åŒ–åçš„å®Œæ•´ Promptï¼ˆå¯ç›´æ¥ä½¿ç”¨ï¼‰
- enhancement_techniques: ä½¿ç”¨çš„ä¼˜åŒ–æŠ€æœ¯åˆ—è¡¨
- keywords_added: æ–°å¢çš„å…³é”®è¯åˆ—è¡¨
- structure_applied: åº”ç”¨çš„æ¡†æ¶åç§°

**é‡è¦**ï¼šimproved_prompt åº”è¯¥æ˜¯ä¸€ä¸ªå®Œæ•´çš„ã€å¯ä»¥ç›´æ¥å¤åˆ¶ä½¿ç”¨çš„é«˜è´¨é‡ Promptï¼Œä¸è¦åŒ…å«ä»»ä½•å…ƒä¿¡æ¯æˆ–è¯´æ˜ã€‚
"""
        return meta_prompt
    
    def _fallback_optimization(self, original_prompt: str, error: str) -> OptimizedPrompt:
        """å½“ä¼˜åŒ–å¤±è´¥æ—¶çš„å¤‡ç”¨æ–¹æ¡ˆ"""
        return OptimizedPrompt(
            thinking_process=f"ä¼˜åŒ–è¿‡ç¨‹ä¸­é‡åˆ°é”™è¯¯ï¼š{error}ã€‚ä»¥ä¸‹æ˜¯åŸºç¡€ä¼˜åŒ–ç‰ˆæœ¬ã€‚",
            improved_prompt=f"""
è¯·ä»¥ä¸“ä¸šçš„æ€åº¦å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š

{original_prompt}

è¦æ±‚ï¼š
1. è¾“å‡ºå†…å®¹åº”è¯¥æ¸…æ™°ã€å‡†ç¡®ã€å®Œæ•´
2. ä½¿ç”¨æ°å½“çš„æ ¼å¼ç»„ç»‡ä¿¡æ¯
3. æ³¨é‡ç»†èŠ‚å’Œä¸“ä¸šæ€§
4. å¦‚æœ‰éœ€è¦ï¼Œè¯·å±•ç¤ºä½ çš„æ€è€ƒè¿‡ç¨‹
""",
            enhancement_techniques=["åŸºç¡€ç»“æ„åŒ–", "æ·»åŠ é€šç”¨è¦æ±‚"],
            keywords_added=[],
            structure_applied="ç®€å•ä¼˜åŒ–"
        )
    
    def compare_results(self, original_prompt: str, optimized_prompt: str, 
                       test_query: Optional[str] = None) -> tuple[str, str]:
        """
        A/B å¯¹æ¯”æµ‹è¯•ï¼šåˆ†åˆ«è¿è¡ŒåŸå§‹å’Œä¼˜åŒ–åçš„ Prompt
        
        Args:
            original_prompt: åŸå§‹ Prompt
            optimized_prompt: ä¼˜åŒ–åçš„ Prompt
            test_query: å¯é€‰çš„æµ‹è¯•æŸ¥è¯¢ï¼ˆå¦‚æœ Prompt æœ¬èº«ä¸æ˜¯ç›´æ¥çš„é—®é¢˜ï¼‰
            
        Returns:
            (åŸå§‹ç»“æœ, ä¼˜åŒ–åç»“æœ)
        """
        try:
            # è¿è¡ŒåŸå§‹ Prompt
            response_original = self.llm.invoke(original_prompt)
            time.sleep(0.3)  # API è°ƒç”¨å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡è¿‡å¿«ï¼ˆA/B æµ‹è¯•éœ€è¦è¾ƒé•¿ç­‰å¾…ï¼‰
            result_original = response_original.content
            
            # è¿è¡Œä¼˜åŒ–åçš„ Prompt
            response_optimized = self.llm.invoke(optimized_prompt)
            time.sleep(0.3)  # API è°ƒç”¨å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡è¿‡å¿«ï¼ˆA/B æµ‹è¯•éœ€è¦è¾ƒé•¿ç­‰å¾…ï¼‰
            result_optimized = response_optimized.content
            
            return result_original, result_optimized
            
        except Exception as e:
            return f"è¿è¡Œå¤±è´¥: {str(e)}", f"è¿è¡Œå¤±è´¥: {str(e)}"
    
    def batch_optimize(self, prompts: list[str], 
                       scene_desc: str = "é€šç”¨",
                       optimization_mode: str = "é€šç”¨å¢å¼º (General)") -> list[OptimizedPrompt]:
        """
        æ‰¹é‡ä¼˜åŒ–å¤šä¸ª Prompt
        
        Args:
            prompts: Prompt åˆ—è¡¨
            scene_desc: åœºæ™¯æè¿°
            optimization_mode: ä¼˜åŒ–æ¨¡å¼
            
        Returns:
            ä¼˜åŒ–ç»“æœåˆ—è¡¨
        """
        results = []
        for prompt in prompts:
            result = self.optimize(prompt, scene_desc, optimization_mode)
            results.append(result)
        return results


    def generate_search_space(self, task_description: str, task_type: str = "classification") -> SearchSpace:
        """
        è®© LLM è‡ªåŠ¨åˆ†æä»»åŠ¡ï¼Œç”Ÿæˆå¯ä¾›æœç´¢çš„å˜é‡æ± 
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            task_type: ä»»åŠ¡ç±»å‹ (classification/summarization/translation)
            
        Returns:
            SearchSpace å¯¹è±¡ï¼ŒåŒ…å« roles, styles, techniques
        """
        print(f"\n{'='*60}")
        print(f"ğŸ§  ç”Ÿæˆæœç´¢ç©ºé—´")
        print(f"{'='*60}")
        print(f"ä»»åŠ¡ç±»å‹: {task_type}")
        print(f"ä»»åŠ¡æè¿°: {task_description}")
        print(f"{'='*60}\n")
        
        system_prompt = """
ä½ æ˜¯ä¸€ä¸ª Prompt ç­–ç•¥ç”Ÿæˆå™¨ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸ºç»™å®šçš„ä»»åŠ¡è®¾è®¡å¤šç§å¯èƒ½çš„ Prompt ç»„ä»¶ã€‚

**è¾“å‡ºè¦æ±‚ï¼š**
è¯·ä»¥ JSON æ ¼å¼è¿”å›ï¼ŒåŒ…å«ä¸‰ä¸ªæ•°ç»„ï¼š
- roles: 5ä¸ªä¸åŒçš„è§’è‰²è®¾å®š
- styles: 5ç§ä¸åŒçš„è¯­æ°”/é£æ ¼  
- techniques: 3ç§ä¸åŒçš„æç¤ºå·¥ç¨‹æŠ€å·§

**è§’è‰²ç¤ºä¾‹**ï¼š
- å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼šèµ„æ·±æ•°æ®åˆ†æå¸ˆã€å¿ƒç†å­¦å®¶ã€å®¢æœç»ç†ã€äº§å“ç»ç†ã€ç¤¾äº¤åª’ä½“ä¸“å®¶
- å¯¹äºæ‘˜è¦ä»»åŠ¡ï¼šæ–°é—»ç¼–è¾‘ã€æŠ€æœ¯æ–‡æ¡£ä¸“å®¶ã€ä¼šè®®è®°å½•å‘˜ã€ç ”ç©¶å‘˜ã€é¡¾é—®
- å¯¹äºç¿»è¯‘ä»»åŠ¡ï¼šä¸“ä¸šè¯‘è€…ã€åŒè¯­ä½œå®¶ã€æœ¬åœ°åŒ–ä¸“å®¶ã€æŠ€æœ¯ç¿»è¯‘ã€æ–‡å­¦ç¿»è¯‘

**é£æ ¼ç¤ºä¾‹**ï¼š
- ä¸¥è°¨å­¦æœ¯ã€é€šä¿—æ˜“æ‡‚ã€ç®€æ´æ˜äº†ã€è¯¦å°½å…¨é¢ã€åˆ›æ„å¹½é»˜

**æŠ€å·§ç¤ºä¾‹**ï¼š
- "Let's think step by step" (æ€ç»´é“¾)
- "Provide only the label without explanation" (ç›´æ¥å›ç­”)
- "First analyze the features, then make a decision" (ç‰¹å¾åˆ†æ)
- "Use few-shot examples" (å°‘æ ·æœ¬å­¦ä¹ )
- "Output in JSON format" (æ ¼å¼åŒ–è¾“å‡º)

è¯·ç¡®ä¿è¾“å‡ºæ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼ã€‚
"""
        
        user_prompt = f"""
ä»»åŠ¡ç±»å‹ï¼š{task_type}
ä»»åŠ¡æè¿°ï¼š{task_description}

è¯·ä¸ºè¿™ä¸ªä»»åŠ¡è®¾è®¡ï¼š
1. 5ä¸ªä¸åŒçš„è§’è‰²å®šä½ï¼ˆä»ä¿å®ˆåˆ°åˆ›æ–°ï¼Œè¦†ç›–ä¸åŒä¸“ä¸šèƒŒæ™¯ï¼‰
2. 5ç§ä¸åŒçš„å›ç­”é£æ ¼/è¯­æ°”
3. 3ç§ä¸åŒçš„æç¤ºå·¥ç¨‹æŠ€å·§æˆ–æŒ‡ä»¤æ¨¡å¼

ç¡®ä¿è¾“å‡ºçº¯ JSON æ ¼å¼ã€‚
"""
        
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", user_prompt)
        ])
        
        try:
            # è°ƒç”¨ LLM
            print("ğŸ“¡ è°ƒç”¨ LLM ç”Ÿæˆæœç´¢ç©ºé—´...")
            messages = prompt_template.format_messages(task_type=task_type, task_description=task_description)
            response = self.llm.invoke(messages)
            
            time.sleep(0.5)  # API è°ƒç”¨å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡è¿‡å¿«ï¼ˆæœç´¢ç©ºé—´ç”Ÿæˆè¾ƒå¿«ï¼‰
            
            print(f"âœ… LLM å“åº”æˆåŠŸ")
            print(f"åŸå§‹å“åº”é•¿åº¦: {len(response.content)} å­—ç¬¦")
            
            # è§£æ JSON
            content = response.content.strip()
            print(f"\nğŸ” è§£æ JSON å“åº”...")
            print(f"åŸå§‹å†…å®¹å‰100å­—ç¬¦: {content[:100]}...")
            
            # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
            if content.startswith("```json"):
                content = content[7:]
                print("  ç§»é™¤äº† ```json æ ‡è®°")
            if content.startswith("```"):
                content = content[3:]
                print("  ç§»é™¤äº† ``` æ ‡è®°")
            if content.endswith("```"):
                content = content[:-3]
                print("  ç§»é™¤äº†å°¾éƒ¨ ``` æ ‡è®°")
            content = content.strip()
            
            # æå– JSON éƒ¨åˆ†ï¼ˆä»ç¬¬ä¸€ä¸ª { åˆ°æœ€åä¸€ä¸ª }ï¼‰
            # è¿™æ ·å¯ä»¥å¿½ç•¥ JSON å‰åçš„é¢å¤–æ–‡æœ¬
            try:
                start_idx = content.index('{')
                end_idx = content.rindex('}') + 1
                content = content[start_idx:end_idx]
                print(f"  æå–äº†çº¯ JSON å†…å®¹ï¼ˆä»ç¬¬ {start_idx} åˆ°ç¬¬ {end_idx} å­—ç¬¦ï¼‰")
            except ValueError:
                print("  âš ï¸ æœªæ‰¾åˆ°å®Œæ•´çš„ JSON å¯¹è±¡ï¼Œå°è¯•ç›´æ¥è§£æ")
            
            print(f"æ¸…ç†åå†…å®¹å‰100å­—ç¬¦: {content[:100]}...")
            
            data = json.loads(content)
            print(f"âœ… JSON è§£ææˆåŠŸ")
            print(f"  - roles: {len(data.get('roles', []))} ä¸ª")
            print(f"  - styles: {len(data.get('styles', []))} ä¸ª")
            print(f"  - techniques: {len(data.get('techniques', []))} ä¸ª")
            
            # å¤„ç† LLM å¯èƒ½è¿”å›å¯¹è±¡æ•°ç»„çš„æƒ…å†µ
            # å¦‚æœè¿”å› [{"name": "xxx", "description": "yyy"}]ï¼Œæå– name å­—æ®µ
            def extract_names(items):
                """æå–å­—ç¬¦ä¸²æˆ–å¯¹è±¡æ•°ç»„ä¸­çš„åç§°"""
                if not items:
                    return []
                result = []
                for item in items:
                    if isinstance(item, str):
                        result.append(item)
                    elif isinstance(item, dict) and 'name' in item:
                        result.append(item['name'])
                        print(f"    æå–: {item['name']}")
                return result
            
            # è½¬æ¢æ•°æ®æ ¼å¼
            print(f"ğŸ”„ å¤„ç†æ•°æ®æ ¼å¼...")
            data['roles'] = extract_names(data.get('roles', []))
            data['styles'] = extract_names(data.get('styles', []))
            data['techniques'] = extract_names(data.get('techniques', []))
            
            print(f"  âœ… roles: {data['roles']}")
            print(f"  âœ… styles: {data['styles']}")
            print(f"  âœ… techniques: {data['techniques']}")
            
            result = SearchSpace(**data)
            print(f"\nâœ… æœç´¢ç©ºé—´ç”Ÿæˆå®Œæˆï¼\n")
            return result
            
        except Exception as e:
            print(f"\nâŒ ç”Ÿæˆæœç´¢ç©ºé—´å¤±è´¥ï¼")
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"é”™è¯¯ä¿¡æ¯: {e}")
            
            import traceback
            print(f"\nå®Œæ•´é”™è¯¯å †æ ˆï¼š")
            traceback.print_exc()
            
            print(f"\nâš ï¸ ä½¿ç”¨é»˜è®¤æœç´¢ç©ºé—´...\n")
            # è¿”å›é»˜è®¤çš„æœç´¢ç©ºé—´
            return SearchSpace(
                roles=[
                    "èµ„æ·±ä¸“å®¶",
                    "æ•°æ®åˆ†æå¸ˆ",
                    "é¢†åŸŸé¡¾é—®",
                    "å®è·µè€…",
                    "ç ”ç©¶å‘˜"
                ],
                styles=[
                    "ä¸¥è°¨å­¦æœ¯",
                    "é€šä¿—æ˜“æ‡‚",
                    "ç®€æ´æ˜äº†",
                    "è¯¦å°½å…¨é¢",
                    "ç³»ç»ŸåŒ–"
                ],
                techniques=[
                    "Let's think step by step",
                    "Provide direct answer without explanation",
                    "Analyze features then decide"
                ]
            )
    
    
    def run_random_search(
        self, 
        task_description: str,
        task_type: str,
        test_dataset: list[dict],
        search_space: SearchSpace,
        iterations: int = 5,
        progress_callback=None
    ) -> tuple[list[SearchResult], SearchResult]:
        """
        æ‰§è¡Œéšæœºæœç´¢ä¼˜åŒ–
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            task_type: ä»»åŠ¡ç±»å‹ (classification/summarization/translation)
            test_dataset: æµ‹è¯•æ•°æ®é›† [{'input': '...', 'ground_truth': '...'}]
            search_space: æœç´¢ç©ºé—´
            iterations: æœç´¢è¿­ä»£æ¬¡æ•°
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° callback(current, total, message)
            
        Returns:
            (æ‰€æœ‰ç»“æœåˆ—è¡¨, æœ€ä½³ç»“æœ)
        """
        from metrics import MetricsCalculator
        
        results_log = []
        calc = MetricsCalculator()
        
        print(f"\n{'='*60}")
        print(f"å¼€å§‹éšæœºæœç´¢ä¼˜åŒ– - {iterations} æ¬¡è¿­ä»£")
        print(f"{'='*60}\n")
        
        for i in range(iterations):
            # 1. éšæœºé‡‡æ ·ï¼šæ‘‡éª°å­ç»„åˆ Prompt
            chosen_role = random.choice(search_space.roles)
            chosen_style = random.choice(search_space.styles)
            chosen_tech = random.choice(search_space.techniques)
            
            print(f"è¿­ä»£ {i+1}/{iterations}")
            print(f"  è§’è‰²: {chosen_role}")
            print(f"  é£æ ¼: {chosen_style}")
            print(f"  æŠ€å·§: {chosen_tech}")
            
            # 2. æ‹¼è£…å€™é€‰ Prompt
            if task_type == "classification":
                candidate_prompt = f"""ä½ æ˜¯ä¸€ä½{chosen_role}ã€‚

ä»»åŠ¡ï¼š{task_description}

é£æ ¼è¦æ±‚ï¼š{chosen_style}

æŒ‡ä»¤ï¼š{chosen_tech}

è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼š
[å¾…åˆ†ç±»æ–‡æœ¬]

åªè¾“å‡ºåˆ†ç±»æ ‡ç­¾ï¼Œä¸è¦é¢å¤–è§£é‡Šã€‚
"""
            elif task_type == "summarization":
                candidate_prompt = f"""ä½ æ˜¯ä¸€ä½{chosen_role}ã€‚

ä»»åŠ¡ï¼š{task_description}

é£æ ¼è¦æ±‚ï¼š{chosen_style}

æŒ‡ä»¤ï¼š{chosen_tech}

è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œæ‘˜è¦ï¼š
[å¾…æ‘˜è¦æ–‡æœ¬]

è¯·æŒ‰ç…§è¦æ±‚è¾“å‡ºæ‘˜è¦ã€‚
"""
            elif task_type == "translation":
                candidate_prompt = f"""ä½ æ˜¯ä¸€ä½{chosen_role}ã€‚

ä»»åŠ¡ï¼š{task_description}

é£æ ¼è¦æ±‚ï¼š{chosen_style}

æŒ‡ä»¤ï¼š{chosen_tech}

è¯·ç¿»è¯‘ä»¥ä¸‹æ–‡æœ¬ï¼š
[å¾…ç¿»è¯‘æ–‡æœ¬]

åªè¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦é¢å¤–è¯´æ˜ã€‚
"""
            else:
                candidate_prompt = f"""è§’è‰²: {chosen_role}
é£æ ¼: {chosen_style}
ä»»åŠ¡: {task_description}
æŒ‡ä»¤: {chosen_tech}

è¾“å…¥: {{input}}
"""
            
            # 3. åœ¨æµ‹è¯•é›†ä¸Šè·‘åˆ†
            scores = []
            for case_idx, case in enumerate(test_dataset):
                try:
                    print(f"\n  ğŸ“ æµ‹è¯•æ ·æœ¬ {case_idx+1}/{len(test_dataset)}")
                    print(f"    è¾“å…¥: {case['input'][:50]}..." if len(case['input']) > 50 else f"    è¾“å…¥: {case['input']}")
                    print(f"    æ ‡å‡†ç­”æ¡ˆ: {case['ground_truth']}")
                    
                    # æ›¿æ¢å ä½ç¬¦
                    if task_type == "classification":
                        prompt_filled = candidate_prompt.replace("[å¾…åˆ†ç±»æ–‡æœ¬]", case['input'])
                    elif task_type == "summarization":
                        prompt_filled = candidate_prompt.replace("[å¾…æ‘˜è¦æ–‡æœ¬]", case['input'])
                    elif task_type == "translation":
                        prompt_filled = candidate_prompt.replace("[å¾…ç¿»è¯‘æ–‡æœ¬]", case['input'])
                    else:
                        prompt_filled = candidate_prompt.replace("{{input}}", case['input'])
                    
                    # è°ƒç”¨ LLM
                    print(f"    ğŸ¤– è°ƒç”¨ LLM...")
                    response = self.llm.invoke(prompt_filled)
                    time.sleep(0.3)  # API è°ƒç”¨å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡è¿‡å¿«ï¼ˆéšæœºæœç´¢æ‰¹é‡è°ƒç”¨ï¼‰
                    prediction = response.content.strip()
                    print(f"    ğŸ’¬ LLM è¾“å‡º: {prediction[:80]}..." if len(prediction) > 80 else f"    ğŸ’¬ LLM è¾“å‡º: {prediction}")
                    
                    # è®¡ç®—åˆ†æ•°
                    if task_type == "classification":
                        # åˆ†ç±»ä»»åŠ¡ï¼šç®€å•åŒ¹é…
                        score = 100.0 if prediction == case['ground_truth'] else 0.0
                        print(f"    ğŸ“Š åŒ¹é…ç»“æœ: {'âœ… æ­£ç¡®' if score == 100.0 else 'âŒ é”™è¯¯'}")
                    elif task_type == "summarization":
                        # æ‘˜è¦ä»»åŠ¡ï¼šROUGE
                        print(f"    ğŸ“Š è®¡ç®— ROUGE åˆ†æ•°...")
                        rouge_scores = calc.calculate_rouge(prediction, case['ground_truth'])
                        score = rouge_scores['rouge1']  # ä½¿ç”¨ ROUGE-1 ä½œä¸ºè¯„åˆ†
                        print(f"    ğŸ“Š ROUGE-1: {score:.2f}")
                    elif task_type == "translation":
                        # ç¿»è¯‘ä»»åŠ¡ï¼šBLEU
                        print(f"    ğŸ“Š è®¡ç®— BLEU åˆ†æ•°...")
                        score = calc.calculate_bleu(prediction, case['ground_truth'])
                        print(f"    ğŸ“Š BLEU: {score:.2f}")
                    else:
                        score = 50.0  # é»˜è®¤åˆ†æ•°
                    
                    scores.append(score)
                    print(f"    âœ… å¾—åˆ†: {score:.1f}")
                    
                except Exception as e:
                    print(f"    âŒ è¯„ä¼°å¤±è´¥ï¼")
                    print(f"    é”™è¯¯ç±»å‹: {type(e).__name__}")
                    print(f"    é”™è¯¯ä¿¡æ¯: {e}")
                    scores.append(0.0)
            
            # è®¡ç®—å¹³å‡åˆ†
            avg_score = sum(scores) / len(scores) if scores else 0.0
            print(f"  å¹³å‡å¾—åˆ†: {avg_score:.2f}\n")
            
            # 4. è®°å½•ç»“æœ
            result = SearchResult(
                iteration_id=i+1,
                role=chosen_role,
                style=chosen_style,
                technique=chosen_tech,
                full_prompt=candidate_prompt,
                avg_score=avg_score,
                task_type=task_type
            )
            results_log.append(result)
            
            # è°ƒç”¨è¿›åº¦å›è°ƒ
            if progress_callback:
                progress_callback(i+1, iterations, f"å®Œæˆè¿­ä»£ {i+1}/{iterations}ï¼Œå¾—åˆ†: {avg_score:.2f}")
        
        # æ‰¾å‡ºæœ€ä½³ç»“æœ
        best_result = max(results_log, key=lambda x: x.avg_score)
        
        print(f"{'='*60}")
        print(f"æœç´¢å®Œæˆï¼æœ€ä½³å¾—åˆ†: {best_result.avg_score:.2f}")
        print(f"æœ€ä½³ç»„åˆ: {best_result.role} + {best_result.style} + {best_result.technique}")
        print(f"{'='*60}\n")
        
        return results_log, best_result
    
    def run_genetic_algorithm(
        self,
        task_description: str,
        task_type: str,
        test_dataset: list,
        search_space: 'SearchSpace',
        generations: int = 5,
        population_size: int = 8,
        elite_ratio: float = 0.2,
        mutation_rate: float = 0.2,
        progress_callback: Optional[callable] = None
    ) -> tuple[list, 'SearchResult', list]:
        """
        é—ä¼ ç®—æ³•ä¼˜åŒ– Prompt
        
        æ ¸å¿ƒæ€æƒ³ï¼šé€šè¿‡å¤šä»£è¿›åŒ–ï¼Œè®©ä¼˜ç§€çš„ Prompt ç»„åˆ"ç¹è¡"å‡ºæ›´å¥½çš„åä»£
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            task_type: ä»»åŠ¡ç±»å‹ (classification/summarization/translation)
            test_dataset: æµ‹è¯•æ•°æ®é›† [{"input": "...", "ground_truth": "..."}, ...]
            search_space: æœç´¢ç©ºé—´
            generations: è¿›åŒ–ä»£æ•°ï¼ˆè¶Šå¤šè¶Šå¥½ï¼Œä½†æ¶ˆè€—æ›´å¤§ï¼‰
            population_size: ç§ç¾¤è§„æ¨¡ï¼ˆæ¯ä»£æœ‰å¤šå°‘ä¸ªä¸ªä½“ï¼‰
            elite_ratio: ç²¾è‹±ä¿ç•™æ¯”ä¾‹ï¼ˆä¿ç•™å¤šå°‘ä¼˜ç§€ä¸ªä½“åˆ°ä¸‹ä¸€ä»£ï¼‰
            mutation_rate: å˜å¼‚æ¦‚ç‡ï¼ˆå¼•å…¥éšæœºæ€§é¿å…å±€éƒ¨æœ€ä¼˜ï¼‰
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° callback(gen, total_gen, best_score, avg_score)
        
        Returns:
            (all_results, best_result, evolution_history)
            - all_results: æ‰€æœ‰è¿­ä»£çš„ç»“æœ
            - best_result: æœ€ä½³ç»“æœ
            - evolution_history: è¿›åŒ–å†å² [{"generation": 1, "best_score": 85.0, "avg_score": 78.5}, ...]
        """
        print(f"\n{'='*60}")
        print(f"ğŸ§¬ é—ä¼ ç®—æ³•ä¼˜åŒ–å¼€å§‹")
        print(f"{'='*60}")
        print(f"ğŸ“‹ ä»»åŠ¡ç±»å‹: {task_type}")
        print(f"ğŸ“Š ä»£æ•°: {generations}, ç§ç¾¤è§„æ¨¡: {population_size}")
        print(f"ğŸ”¬ ç²¾è‹±æ¯”ä¾‹: {elite_ratio * 100}%, å˜å¼‚ç‡: {mutation_rate * 100}%")
        print(f"ğŸ“ æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_dataset)}")
        print(f"ğŸ’° é¢„è®¡ API è°ƒç”¨: {generations * population_size * len(test_dataset)} æ¬¡")
        print(f"{'='*60}\n")
        
        def create_individual():
            """åˆ›å»ºä¸€ä¸ªéšæœºä¸ªä½“ï¼ˆPrompt ç»„åˆï¼‰"""
            return {
                "role": random.choice(search_space.roles),
                "style": random.choice(search_space.styles),
                "technique": random.choice(search_space.techniques),
                "score": 0.0,
                "full_prompt": ""
            }
        
        def evaluate_individual(individual, generation: int, index: int):
            """è¯„ä¼°ä¸ªä½“çš„é€‚åº”åº¦ï¼ˆåœ¨æµ‹è¯•é›†ä¸Šçš„å¾—åˆ†ï¼‰"""
            role = individual["role"]
            style = individual["style"]
            technique = individual["technique"]
            
            # æ„å»º Promptï¼ˆæ ¹æ®ä»»åŠ¡ç±»å‹ä¼˜åŒ–è¾“å‡ºæ ¼å¼ï¼‰
            if task_type == "classification":
                # åˆ†ç±»ä»»åŠ¡ï¼šå¼ºåˆ¶è¦æ±‚åªè¾“å‡ºæ ‡ç­¾
                prompt_template = f"""ä½ æ˜¯ä¸€ä½{role}ã€‚

è¯·ä»¥{style}çš„é£æ ¼å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
{task_description}

ç­–ç•¥æç¤ºï¼š{technique}

**é‡è¦ï¼šä½ å¿…é¡»åªè¾“å‡ºåˆ†ç±»æ ‡ç­¾ï¼ˆå¦‚ï¼šç§¯æã€æ¶ˆæã€ä¸­ç«‹ï¼‰ï¼Œä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šã€åˆ†ææˆ–å…¶ä»–å†…å®¹ã€‚**

è¾“å…¥ï¼š{{{{text}}}}
è¾“å‡ºï¼ˆåªè¾“å‡ºæ ‡ç­¾ï¼‰ï¼š"""
            else:
                # å…¶ä»–ä»»åŠ¡ï¼šå¸¸è§„æ ¼å¼
                prompt_template = f"""ä½ æ˜¯ä¸€ä½{role}ã€‚

è¯·ä»¥{style}çš„é£æ ¼å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
{task_description}

ç­–ç•¥æç¤ºï¼š{technique}

è¾“å…¥ï¼š{{{{text}}}}
"""
            
            individual["full_prompt"] = prompt_template
            
            # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
            predictions = []
            ground_truths = []
            
            print(f"  ç¬¬ {generation} ä»£ä¸ªä½“ {index}: {role} + {style} + {technique}")
            
            for idx, sample in enumerate(test_dataset, 1):
                test_input = sample.get("input", "")
                ground_truth = sample.get("ground_truth", "")
                
                # æ›¿æ¢å ä½ç¬¦
                final_prompt = prompt_template.replace("{{text}}", test_input)
                
                # è°ƒç”¨ LLMï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
                prediction = ""
                max_retries = 3
                retry_delay = 2.0
                
                for retry in range(max_retries):
                    try:
                        response = self.llm.invoke(final_prompt)
                        time.sleep(1.0)  # API è°ƒç”¨å»¶è¿Ÿï¼Œé—ä¼ ç®—æ³•å¯†é›†è°ƒç”¨éœ€è¦æ›´é•¿å»¶è¿Ÿ
                        prediction = response.content.strip()
                        break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯
                        
                    except Exception as e:
                        error_msg = str(e)
                        if "429" in error_msg or "Too Many Requests" in error_msg:
                            if retry < max_retries - 1:
                                wait_time = retry_delay * (2 ** retry)  # æŒ‡æ•°é€€é¿: 2s, 4s, 8s
                                print(f"    âš ï¸ æ ·æœ¬ {idx} è¯·æ±‚è¿‡å¿«ï¼Œç­‰å¾… {wait_time:.0f}s åé‡è¯•ï¼ˆç¬¬{retry+1}æ¬¡ï¼‰...")
                                time.sleep(wait_time)
                                continue
                            else:
                                print(f"    âŒ æ ·æœ¬ {idx} è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡")
                                prediction = ""
                                break
                        else:
                            print(f"    âŒ æ ·æœ¬ {idx} è¯„ä¼°å¤±è´¥: {error_msg[:50]}")
                            prediction = ""
                            break
                
                # æ¸…ç†é¢„æµ‹ç»“æœ
                if prediction and task_type == "classification":
                    # å–ç¬¬ä¸€è¡Œ
                    prediction = prediction.split('\n')[0].strip()
                    # ç§»é™¤å¸¸è§çš„å‰ç¼€è¯
                    for prefix in ["è¾“å‡ºï¼š", "è¾“å‡º:", "ç»“æœï¼š", "ç»“æœ:", "åˆ†ç±»ï¼š", "åˆ†ç±»:", "æ ‡ç­¾ï¼š", "æ ‡ç­¾:"]:
                        if prediction.startswith(prefix):
                            prediction = prediction[len(prefix):].strip()
                    # å¦‚æœåŒ…å«å¤šä¸ªè¯ï¼Œå°è¯•æå–å…³é”®æ ‡ç­¾
                    if len(prediction) > 10:  # å¤ªé•¿äº†ï¼Œå¯èƒ½æ˜¯å¥å­
                        # å°è¯•åœ¨å¥å­ä¸­æŸ¥æ‰¾æ ‡ç­¾å…³é”®è¯
                        for label in ["ç§¯æ", "æ¶ˆæ", "ä¸­ç«‹", "æ­£é¢", "è´Ÿé¢", "ä¸­æ€§"]:
                            if label in prediction:
                                prediction = label
                                break
                
                predictions.append(prediction)
                ground_truths.append(ground_truth)
                
                # è°ƒè¯•è¾“å‡ºï¼šæ˜¾ç¤ºé¢„æµ‹å’ŒçœŸå®å€¼
                if generation == 1 and index == 1 and idx <= 2:  # åªæ˜¾ç¤ºç¬¬ä¸€ä»£ç¬¬ä¸€ä¸ªä½“çš„å‰2ä¸ªæ ·æœ¬
                    print(f"      [è°ƒè¯•] æ ·æœ¬{idx} é¢„æµ‹='{prediction}' vs çœŸå®='{ground_truth}'")
            
            # è®¡ç®—åˆ†æ•°
            calc = MetricsCalculator()
            
            # è¿‡æ»¤æ‰ç©ºé¢„æµ‹ï¼ˆè¯„ä¼°å¤±è´¥çš„æ ·æœ¬ï¼‰
            valid_pairs = [(p, g) for p, g in zip(predictions, ground_truths) if p]
            
            if not valid_pairs:
                # æ‰€æœ‰æ ·æœ¬éƒ½å¤±è´¥äº†
                score = 0.0
                print(f"    â†’ å¾—åˆ†: 0.00 (æ‰€æœ‰æ ·æœ¬è¯„ä¼°å¤±è´¥)")
            else:
                valid_predictions = [p for p, g in valid_pairs]
                valid_ground_truths = [g for p, g in valid_pairs]
                
                if task_type == "classification":
                    score = calc.calculate_accuracy(valid_predictions, valid_ground_truths)
                elif task_type == "summarization":
                    scores = [calc.calculate_rouge(p, g)['rougeL'] for p, g in valid_pairs]
                    score = sum(scores) / len(scores) if scores else 0
                elif task_type == "translation":
                    scores = [calc.calculate_bleu(p, g) for p, g in valid_pairs]
                    score = sum(scores) / len(scores) if scores else 0
                else:
                    score = 0.0
                
                # å¦‚æœéƒ¨åˆ†æ ·æœ¬å¤±è´¥ï¼Œæ˜¾ç¤ºæˆåŠŸç‡
                failed_count = len(predictions) - len(valid_pairs)
                if failed_count > 0:
                    print(f"    â†’ å¾—åˆ†: {score:.2f} ({len(valid_pairs)}/{len(predictions)} æ ·æœ¬æˆåŠŸ)")
                else:
                    print(f"    â†’ å¾—åˆ†: {score:.2f}")
            
            individual["score"] = score
            
            # å¦‚æœå¾—åˆ†ä¸º0ä¸”æœ‰æˆåŠŸçš„æ ·æœ¬ï¼Œæ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
            if score == 0.0 and valid_pairs:
                print(f"      [0åˆ†è°ƒè¯•] é¢„æµ‹='{valid_predictions[0][:50]}' vs çœŸå®='{valid_ground_truths[0]}'")
            
            return individual
        
        def crossover(parent1, parent2):
            """äº¤å‰ï¼šå­©å­ç»§æ‰¿çˆ¶æ¯çš„ä¼˜è‰¯åŸºå› """
            child = {
                "role": random.choice([parent1["role"], parent2["role"]]),
                "style": random.choice([parent1["style"], parent2["style"]]),
                "technique": random.choice([parent1["technique"], parent2["technique"]]),
                "score": 0.0,
                "full_prompt": ""
            }
            return child
        
        def mutate(individual):
            """å˜å¼‚ï¼šéšæœºæ”¹å˜æŸäº›åŸºå› ï¼Œå¼•å…¥æ–°å¯èƒ½æ€§"""
            if random.random() < mutation_rate:
                individual["role"] = random.choice(search_space.roles)
                print(f"    ğŸ”€ å˜å¼‚: æ›´æ¢è§’è‰² â†’ {individual['role']}")
            if random.random() < mutation_rate:
                individual["style"] = random.choice(search_space.styles)
                print(f"    ğŸ”€ å˜å¼‚: æ›´æ¢é£æ ¼ â†’ {individual['style']}")
            if random.random() < mutation_rate:
                individual["technique"] = random.choice(search_space.techniques)
                print(f"    ğŸ”€ å˜å¼‚: æ›´æ¢æŠ€å·§ â†’ {individual['technique']}")
            return individual
        
        # === é—ä¼ ç®—æ³•ä¸»å¾ªç¯ ===
        
        # åˆå§‹åŒ–ç¬¬ä¸€ä»£ç§ç¾¤
        print(f"ğŸ§¬ ç¬¬ 1 ä»£ï¼šåˆå§‹åŒ–ç§ç¾¤ï¼ˆ{population_size} ä¸ªä¸ªä½“ï¼‰")
        population = [create_individual() for _ in range(population_size)]
        
        evolution_history = []
        all_results = []
        
        for gen in range(generations):
            print(f"\n{'='*60}")
            print(f"ğŸ§¬ ç¬¬ {gen + 1}/{generations} ä»£è¿›åŒ–")
            print(f"{'='*60}")
            
            # è¯„ä¼°å½“å‰ç§ç¾¤
            for i, individual in enumerate(population, 1):
                evaluate_individual(individual, gen + 1, i)
            
            # æŒ‰é€‚åº”åº¦æ’åº
            population.sort(key=lambda x: x["score"], reverse=True)
            
            # è®°å½•å†å²
            best_score = population[0]["score"]
            avg_score = sum(ind["score"] for ind in population) / len(population)
            
            evolution_history.append({
                "generation": gen + 1,
                "best_score": best_score,
                "avg_score": avg_score,
                "worst_score": population[-1]["score"]
            })
            
            print(f"\nğŸ“Š ç¬¬ {gen + 1} ä»£ç»Ÿè®¡:")
            print(f"  ğŸ¥‡ æœ€é«˜åˆ†: {best_score:.2f}")
            print(f"  ğŸ“Š å¹³å‡åˆ†: {avg_score:.2f}")
            print(f"  ğŸ“‰ æœ€ä½åˆ†: {population[-1]['score']:.2f}")
            print(f"  ğŸ† å† å†›: {population[0]['role']} + {population[0]['style']} + {population[0]['technique']}")
            
            # ä¿å­˜æ‰€æœ‰ç»“æœ
            for i, ind in enumerate(population):
                result = SearchResult(
                    iteration_id=gen * population_size + i + 1,
                    role=ind["role"],
                    style=ind["style"],
                    technique=ind["technique"],
                    full_prompt=ind["full_prompt"],
                    avg_score=ind["score"],
                    task_type=task_type
                )
                all_results.append(result)
            
            # è°ƒç”¨è¿›åº¦å›è°ƒ
            if progress_callback:
                progress_callback(gen + 1, generations, best_score, avg_score)
            
            # å¦‚æœæ˜¯æœ€åä¸€ä»£ï¼Œè·³è¿‡ç¹è¡
            if gen == generations - 1:
                break
            
            # é€‰æ‹©ï¼ˆç²¾è‹±ç­–ç•¥ï¼‰
            elite_count = max(1, int(population_size * elite_ratio))
            print(f"\nğŸ§¬ é€‰æ‹©: ä¿ç•™ {elite_count} ä¸ªç²¾è‹±åˆ°ä¸‹ä¸€ä»£")
            new_population = population[:elite_count].copy()
            
            # ç¹è¡ï¼ˆäº¤å‰ + å˜å¼‚ï¼‰
            print(f"ğŸ§¬ ç¹è¡: ç”Ÿæˆ {population_size - elite_count} ä¸ªæ–°ä¸ªä½“")
            while len(new_population) < population_size:
                # è½®ç›˜èµŒé€‰æ‹©çˆ¶æ¯ï¼ˆæ›´å€¾å‘äºé€‰æ‹©é«˜åˆ†ä¸ªä½“ï¼‰
                # ç®€åŒ–ç‰ˆï¼šä»å‰50%ä¸­éšæœºé€‰
                parent_pool_size = max(2, population_size // 2)
                parent1 = random.choice(population[:parent_pool_size])
                parent2 = random.choice(population[:parent_pool_size])
                
                # äº¤å‰
                child = crossover(parent1, parent2)
                
                # å˜å¼‚
                child = mutate(child)
                
                new_population.append(child)
            
            population = new_population
        
        # æœ€ç»ˆç»“æœ
        best_result = max(all_results, key=lambda x: x.avg_score)
        
        print(f"\n{'='*60}")
        print(f"ğŸ† é—ä¼ ç®—æ³•å®Œæˆï¼")
        print(f"{'='*60}")
        print(f"ğŸ¥‡ æœ€ç»ˆå† å†›å¾—åˆ†: {best_result.avg_score:.2f}")
        print(f"ğŸ§¬ æœ€ä½³ç»„åˆ: {best_result.role} + {best_result.style} + {best_result.technique}")
        print(f"ğŸ“ˆ è¿›åŒ–å¢ç›Š: {evolution_history[-1]['best_score'] - evolution_history[0]['best_score']:.2f} åˆ†")
        print(f"{'='*60}\n")
        
        return all_results, best_result, evolution_history


# ä¾¿æ·å‡½æ•°
def quick_optimize(user_prompt: str, 
                   api_key: Optional[str] = None,
                   scene: str = "é€šç”¨",
                   mode: str = "é€šç”¨å¢å¼º (General)",
                   provider: str = "nvidia") -> OptimizedPrompt:
    """
    å¿«é€Ÿä¼˜åŒ–å‡½æ•°ï¼Œé€‚åˆç®€å•è°ƒç”¨
    
    Example:
        result = quick_optimize("å†™ä¸ªè´ªåƒè›‡", scene="Pythonåˆå­¦è€…", mode="ä»£ç ç”Ÿæˆ (Coding)")
        print(result.improved_prompt)
    """
    optimizer = PromptOptimizer(api_key=api_key, provider=provider)
    return optimizer.optimize(user_prompt, scene, mode)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    from dotenv import load_dotenv
    load_dotenv()
    
    # ç¤ºä¾‹æµ‹è¯•
    test_prompt = "å†™ä¸ªè´ªåƒè›‡æ¸¸æˆ"
    
    optimizer = PromptOptimizer()
    result = optimizer.optimize(
        test_prompt, 
        scene_desc="Python, ç»™å°å­©å­¦ç¼–ç¨‹ç”¨",
        optimization_mode="ä»£ç ç”Ÿæˆ (Coding)"
    )
    
    print("=" * 50)
    print("ä¼˜åŒ–æ€è€ƒè¿‡ç¨‹ï¼š")
    print(result.thinking_process)
    print("\n" + "=" * 50)
    print("ä¼˜åŒ–åçš„ Promptï¼š")
    print(result.improved_prompt)
    print("\n" + "=" * 50)
    print("ä½¿ç”¨çš„æŠ€æœ¯ï¼š", result.enhancement_techniques)
    print("æ–°å¢å…³é”®è¯ï¼š", result.keywords_added)
