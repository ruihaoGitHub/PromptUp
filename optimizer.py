"""
Prompt ä¼˜åŒ–æ ¸å¿ƒæ¨¡å—
å®ç°è‡ªåŠ¨åŒ–çš„ Prompt ç”Ÿæˆã€ä¼˜åŒ–å’Œè¯„ä¼°
"""
import os
from typing import Optional, Literal
from langchain_openai import ChatOpenAI
from langchain_nvidia_ai_endpoints import ChatNVIDIA
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
import json
from templates import get_strategy_by_scene, OPTIMIZATION_PRINCIPLES


class OptimizedPrompt(BaseModel):
    """ä¼˜åŒ–åçš„ Prompt ç»“æ„"""
    thinking_process: str = Field(description="ä¼˜åŒ–æ—¶çš„æ€è€ƒè¿‡ç¨‹ï¼Œåˆ†æåŸå§‹ Prompt çš„é—®é¢˜å’Œæ”¹è¿›æ–¹å‘")
    improved_prompt: str = Field(description="ä¼˜åŒ–åçš„å®Œæ•´ Promptï¼Œå¯ç›´æ¥ä½¿ç”¨")
    enhancement_techniques: list[str] = Field(description="ä½¿ç”¨çš„ä¼˜åŒ–æŠ€æœ¯ï¼Œå¦‚ï¼šå¢åŠ è§’è‰²è®¾å®šã€æ˜ç¡®è¾“å‡ºæ ¼å¼ç­‰")
    keywords_added: list[str] = Field(description="æ–°å¢çš„å…³é”®è¯å’Œä¸“ä¸šæœ¯è¯­")
    structure_applied: str = Field(description="åº”ç”¨çš„ Prompt æ¡†æ¶åç§°ï¼Œå¦‚ CO-STARã€BROKE ç­‰")


class PromptOptimizer:
    """Prompt è‡ªåŠ¨ä¼˜åŒ–å™¨"""
    
    def __init__(self, 
                 api_key: Optional[str] = None, 
                 model: str = "meta/llama-3.1-405b-instruct", 
                 base_url: Optional[str] = None,
                 provider: Literal["openai", "nvidia"] = "nvidia",
                 temperature: float = 0.7,
                 top_p: float = 0.7,
                 max_tokens: int = 2048):
        """
        åˆå§‹åŒ–ä¼˜åŒ–å™¨
        
        Args:
            api_key: API Keyï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            base_url: API base URL
            provider: API æä¾›å•† ("openai" æˆ– "nvidia")
            temperature: æ¸©åº¦å‚æ•°
            top_p: Top-p é‡‡æ ·å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
        """
        self.provider = provider
        self.model = model
        
        # æ ¹æ®æä¾›å•†åˆå§‹åŒ– LLM
        if provider == "nvidia":
            if api_key:
                os.environ["NVIDIA_API_KEY"] = api_key
            
            llm_params = {
                "model": model,
                "temperature": temperature,
                "top_p": top_p,
                "max_tokens": max_tokens
            }
            if base_url:
                llm_params["base_url"] = base_url
            
            self.llm = ChatNVIDIA(**llm_params)
            
        else:  # openai
            if api_key:
                os.environ["OPENAI_API_KEY"] = api_key
            
            llm_params = {
                "model": model,
                "temperature": temperature,
                "max_tokens": max_tokens
            }
            if base_url:
                llm_params["base_url"] = base_url
            
            self.llm = ChatOpenAI(**llm_params)
    
    def optimize(self, 
                 user_prompt: str, 
                 scene_desc: str = "é€šç”¨",
                 optimization_mode: str = "é€šç”¨å¢å¼º (General)") -> OptimizedPrompt:
        """
        æ ¸å¿ƒä¼˜åŒ–å‡½æ•°
        
        Args:
            user_prompt: ç”¨æˆ·è¾“å…¥çš„åŸå§‹ Prompt
            scene_desc: åœºæ™¯æè¿°
            optimization_mode: ä¼˜åŒ–æ¨¡å¼
            
        Returns:
            OptimizedPrompt: ä¼˜åŒ–åçš„ç»“æ„åŒ– Prompt
        """
        # æ‰“å°ä¼˜åŒ–å¼€å§‹ä¿¡æ¯
        print(f"\n{'='*60}")
        print(f"âš™ï¸  å¼€å§‹ Prompt ä¼˜åŒ–")
        print(f"{'='*60}")
        print(f"ğŸ”Œ API æä¾›å•†: {self.provider.upper()}")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {self.model}")
        print(f"ğŸ¯ ä¼˜åŒ–æ¨¡å¼: {optimization_mode}")
        print(f"ğŸ“ åŸå§‹ Prompt: {user_prompt[:50]}{'...' if len(user_prompt) > 50 else ''}")
        if scene_desc:
            print(f"ğŸ¬ åœºæ™¯æè¿°: {scene_desc[:50]}{'...' if len(scene_desc) > 50 else ''}")
        print(f"{'='*60}\n")
        
        # è·å–åœºæ™¯å¯¹åº”çš„ä¼˜åŒ–ç­–ç•¥
        strategy = get_strategy_by_scene(optimization_mode)
        
        # æ„å»º Meta-Prompt
        system_prompt = self._build_meta_prompt(strategy, scene_desc)
        
        # æ„å»ºæ¶ˆæ¯é“¾
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "ç”¨æˆ·åŸå§‹ Promptï¼š{input}\n\nåœºæ™¯è¡¥å……è¯´æ˜ï¼š{scene}")
        ])
        
        # æ‰§è¡Œä¼˜åŒ–
        try:
            print("ğŸ“¤ æ­£åœ¨è°ƒç”¨ API...")
            
            # æ„å»ºå®Œæ•´æç¤º
            messages = prompt_template.format_messages(
                input=user_prompt,
                scene=scene_desc if scene_desc else "æ— ç‰¹æ®Šè¯´æ˜"
            )
            
            print(f"ğŸ’¬ æ¶ˆæ¯é•¿åº¦: {len(str(messages))} å­—ç¬¦")
            
            # è°ƒç”¨ LLM
            if self.provider == "openai":
                # OpenAI æ”¯æŒ JSON mode
                print("ğŸ”§ ä½¿ç”¨ OpenAI JSON mode")
                response = self.llm.invoke(
                    messages,
                    response_format={"type": "json_object"}
                )
            else:
                # NVIDIA ä½¿ç”¨æ™®é€šè°ƒç”¨
                print("ğŸ”§ ä½¿ç”¨ NVIDIA æ ‡å‡†è°ƒç”¨")
                response = self.llm.invoke(messages)
            
            # è§£æç»“æœ
            content = response.content
            print(f"ğŸ“¥ æ”¶åˆ°å“åº”ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
            print(f"ğŸ“„ å“åº”å‰100å­—ç¬¦: {content[:100]}...")
            
            # å°è¯•æå– JSONï¼ˆå¯èƒ½åŒ…å«åœ¨ markdown ä»£ç å—ä¸­ï¼‰
            if "```json" in content:
                print("ğŸ” æ£€æµ‹åˆ° JSON ä»£ç å—ï¼Œæ­£åœ¨æå–...")
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                print("ğŸ” æ£€æµ‹åˆ°ä»£ç å—ï¼Œæ­£åœ¨æå–...")
                content = content.split("```")[1].split("```")[0].strip()
            
            print("âš™ï¸ æ­£åœ¨è§£æ JSON...")
            result_dict = json.loads(content)
            
            print("âœ… JSON è§£ææˆåŠŸ")
            print("ğŸ”¨ æ­£åœ¨éªŒè¯æ•°æ®ç»“æ„...")
            optimized = OptimizedPrompt(**result_dict)
            
            print("âœ… ä¼˜åŒ–å®Œæˆï¼")
            print(f"{'='*60}\n")
            
            return optimized
            
        except Exception as e:
            # é”™è¯¯å¤„ç†ï¼šè¯¦ç»†è®°å½•åˆ°ç»ˆç«¯
            print(f"\nâŒ ä¼˜åŒ–å¤±è´¥ï¼")
            print(f"{'='*60}")
            
            error_msg = str(e)
            print(f"ğŸ› é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"ğŸ“ é”™è¯¯è¯¦æƒ…: {error_msg[:500]}")
            
            # å¦‚æœæ˜¯ Pydantic éªŒè¯é”™è¯¯ï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯
            if "validation" in error_msg.lower() or "Field required" in error_msg:
                print("\nâš ï¸ è¿™æ˜¯æ•°æ®ç»“æ„éªŒè¯é”™è¯¯ï¼Œå¯èƒ½åŸå› ï¼š")
                print("   1. æ¨¡å‹è¿”å›çš„ JSON æ ¼å¼ä¸ç¬¦åˆè¦æ±‚")
                print("   2. ç¼ºå°‘å¿…éœ€çš„å­—æ®µï¼ˆthinking_process, improved_prompt ç­‰ï¼‰")
                print("   3. æ¨¡å‹å¯èƒ½ä¸æ”¯æŒ JSON æ ¼å¼è¾“å‡º")
                print("\nğŸ’¡ å»ºè®®ï¼šå°è¯•æ›´æ¢æ¨¡å‹ï¼Œæ¨èä½¿ç”¨ meta/llama-3.1-405b-instruct")
            
            # æ‰“å°å®Œæ•´å †æ ˆ
            import traceback
            print(f"\nğŸ“„ å®Œæ•´å †æ ˆä¿¡æ¯ï¼š")
            traceback.print_exc()
            print(f"{'='*60}\n")
            
            # æ ¹æ®é”™è¯¯ç±»å‹æŠ›å‡ºæ˜ç¡®çš„å¼‚å¸¸
            if "404" in error_msg:
                raise Exception(f"API è°ƒç”¨å¤±è´¥ (404): è¯·æ£€æŸ¥ API Key æ˜¯å¦æœ‰æ•ˆï¼Œæˆ–æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®ã€‚è¯¦ç»†ä¿¡æ¯ï¼š{error_msg[:200]}")
            elif "401" in error_msg or "Unauthorized" in error_msg:
                raise Exception(f"API Key æ— æ•ˆæˆ–å·²è¿‡æœŸã€‚è¯·æ£€æŸ¥æ‚¨çš„ API Key é…ç½®ã€‚")
            elif "rate_limit" in error_msg.lower():
                raise Exception(f"API è¯·æ±‚é¢‘ç‡è¶…é™ï¼Œè¯·ç¨åå†è¯•ã€‚")
            else:
                raise Exception(f"ä¼˜åŒ–å¤±è´¥: {error_msg[:300]}")
    
    def _build_meta_prompt(self, strategy: dict, scene_desc: str) -> str:
        """æ„å»º Meta-Promptï¼ˆæ•™ LLM å¦‚ä½•ä¼˜åŒ– Prompt çš„æç¤ºè¯ï¼‰"""
        
        template_name = strategy.get("template", "CO-STAR")
        focus_principles = strategy.get("focus", ["clarity", "structure"])
        extra_requirements = strategy.get("extra_requirements", [])
        
        # è·å–ç„¦ç‚¹åŸåˆ™çš„è¯¦ç»†è¯´æ˜
        principles_text = "\n".join([
            f"   - {OPTIMIZATION_PRINCIPLES.get(p, p)}"
            for p in focus_principles
        ])
        
        # æ„å»ºé¢å¤–è¦æ±‚æ–‡æœ¬
        extra_text = ""
        if extra_requirements:
            extra_text = "\n\n**åœºæ™¯ç‰¹å®šè¦æ±‚**ï¼š\n" + "\n".join([
                f"   - {req}" for req in extra_requirements
            ])
        
        meta_prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸–ç•Œçº§çš„ Prompt Engineering ä¸“å®¶ï¼Œæ“…é•¿å°†ç®€å•çš„æŒ‡ä»¤è½¬åŒ–ä¸ºç»“æ„åŒ–ã€é«˜æ€§èƒ½çš„ä¸“å®¶çº§ Promptã€‚

**ä½ çš„ä»»åŠ¡æµç¨‹**ï¼š

1. **æ·±åº¦ç†è§£**ï¼šä»”ç»†åˆ†æç”¨æˆ·çš„åŸå§‹ Promptï¼Œè¯†åˆ«å…¶æ ¸å¿ƒæ„å›¾å’Œéšå«éœ€æ±‚

2. **ä¸‰å¤§ä¼˜åŒ–ç­–ç•¥**ï¼š
   
   a) **è¯­ä¹‰æ‰©å±• (Semantic Expansion)**
      - è¡¥å……ç¼ºå¤±çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
      - æ˜ç¡®éšå«çš„çº¦æŸæ¡ä»¶
      - è§„èŒƒè¾“å‡ºæ ¼å¼è¦æ±‚
   
   b) **å…³é”®è¯å¢å¼º (Keywords Enhancement)**
      - è¯†åˆ«ä»»åŠ¡æ‰€å±çš„ä¸“ä¸šé¢†åŸŸ
      - åŠ å…¥è¯¥é¢†åŸŸçš„ä¸“ä¸šæœ¯è¯­å’Œè¡Œä¸šæ¦‚å¿µ
      - ç”¨ç²¾ç¡®çš„è¯æ±‡æ›¿æ¢æ¨¡ç³Šè¡¨è¾¾
   
   c) **ç»“æ„åŒ–é‡å†™ (Template Application)**
      - å¿…é¡»ä½¿ç”¨ **{template_name}** æ¡†æ¶è¿›è¡Œé‡å†™
      - ç¡®ä¿ Prompt é€»è¾‘æ¸…æ™°ã€å±‚æ¬¡åˆ†æ˜

3. **ä¼˜åŒ–åŸåˆ™**ï¼ˆæœ¬æ¬¡ä¼˜åŒ–é‡ç‚¹å…³æ³¨ï¼‰ï¼š
{principles_text}
{extra_text}

**åœºæ™¯ä¸Šä¸‹æ–‡**ï¼š{scene_desc if scene_desc else "é€šç”¨åœºæ™¯"}

**è¾“å‡ºè¦æ±‚**ï¼š
è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- thinking_process: ä½ çš„ä¼˜åŒ–æ€è€ƒè¿‡ç¨‹ï¼ˆ200å­—å·¦å³ï¼‰
- improved_prompt: ä¼˜åŒ–åçš„å®Œæ•´ Promptï¼ˆå¯ç›´æ¥ä½¿ç”¨ï¼‰
- enhancement_techniques: ä½¿ç”¨çš„ä¼˜åŒ–æŠ€æœ¯åˆ—è¡¨
- keywords_added: æ–°å¢çš„å…³é”®è¯åˆ—è¡¨
- structure_applied: åº”ç”¨çš„æ¡†æ¶åç§°

**é‡è¦**ï¼šimproved_prompt åº”è¯¥æ˜¯ä¸€ä¸ªå®Œæ•´çš„ã€å¯ä»¥ç›´æ¥å¤åˆ¶ä½¿ç”¨çš„é«˜è´¨é‡ Promptï¼Œä¸è¦åŒ…å«ä»»ä½•å…ƒä¿¡æ¯æˆ–è¯´æ˜ã€‚
"""
        return meta_prompt
    
    def _fallback_optimization(self, original_prompt: str, error: str) -> OptimizedPrompt:
        """å½“ä¼˜åŒ–å¤±è´¥æ—¶çš„å¤‡ç”¨æ–¹æ¡ˆ"""
        return OptimizedPrompt(
            thinking_process=f"ä¼˜åŒ–è¿‡ç¨‹ä¸­é‡åˆ°é”™è¯¯ï¼š{error}ã€‚ä»¥ä¸‹æ˜¯åŸºç¡€ä¼˜åŒ–ç‰ˆæœ¬ã€‚",
            improved_prompt=f"""
è¯·ä»¥ä¸“ä¸šçš„æ€åº¦å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š

{original_prompt}

è¦æ±‚ï¼š
1. è¾“å‡ºå†…å®¹åº”è¯¥æ¸…æ™°ã€å‡†ç¡®ã€å®Œæ•´
2. ä½¿ç”¨æ°å½“çš„æ ¼å¼ç»„ç»‡ä¿¡æ¯
3. æ³¨é‡ç»†èŠ‚å’Œä¸“ä¸šæ€§
4. å¦‚æœ‰éœ€è¦ï¼Œè¯·å±•ç¤ºä½ çš„æ€è€ƒè¿‡ç¨‹
""",
            enhancement_techniques=["åŸºç¡€ç»“æ„åŒ–", "æ·»åŠ é€šç”¨è¦æ±‚"],
            keywords_added=[],
            structure_applied="ç®€å•ä¼˜åŒ–"
        )
    
    def compare_results(self, original_prompt: str, optimized_prompt: str, 
                       test_query: Optional[str] = None) -> tuple[str, str]:
        """
        A/B å¯¹æ¯”æµ‹è¯•ï¼šåˆ†åˆ«è¿è¡ŒåŸå§‹å’Œä¼˜åŒ–åçš„ Prompt
        
        Args:
            original_prompt: åŸå§‹ Prompt
            optimized_prompt: ä¼˜åŒ–åçš„ Prompt
            test_query: å¯é€‰çš„æµ‹è¯•æŸ¥è¯¢ï¼ˆå¦‚æœ Prompt æœ¬èº«ä¸æ˜¯ç›´æ¥çš„é—®é¢˜ï¼‰
            
        Returns:
            (åŸå§‹ç»“æœ, ä¼˜åŒ–åç»“æœ)
        """
        try:
            # è¿è¡ŒåŸå§‹ Prompt
            response_original = self.llm.invoke(original_prompt)
            result_original = response_original.content
            
            # è¿è¡Œä¼˜åŒ–åçš„ Prompt
            response_optimized = self.llm.invoke(optimized_prompt)
            result_optimized = response_optimized.content
            
            return result_original, result_optimized
            
        except Exception as e:
            return f"è¿è¡Œå¤±è´¥: {str(e)}", f"è¿è¡Œå¤±è´¥: {str(e)}"
    
    def batch_optimize(self, prompts: list[str], 
                       scene_desc: str = "é€šç”¨",
                       optimization_mode: str = "é€šç”¨å¢å¼º (General)") -> list[OptimizedPrompt]:
        """
        æ‰¹é‡ä¼˜åŒ–å¤šä¸ª Prompt
        
        Args:
            prompts: Prompt åˆ—è¡¨
            scene_desc: åœºæ™¯æè¿°
            optimization_mode: ä¼˜åŒ–æ¨¡å¼
            
        Returns:
            ä¼˜åŒ–ç»“æœåˆ—è¡¨
        """
        results = []
        for prompt in prompts:
            result = self.optimize(prompt, scene_desc, optimization_mode)
            results.append(result)
        return results


# ä¾¿æ·å‡½æ•°
def quick_optimize(user_prompt: str, 
                   api_key: Optional[str] = None,
                   scene: str = "é€šç”¨",
                   mode: str = "é€šç”¨å¢å¼º (General)",
                   provider: str = "nvidia") -> OptimizedPrompt:
    """
    å¿«é€Ÿä¼˜åŒ–å‡½æ•°ï¼Œé€‚åˆç®€å•è°ƒç”¨
    
    Example:
        result = quick_optimize("å†™ä¸ªè´ªåƒè›‡", scene="Pythonåˆå­¦è€…", mode="ä»£ç ç”Ÿæˆ (Coding)")
        print(result.improved_prompt)
    """
    optimizer = PromptOptimizer(api_key=api_key, provider=provider)
    return optimizer.optimize(user_prompt, scene, mode)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    from dotenv import load_dotenv
    load_dotenv()
    
    # ç¤ºä¾‹æµ‹è¯•
    test_prompt = "å†™ä¸ªè´ªåƒè›‡æ¸¸æˆ"
    
    optimizer = PromptOptimizer()
    result = optimizer.optimize(
        test_prompt, 
        scene_desc="Python, ç»™å°å­©å­¦ç¼–ç¨‹ç”¨",
        optimization_mode="ä»£ç ç”Ÿæˆ (Coding)"
    )
    
    print("=" * 50)
    print("ä¼˜åŒ–æ€è€ƒè¿‡ç¨‹ï¼š")
    print(result.thinking_process)
    print("\n" + "=" * 50)
    print("ä¼˜åŒ–åçš„ Promptï¼š")
    print(result.improved_prompt)
    print("\n" + "=" * 50)
    print("ä½¿ç”¨çš„æŠ€æœ¯ï¼š", result.enhancement_techniques)
    print("æ–°å¢å…³é”®è¯ï¼š", result.keywords_added)
