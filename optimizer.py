"""
Prompt ä¼˜åŒ–æ ¸å¿ƒæ¨¡å—
å®ç°è‡ªåŠ¨åŒ–çš„ Prompt ç”Ÿæˆã€ä¼˜åŒ–å’Œè¯„ä¼°
"""
import time
from typing import Optional, Literal
from langchain_core.prompts import ChatPromptTemplate
from templates import get_strategy_by_scene, OPTIMIZATION_PRINCIPLES
from config.models import OptimizedPrompt, ClassificationPrompt, SummarizationPrompt, TranslationPrompt, SearchSpace, SearchResult
from config.template_loader import get_generation_meta_prompt
from optimizers import ClassificationOptimizer, SummarizationOptimizer, TranslationOptimizer
from algorithms import SearchSpaceGenerator, RandomSearchAlgorithm, GeneticAlgorithm, BayesianOptimization
from services import LLMService, ResponseParser

try:
    import optuna
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    print("âš ï¸ Optuna æœªå®‰è£…ï¼Œè´å¶æ–¯ä¼˜åŒ–åŠŸèƒ½ä¸å¯ç”¨ã€‚è¿è¡Œ: pip install optuna")


class PromptOptimizer:
    """Prompt è‡ªåŠ¨ä¼˜åŒ–å™¨"""
    
    def __init__(self, 
                 api_key: Optional[str] = None, 
                 model: str = "meta/llama-3.1-405b-instruct", 
                 base_url: Optional[str] = None,
                 provider: Literal["openai", "nvidia"] = "nvidia",
                 temperature: float = 0.7,
                 top_p: float = 0.7,
                 max_tokens: int = 2048):
        """
        åˆå§‹åŒ–ä¼˜åŒ–å™¨
        
        Args:
            api_key: API Keyï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            base_url: API base URL
            provider: API æä¾›å•† ("openai" æˆ– "nvidia")
            temperature: æ¸©åº¦å‚æ•°
            top_p: Top-p é‡‡æ ·å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
        """
        self.provider = provider
        self.model = model
        
        # ä½¿ç”¨ LLMService åˆ›å»º LLM å®ä¾‹
        self.llm = LLMService.create_llm(
            provider=provider,
            api_key=api_key,
            model=model,
            base_url=base_url,
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens
        )
        
        # åˆå§‹åŒ–ä»»åŠ¡ä¼˜åŒ–å™¨
        self.classification_optimizer = ClassificationOptimizer(self.llm, provider, model)
        self.summarization_optimizer = SummarizationOptimizer(self.llm, provider, model)
        self.translation_optimizer = TranslationOptimizer(self.llm, provider, model)
        
        # åˆå§‹åŒ–æœç´¢ç®—æ³•
        self.search_space_generator = SearchSpaceGenerator(self.llm, provider)
        self.random_search = RandomSearchAlgorithm(self.llm)
        self.genetic_algorithm = GeneticAlgorithm(self.llm)
        self.bayesian_optimization = BayesianOptimization(self.llm)
    
    def optimize(self, 
                 user_prompt: str, 
                 scene_desc: str = "é€šç”¨",
                 optimization_mode: str = "é€šç”¨å¢å¼º (General)") -> OptimizedPrompt:
        """
        æ ¸å¿ƒä¼˜åŒ–å‡½æ•°
        
        Args:
            user_prompt: ç”¨æˆ·è¾“å…¥çš„åŸå§‹ Prompt
            scene_desc: åœºæ™¯æè¿°
            optimization_mode: ä¼˜åŒ–æ¨¡å¼
            
        Returns:
            OptimizedPrompt: ä¼˜åŒ–åçš„ç»“æ„åŒ– Prompt
        """
        # æ‰“å°ä¼˜åŒ–å¼€å§‹ä¿¡æ¯
        print(f"\n{'='*60}")
        print(f"âš™ï¸  å¼€å§‹ Prompt ä¼˜åŒ–")
        print(f"{'='*60}")
        print(f"ğŸ”Œ API æä¾›å•†: {self.provider.upper()}")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {self.model}")
        print(f"ğŸ¯ ä¼˜åŒ–æ¨¡å¼: {optimization_mode}")
        print(f"ğŸ“ åŸå§‹ Prompt: {user_prompt[:50]}{'...' if len(user_prompt) > 50 else ''}")
        if scene_desc:
            print(f"ğŸ¬ åœºæ™¯æè¿°: {scene_desc[:50]}{'...' if len(scene_desc) > 50 else ''}")
        print(f"{'='*60}\n")
        
        # è·å–åœºæ™¯å¯¹åº”çš„ä¼˜åŒ–ç­–ç•¥
        strategy = get_strategy_by_scene(optimization_mode)
        
        # æ„å»º Meta-Prompt
        system_prompt = self._build_meta_prompt(strategy, scene_desc)
        
        # æ„å»ºæ¶ˆæ¯é“¾
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "ç”¨æˆ·åŸå§‹ Promptï¼š{input}\n\nåœºæ™¯è¡¥å……è¯´æ˜ï¼š{scene}")
        ])
        
        # æ‰§è¡Œä¼˜åŒ–
        try:
            print("ğŸ“¤ æ­£åœ¨è°ƒç”¨ API...")
            
            # æ„å»ºå®Œæ•´æç¤º
            messages = prompt_template.format_messages(
                input=user_prompt,
                scene=scene_desc if scene_desc else "æ— ç‰¹æ®Šè¯´æ˜"
            )
            
            print(f"ğŸ’¬ æ¶ˆæ¯é•¿åº¦: {len(str(messages))} å­—ç¬¦")
            
            # è°ƒç”¨ LLMï¼ˆæ ¹æ®æä¾›å•†é€‰æ‹©æ˜¯å¦ä½¿ç”¨ JSON modeï¼‰
            if LLMService.supports_json_mode(self.provider):
                print("ğŸ”§ ä½¿ç”¨ JSON mode")
                response = self.llm.invoke(
                    messages,
                    response_format={"type": "json_object"}
                )
            else:
                print("ğŸ”§ ä½¿ç”¨æ ‡å‡†è°ƒç”¨")
                response = self.llm.invoke(messages)
            
            time.sleep(0.5)  # API è°ƒç”¨å»¶è¿Ÿ
            
            # ä½¿ç”¨ ResponseParser è§£æç»“æœ
            content = response.content
            print(f"ğŸ“¥ æ”¶åˆ°å“åº”ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
            print(f"ğŸ“„ å“åº”å‰100å­—ç¬¦: {content[:100]}...")
            
            # è§£æ JSON å“åº”
            result_dict = ResponseParser.parse_optimization_response(content)
            
            print("ğŸ”¨ æ­£åœ¨éªŒè¯æ•°æ®ç»“æ„...")
            optimized = OptimizedPrompt(**result_dict)
            
            # æ¸…ç† improved_prompt å­—æ®µ
            original_prompt = optimized.improved_prompt
            cleaned_prompt, was_cleaned = ResponseParser.clean_prompt_field(original_prompt)
            
            if was_cleaned:
                # åˆ›å»ºæ–°çš„ä¼˜åŒ–ç»“æœå¯¹è±¡
                optimized = OptimizedPrompt(
                    thinking_process=optimized.thinking_process,
                    improved_prompt=cleaned_prompt,
                    enhancement_techniques=optimized.enhancement_techniques,
                    keywords_added=optimized.keywords_added,
                    structure_applied=optimized.structure_applied
                )
            
            print("âœ… ä¼˜åŒ–å®Œæˆï¼")
            print(f"{'='*60}\n")
            
            return optimized
            
        except Exception as e:
            # é”™è¯¯å¤„ç†ï¼šè¯¦ç»†è®°å½•åˆ°ç»ˆç«¯
            print(f"\nâŒ ä¼˜åŒ–å¤±è´¥ï¼")
            print(f"{'='*60}")
            
            error_msg = str(e)
            print(f"ğŸ› é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"ğŸ“ é”™è¯¯è¯¦æƒ…: {error_msg[:500]}")
            
            # å¦‚æœæ˜¯ Pydantic éªŒè¯é”™è¯¯ï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯
            if "validation" in error_msg.lower() or "Field required" in error_msg:
                print("\nâš ï¸ è¿™æ˜¯æ•°æ®ç»“æ„éªŒè¯é”™è¯¯ï¼Œå¯èƒ½åŸå› ï¼š")
                print("   1. æ¨¡å‹è¿”å›çš„ JSON æ ¼å¼ä¸ç¬¦åˆè¦æ±‚")
                print("   2. ç¼ºå°‘å¿…éœ€çš„å­—æ®µï¼ˆthinking_process, improved_prompt ç­‰ï¼‰")
                print("   3. æ¨¡å‹å¯èƒ½ä¸æ”¯æŒ JSON æ ¼å¼è¾“å‡º")
                print("\nğŸ’¡ å»ºè®®ï¼šå°è¯•æ›´æ¢æ¨¡å‹ï¼Œæ¨èä½¿ç”¨ meta/llama-3.1-405b-instruct")
            
            # æ‰“å°å®Œæ•´å †æ ˆ
            import traceback
            print(f"\nğŸ“„ å®Œæ•´å †æ ˆä¿¡æ¯ï¼š")
            traceback.print_exc()
            print(f"{'='*60}\n")
            
            # æ ¹æ®é”™è¯¯ç±»å‹æŠ›å‡ºæ˜ç¡®çš„å¼‚å¸¸
            if "404" in error_msg:
                raise Exception(f"API è°ƒç”¨å¤±è´¥ (404): è¯·æ£€æŸ¥ API Key æ˜¯å¦æœ‰æ•ˆï¼Œæˆ–æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®ã€‚è¯¦ç»†ä¿¡æ¯ï¼š{error_msg[:200]}")
            elif "401" in error_msg or "Unauthorized" in error_msg:
                raise Exception(f"API Key æ— æ•ˆæˆ–å·²è¿‡æœŸã€‚è¯·æ£€æŸ¥æ‚¨çš„ API Key é…ç½®ã€‚")
            elif "rate_limit" in error_msg.lower():
                raise Exception(f"API è¯·æ±‚é¢‘ç‡è¶…é™ï¼Œè¯·ç¨åå†è¯•ã€‚")
            else:
                raise Exception(f"ä¼˜åŒ–å¤±è´¥: {error_msg[:300]}")
    
    def optimize_classification(self,
                               task_description: str,
                               labels: list[str]) -> ClassificationPrompt:
        """
        é’ˆå¯¹åˆ†ç±»ä»»åŠ¡çš„ä¼˜åŒ–å‡½æ•°
        
        Args:
            task_description: åˆ†ç±»ä»»åŠ¡æè¿°ï¼Œå¦‚ "åˆ¤æ–­ç”¨æˆ·è¯„è®ºçš„æƒ…æ„Ÿå€¾å‘"
            labels: ç›®æ ‡æ ‡ç­¾åˆ—è¡¨ï¼Œå¦‚ ["Positive", "Negative", "Neutral"]
            
        Returns:
            ClassificationPrompt: ä¼˜åŒ–åçš„åˆ†ç±» Prompt
        """
        return self.classification_optimizer.optimize(task_description, labels)
    
    def optimize_summarization(self,
                              task_description: str,
                              source_type: str,
                              target_audience: str,
                              focus_points: str,
                              length_constraint: Optional[str] = None) -> SummarizationPrompt:
        """
        é’ˆå¯¹æ‘˜è¦ä»»åŠ¡çš„ä¼˜åŒ–å‡½æ•°
        
        Args:
            task_description: æ‘˜è¦ä»»åŠ¡æè¿°ï¼Œå¦‚ "æ€»ç»“æŠ€æœ¯ä¼šè®®çš„æ ¸å¿ƒå†³ç­–"
            source_type: æºæ–‡æœ¬ç±»å‹ï¼Œå¦‚ "ä¼šè®®è®°å½•"ã€"å­¦æœ¯è®ºæ–‡"ã€"æ–°é—»æŠ¥é“"
            target_audience: ç›®æ ‡è¯»è€…ï¼Œå¦‚ "æŠ€æœ¯ç»ç†"ã€"æ™®é€šç”¨æˆ·"
            focus_points: æ ¸å¿ƒå…³æ³¨ç‚¹ï¼Œå¦‚ "è¡ŒåŠ¨è®¡åˆ’å’Œè´Ÿè´£äºº"
            length_constraint: ç¯‡å¹…é™åˆ¶ï¼Œå¦‚ "100å­—ä»¥å†…"ã€"3-5ä¸ªè¦ç‚¹"
            
        Returns:
            SummarizationPrompt: ä¼˜åŒ–åçš„æ‘˜è¦ Prompt
        """
        return self.summarization_optimizer.optimize(
            task_description, source_type, target_audience, focus_points, length_constraint
        )
    
    def optimize_translation(self,
                           source_lang: str,
                           target_lang: str,
                           domain: str,
                           tone: str,
                           user_glossary: str = "") -> TranslationPrompt:
        """
        é’ˆå¯¹ç¿»è¯‘ä»»åŠ¡çš„ä¼˜åŒ–å‡½æ•°
        
        Args:
            source_lang: æºè¯­è¨€ï¼Œå¦‚ "ä¸­æ–‡"ã€"è‹±æ–‡"
            target_lang: ç›®æ ‡è¯­è¨€
            domain: åº”ç”¨é¢†åŸŸï¼Œå¦‚ "é€šç”¨æ—¥å¸¸"ã€"IT/æŠ€æœ¯æ–‡æ¡£"ã€"æ³•å¾‹åˆåŒ"ç­‰
            tone: æœŸæœ›é£æ ¼ï¼Œå¦‚ "æ ‡å‡†/å‡†ç¡®"ã€"åœ°é“/å£è¯­åŒ–"
            user_glossary: ç”¨æˆ·æä¾›çš„æœ¯è¯­è¡¨ï¼Œæ ¼å¼å¦‚ "Prompt=æç¤ºè¯\nLLM=å¤§è¯­è¨€æ¨¡å‹"
            
        Returns:
            TranslationPrompt: ä¼˜åŒ–åçš„ç¿»è¯‘ Prompt
        """
        return self.translation_optimizer.optimize(
            source_lang, target_lang, domain, tone, user_glossary
        )
    
    def _build_meta_prompt(self, strategy: dict, scene_desc: str) -> str:
        """æ„å»º Meta-Promptï¼ˆæ•™ LLM å¦‚ä½•ä¼˜åŒ– Prompt çš„æç¤ºè¯ï¼‰"""
        
        template_name = strategy.get("template", "CO-STAR")
        focus_principles = strategy.get("focus", ["clarity", "structure"])
        extra_requirements = strategy.get("extra_requirements", [])
        
        # ä½¿ç”¨å¤–éƒ¨æ¨¡æ¿åŠ è½½ Meta-Prompt
        return get_generation_meta_prompt(
            template_name,
            focus_principles,
            extra_requirements,
            scene_desc,
            OPTIMIZATION_PRINCIPLES
        )
    
    def _fallback_optimization(self, original_prompt: str, error: str) -> OptimizedPrompt:
        """å½“ä¼˜åŒ–å¤±è´¥æ—¶çš„å¤‡ç”¨æ–¹æ¡ˆ"""
        return OptimizedPrompt(
            thinking_process=f"ä¼˜åŒ–è¿‡ç¨‹ä¸­é‡åˆ°é”™è¯¯ï¼š{error}ã€‚ä»¥ä¸‹æ˜¯åŸºç¡€ä¼˜åŒ–ç‰ˆæœ¬ã€‚",
            improved_prompt=f"""
è¯·ä»¥ä¸“ä¸šçš„æ€åº¦å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š

{original_prompt}

è¦æ±‚ï¼š
1. è¾“å‡ºå†…å®¹åº”è¯¥æ¸…æ™°ã€å‡†ç¡®ã€å®Œæ•´
2. ä½¿ç”¨æ°å½“çš„æ ¼å¼ç»„ç»‡ä¿¡æ¯
3. æ³¨é‡ç»†èŠ‚å’Œä¸“ä¸šæ€§
4. å¦‚æœ‰éœ€è¦ï¼Œè¯·å±•ç¤ºä½ çš„æ€è€ƒè¿‡ç¨‹
""",
            enhancement_techniques=["åŸºç¡€ç»“æ„åŒ–", "æ·»åŠ é€šç”¨è¦æ±‚"],
            keywords_added=[],
            structure_applied="ç®€å•ä¼˜åŒ–"
        )
    
    def compare_results(self, original_prompt: str, optimized_prompt: str, 
                       test_query: Optional[str] = None) -> tuple[str, str]:
        """
        A/B å¯¹æ¯”æµ‹è¯•ï¼šåˆ†åˆ«è¿è¡ŒåŸå§‹å’Œä¼˜åŒ–åçš„ Prompt
        
        Args:
            original_prompt: åŸå§‹ Prompt
            optimized_prompt: ä¼˜åŒ–åçš„ Prompt
            test_query: å¯é€‰çš„æµ‹è¯•æŸ¥è¯¢ï¼ˆå¦‚æœ Prompt æœ¬èº«ä¸æ˜¯ç›´æ¥çš„é—®é¢˜ï¼‰
            
        Returns:
            (åŸå§‹ç»“æœ, ä¼˜åŒ–åç»“æœ)
        """
        try:
            # è¿è¡ŒåŸå§‹ Prompt
            response_original = self.llm.invoke(original_prompt)
            time.sleep(0.3)  # API è°ƒç”¨å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡è¿‡å¿«ï¼ˆA/B æµ‹è¯•éœ€è¦è¾ƒé•¿ç­‰å¾…ï¼‰
            result_original = response_original.content
            
            # è¿è¡Œä¼˜åŒ–åçš„ Prompt
            response_optimized = self.llm.invoke(optimized_prompt)
            time.sleep(0.3)  # API è°ƒç”¨å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡è¿‡å¿«ï¼ˆA/B æµ‹è¯•éœ€è¦è¾ƒé•¿ç­‰å¾…ï¼‰
            result_optimized = response_optimized.content
            
            return result_original, result_optimized
            
        except Exception as e:
            return f"è¿è¡Œå¤±è´¥: {str(e)}", f"è¿è¡Œå¤±è´¥: {str(e)}"
    
    def batch_optimize(self, prompts: list[str], 
                       scene_desc: str = "é€šç”¨",
                       optimization_mode: str = "é€šç”¨å¢å¼º (General)") -> list[OptimizedPrompt]:
        """
        æ‰¹é‡ä¼˜åŒ–å¤šä¸ª Prompt
        
        Args:
            prompts: Prompt åˆ—è¡¨
            scene_desc: åœºæ™¯æè¿°
            optimization_mode: ä¼˜åŒ–æ¨¡å¼
            
        Returns:
            ä¼˜åŒ–ç»“æœåˆ—è¡¨
        """
        results = []
        for prompt in prompts:
            result = self.optimize(prompt, scene_desc, optimization_mode)
            results.append(result)
        return results


    def generate_search_space(self, task_description: str, task_type: str = "classification") -> SearchSpace:
        """
        è®© LLM è‡ªåŠ¨åˆ†æä»»åŠ¡ï¼Œç”Ÿæˆå¯ä¾›æœç´¢çš„å˜é‡æ± 
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            task_type: ä»»åŠ¡ç±»å‹ (classification/summarization/translation)
            
        Returns:
            SearchSpace å¯¹è±¡ï¼ŒåŒ…å« roles, styles, techniques
        """
        return self.search_space_generator.generate(task_description, task_type)
    
    
    def run_random_search(
        self, 
        task_description: str,
        task_type: str,
        test_dataset: list[dict],
        search_space: SearchSpace,
        iterations: int = 5,
        progress_callback=None,
        labels: list[str] = None
    ) -> tuple[list[SearchResult], SearchResult]:
        """
        æ‰§è¡Œéšæœºæœç´¢ä¼˜åŒ–
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            task_type: ä»»åŠ¡ç±»å‹ (classification/summarization/translation)
            test_dataset: æµ‹è¯•æ•°æ®é›† [{'input': '...', 'ground_truth': '...'}]
            search_space: æœç´¢ç©ºé—´
            iterations: æœç´¢è¿­ä»£æ¬¡æ•°
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° callback(current, total, message)
            labels: åˆ†ç±»ä»»åŠ¡çš„æ ‡ç­¾åˆ—è¡¨ï¼ˆä»…åˆ†ç±»ä»»åŠ¡éœ€è¦ï¼‰
            
        Returns:
            (æ‰€æœ‰ç»“æœåˆ—è¡¨, æœ€ä½³ç»“æœ)
        """
        return self.random_search.run(
            task_description, task_type, test_dataset, search_space, iterations, progress_callback, labels
        )
    
    def run_genetic_algorithm(
        self,
        task_description: str,
        task_type: str,
        test_dataset: list,
        search_space: 'SearchSpace',
        generations: int = 5,
        population_size: int = 8,
        elite_ratio: float = 0.2,
        mutation_rate: float = 0.2,
        progress_callback: Optional[callable] = None
    ) -> tuple[list, 'SearchResult', list]:
        """
        é—ä¼ ç®—æ³•ä¼˜åŒ– Prompt
        
        å§”æ‰˜ç»™ GeneticAlgorithm ç±»æ‰§è¡Œ
        """
        return self.genetic_algorithm.run(
            task_description, task_type, test_dataset, search_space,
            generations, population_size, elite_ratio, mutation_rate, progress_callback
        )

    def run_bayesian_optimization(
        self,
        task_description: str,
        task_type: str,
        test_dataset: list,
        search_space: 'SearchSpace',
        n_trials: int = 20,
        progress_callback: Optional[callable] = None
    ) -> tuple[list, 'SearchResult', list]:
        """
        è´å¶æ–¯ä¼˜åŒ– Prompt
        
        å§”æ‰˜ç»™ BayesianOptimization ç±»æ‰§è¡Œ
        """
        return self.bayesian_optimization.run(
            task_description, task_type, test_dataset, search_space,
            n_trials, progress_callback
        )


# ä¾¿æ·å‡½æ•°
def quick_optimize(user_prompt: str, 
                   api_key: Optional[str] = None,
                   scene: str = "é€šç”¨",
                   mode: str = "é€šç”¨å¢å¼º (General)",
                   provider: str = "nvidia") -> OptimizedPrompt:
    """
    å¿«é€Ÿä¼˜åŒ–å‡½æ•°ï¼Œé€‚åˆç®€å•è°ƒç”¨
    
    Example:
        result = quick_optimize("å†™ä¸ªè´ªåƒè›‡", scene="Pythonåˆå­¦è€…", mode="ä»£ç ç”Ÿæˆ (Coding)")
        print(result.improved_prompt)
    """
    optimizer = PromptOptimizer(api_key=api_key, provider=provider)
    return optimizer.optimize(user_prompt, scene, mode)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    from dotenv import load_dotenv
    load_dotenv()
    
    # ç¤ºä¾‹æµ‹è¯•
    test_prompt = "å†™ä¸ªè´ªåƒè›‡æ¸¸æˆ"
    
    optimizer = PromptOptimizer()
    result = optimizer.optimize(
        test_prompt, 
        scene_desc="Python, ç»™å°å­©å­¦ç¼–ç¨‹ç”¨",
        optimization_mode="ä»£ç ç”Ÿæˆ (Coding)"
    )
    
    print("=" * 50)
    print("ä¼˜åŒ–æ€è€ƒè¿‡ç¨‹ï¼š")
    print(result.thinking_process)
    print("\n" + "=" * 50)
    print("ä¼˜åŒ–åçš„ Promptï¼š")
    print(result.improved_prompt)
    print("\n" + "=" * 50)
    print("ä½¿ç”¨çš„æŠ€æœ¯ï¼š", result.enhancement_techniques)
    print("æ–°å¢å…³é”®è¯ï¼š", result.keywords_added)
