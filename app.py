"""
AI Prompt è‡ªåŠ¨ä¼˜åŒ–ç³»ç»Ÿ - Streamlit ä¸»ç•Œé¢
é‡æ„åçš„ç²¾ç®€ç‰ˆæœ¬ï¼Œä½¿ç”¨é¡µé¢æ¨¡å—å’Œ UI ç»„ä»¶ç³»ç»Ÿ
"""
import streamlit as st
import pandas as pd
from config.defaults import get_default_value, get_default_dataset
from config.defaults import get_default_value
from dotenv import load_dotenv
from optimizer import PromptOptimizer
from page_modules import (
    GenerationPage,
    ClassificationPage, 
    SummarizationPage,
    TranslationPage
)
from ui import apply_custom_styles, render_sidebar

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# åº”ç”¨è‡ªå®šä¹‰æ ·å¼ï¼ˆåŒ…å«é¡µé¢é…ç½®ï¼‰
apply_custom_styles()

# æ ‡é¢˜åŒºåŸŸ
st.markdown('<p class="main-header">ğŸš€ AI Prompt è‡ªåŠ¨ä¼˜åŒ–å¤§å¸ˆ</p>', unsafe_allow_html=True)

# æ¸²æŸ“ä¾§è¾¹æ å¹¶è·å–é…ç½®
config = render_sidebar()
task_type = config['task_type']
api_provider = config['api_provider']
api_key_input = config['api_key']
base_url = config['base_url']
model_choice = config['model']

# æ ¹æ®ä»»åŠ¡ç±»å‹æ˜¾ç¤ºä¸åŒçš„å‰¯æ ‡é¢˜
if task_type == "ç”Ÿæˆä»»åŠ¡":
    st.markdown('<p class="sub-header">è¾“å…¥ç®€å•çš„æƒ³æ³•ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨åˆ©ç”¨ <b>ç»“æ„åŒ–æ¨¡æ¿ã€è¯­ä¹‰æ‰©å±•ã€å…³é”®è¯å¢å¼º</b> æŠ€æœ¯ä¸ºæ‚¨ç”Ÿæˆä¸“å®¶çº§ Prompt</p>', unsafe_allow_html=True)
elif task_type == "åˆ†ç±»ä»»åŠ¡":
    st.markdown('<p class="sub-header">ç³»ç»Ÿå°†ä¸ºæ‚¨è®¾è®¡ä¸“ä¸šçš„åˆ†ç±»å™¨ Promptï¼Œè‡ªåŠ¨ç”Ÿæˆæœ€ä½³åˆ†ç±»ç­–ç•¥</p>', unsafe_allow_html=True)
elif task_type == "æ‘˜è¦ä»»åŠ¡":
    st.markdown('<p class="sub-header">ç³»ç»Ÿå°†ä¸ºæ‚¨è®¾è®¡æ™ºèƒ½çš„æ‘˜è¦å™¨ Promptï¼Œè‡ªåŠ¨ä¼˜åŒ– <b>ä¿¡æ¯æå–è§„åˆ™ã€å‹ç¼©ç­–ç•¥</b> å’Œè¾“å‡ºæ ¼å¼</p>', unsafe_allow_html=True)
elif task_type == "ç¿»è¯‘ä»»åŠ¡":
    st.markdown('<p class="sub-header">ç³»ç»Ÿå°†ä¸ºæ‚¨æ„å»ºä¸“ä¸šçš„ç¿»è¯‘å™¨ Promptï¼Œæ•´åˆ <b>æœ¯è¯­è¡¨ã€é£æ ¼æŒ‡å—</b> å’Œé¢†åŸŸçŸ¥è¯†åº“</p>', unsafe_allow_html=True)

# åˆ›å»ºä¼˜åŒ–å™¨å®ä¾‹ï¼ˆæ‰€æœ‰é¡µé¢å…±äº«ï¼‰
if api_key_input and api_key_input.strip():
    optimizer = PromptOptimizer(
        api_key=api_key_input,
        model=model_choice,
        base_url=base_url if base_url else None,
        provider=api_provider.lower()
    )
    
    # å°†é…ç½®ä¿å­˜åˆ° session_stateï¼Œä¾›é¡µé¢æ¨¡å—ä½¿ç”¨
    st.session_state.api_key_input = api_key_input
    st.session_state.api_provider = api_provider
    st.session_state.model_choice = model_choice
    st.session_state.base_url = base_url
else:
    optimizer = None

# æ ¹æ®ä»»åŠ¡ç±»å‹æ¸²æŸ“å¯¹åº”çš„é¡µé¢
if not optimizer:
    # å¦‚æœæ²¡æœ‰é…ç½® API Keyï¼Œæ˜¾ç¤ºæç¤º
    st.warning("âš ï¸ è¯·å…ˆåœ¨å·¦ä¾§è¾¹æ é…ç½® API Key")
elif task_type == "ç”Ÿæˆä»»åŠ¡":
    generation_page = GenerationPage(optimizer)
    generation_page.render()
elif task_type == "åˆ†ç±»ä»»åŠ¡":
    classification_page = ClassificationPage(optimizer)
    classification_page.render()
elif task_type == "æ‘˜è¦ä»»åŠ¡":
    summarization_page = SummarizationPage(optimizer)
    summarization_page.render()
elif task_type == "ç¿»è¯‘ä»»åŠ¡":
    translation_page = TranslationPage(optimizer)
    translation_page.render()


def _get_classification_test_dataset():
    """è·å–åˆ†ç±»ä»»åŠ¡çš„æµ‹è¯•æ•°æ®é›†ï¼Œä¼˜å…ˆä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰æ•°æ®"""
    # æ£€æŸ¥æ˜¯å¦æœ‰è‡ªå®šä¹‰CSVæ•°æ®
    if 'custom_test_data' in st.session_state and st.session_state.custom_test_data:
        custom_data = st.session_state.custom_test_data
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        return [{"input": item["text"], "ground_truth": item["expected"]} for item in custom_data]

    # æ£€æŸ¥æ˜¯å¦æœ‰æ‰‹åŠ¨è¾“å…¥æ•°æ®
    if 'manual_test_data' in st.session_state:
        manual_data = [item for item in st.session_state.manual_test_data
                      if item["text"].strip() and item["expected"].strip()]
        if manual_data:
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            return [{"input": item["text"], "ground_truth": item["expected"]} for item in manual_data]

    # ä½¿ç”¨é»˜è®¤æ•°æ®é›†
    return get_default_dataset("classification")


def _get_random_search_test_dataset(task_key: str):
    """è·å–éšæœºæœç´¢ä½¿ç”¨çš„æµ‹è¯•æ•°æ®é›†ï¼Œæ ¹æ®ç”¨æˆ·é€‰æ‹©çš„æ•°æ®æº"""
    if task_key == "classification":
        data_source_key = "random_search_data_source"
        custom_key = "random_search_custom_test_data"
        manual_key = "random_search_manual_test_data"
        default_dataset = get_default_dataset("classification")
    elif task_key == "summarization":
        data_source_key = "random_search_data_source_summarization"
        custom_key = "random_search_custom_test_data_summarization"
        manual_key = "random_search_manual_test_data_summarization"
        default_dataset = get_default_dataset("summarization")
    elif task_key == "translation":
        data_source_key = "random_search_data_source_translation"
        custom_key = "random_search_custom_test_data_translation"
        manual_key = "random_search_manual_test_data_translation"
        default_dataset = get_default_dataset("translation")
    else:
        return []

    data_source = st.session_state.get(data_source_key, 'ä½¿ç”¨é»˜è®¤æ•°æ®')

    if data_source == "ä½¿ç”¨é»˜è®¤æ•°æ®":
        # æ¸…é™¤éšæœºæœç´¢çš„è‡ªå®šä¹‰æ•°æ®
        if custom_key in st.session_state:
            del st.session_state[custom_key]
        if manual_key in st.session_state:
            del st.session_state[manual_key]
        return default_dataset

    elif data_source == "ä¸Šä¼ CSVæ–‡ä»¶":
        # è¿”å›éšæœºæœç´¢çš„CSVæ•°æ®ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›é»˜è®¤æ•°æ®
        if custom_key in st.session_state and st.session_state[custom_key]:
            custom_data = st.session_state[custom_key]
            return [{"input": item["text"], "ground_truth": item["expected"]} for item in custom_data]
        else:
            return default_dataset

    elif data_source == "æ‰‹åŠ¨è¾“å…¥":
        # è¿”å›éšæœºæœç´¢çš„æ‰‹åŠ¨è¾“å…¥æ•°æ®ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›é»˜è®¤æ•°æ®
        if manual_key in st.session_state:
            manual_data = [item for item in st.session_state[manual_key]
                          if item["text"].strip() and item["expected"].strip()]
            if manual_data:
                return [{"input": item["text"], "ground_truth": item["expected"]} for item in manual_data]
        return default_dataset

    # é»˜è®¤æƒ…å†µ
    return default_dataset


def _render_random_search_csv_upload():
    """æ¸²æŸ“éšæœºæœç´¢CSVæ–‡ä»¶ä¸Šä¼ ç•Œé¢"""
    st.markdown("**ğŸ“ éšæœºæœç´¢CSVæ–‡ä»¶ä¸Šä¼ **")
    st.info("CSVæ–‡ä»¶åº”åŒ…å«ä¸¤åˆ—ï¼š'text'ï¼ˆæ–‡æœ¬ï¼‰å’Œ 'expected'ï¼ˆé¢„æœŸæ ‡ç­¾ï¼‰")

    uploaded_file = st.file_uploader(
        "é€‰æ‹©ç”¨äºéšæœºæœç´¢çš„CSVæ–‡ä»¶",
        type=["csv"],
        key="random_search_csv_upload",
        help="ä¸Šä¼ åŒ…å«æµ‹è¯•æ•°æ®çš„CSVæ–‡ä»¶ç”¨äºéšæœºæœç´¢ä¼˜åŒ–"
    )

    if uploaded_file is not None:
        try:
            # è¯»å–CSVæ–‡ä»¶
            df = pd.read_csv(uploaded_file)

            # éªŒè¯åˆ—å
            required_columns = ["text", "expected"]
            if not all(col in df.columns for col in required_columns):
                st.error(f"âŒ CSVæ–‡ä»¶å¿…é¡»åŒ…å«ä»¥ä¸‹åˆ—ï¼š{', '.join(required_columns)}")
                return

            # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
            st.success(f"âœ… æˆåŠŸåŠ è½½ {len(df)} æ¡æµ‹è¯•æ•°æ®ç”¨äºéšæœºæœç´¢")
            st.markdown("**æ•°æ®é¢„è§ˆï¼š**")
            st.dataframe(df.head(), use_container_width=True)

            # ä¿å­˜åˆ°session_state
            st.session_state.random_search_custom_test_data = df.to_dict('records')

        except Exception as e:
            st.error(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{str(e)}")


def _render_random_search_manual_input():
    """æ¸²æŸ“éšæœºæœç´¢æ‰‹åŠ¨è¾“å…¥ç•Œé¢"""
    st.markdown("**âœï¸ éšæœºæœç´¢æ‰‹åŠ¨è¾“å…¥æµ‹è¯•æ•°æ®**")

    # è·å–å½“å‰çš„æ‰‹åŠ¨è¾“å…¥æ•°æ®
    manual_data = st.session_state.get('random_search_manual_test_data', [
        {"text": "", "expected": ""},
        {"text": "", "expected": ""},
        {"text": "", "expected": ""}
    ])

    st.markdown("æ·»åŠ ç”¨äºéšæœºæœç´¢çš„æµ‹è¯•æ ·æœ¬ï¼š")

    # æ˜¾ç¤ºç°æœ‰çš„è¾“å…¥æ¡†
    updated_data = []
    for i, item in enumerate(manual_data):
        col1, col2, col3 = st.columns([4, 2, 1])
        with col1:
            text = st.text_input(
                f"æ–‡æœ¬ {i+1}",
                value=item["text"],
                key=f"random_search_manual_text_{i}",
                placeholder="è¾“å…¥æµ‹è¯•æ–‡æœ¬"
            )
        with col2:
            expected = st.text_input(
                f"æ ‡ç­¾ {i+1}",
                value=item["expected"],
                key=f"random_search_manual_expected_{i}",
                placeholder="é¢„æœŸæ ‡ç­¾"
            )
        with col3:
            if st.button("ğŸ—‘ï¸", key=f"random_search_delete_{i}", help=f"åˆ é™¤ç¬¬{i+1}è¡Œ"):
                continue  # è·³è¿‡è¿™è¡Œï¼Œä¸æ·»åŠ åˆ°updated_dataä¸­

        if text.strip() or expected.strip():  # åªä¿å­˜éç©ºè¡Œ
            updated_data.append({"text": text, "expected": expected})

    # æ·»åŠ æ–°è¡Œçš„æŒ‰é’®
    if st.button("â• æ·»åŠ ä¸€è¡Œ", key="random_search_add_manual_row"):
        updated_data.append({"text": "", "expected": ""})

    # ä¿å­˜åˆ°session_state
    st.session_state.random_search_manual_test_data = updated_data

    # æ˜¾ç¤ºæœ‰æ•ˆæ•°æ®æ•°é‡
    valid_count = sum(1 for item in updated_data if item["text"].strip() and item["expected"].strip())
    st.info(f"å½“å‰æœ‰ {valid_count} æ¡æœ‰æ•ˆæµ‹è¯•æ•°æ®ç”¨äºéšæœºæœç´¢")


def _render_random_search_csv_upload_summarization():
    """æ¸²æŸ“æ‘˜è¦ä»»åŠ¡éšæœºæœç´¢CSVæ–‡ä»¶ä¸Šä¼ ç•Œé¢"""
    st.markdown("**ğŸ“ éšæœºæœç´¢CSVæ–‡ä»¶ä¸Šä¼ ï¼ˆæ‘˜è¦ä»»åŠ¡ï¼‰**")
    st.info("CSVæ–‡ä»¶åº”åŒ…å«ä¸¤åˆ—ï¼š'text'ï¼ˆåŸæ–‡ï¼‰å’Œ 'expected'ï¼ˆå‚è€ƒæ‘˜è¦ï¼‰")

    uploaded_file = st.file_uploader(
        "é€‰æ‹©ç”¨äºéšæœºæœç´¢çš„CSVæ–‡ä»¶",
        type=["csv"],
        key="random_search_csv_upload_summarization",
        help="ä¸Šä¼ åŒ…å«æ‘˜è¦æµ‹è¯•æ•°æ®çš„CSVæ–‡ä»¶ç”¨äºéšæœºæœç´¢ä¼˜åŒ–"
    )

    if uploaded_file is not None:
        try:
            df = pd.read_csv(uploaded_file)

            required_columns = ["text", "expected"]
            if not all(col in df.columns for col in required_columns):
                st.error(f"âŒ CSVæ–‡ä»¶å¿…é¡»åŒ…å«ä»¥ä¸‹åˆ—ï¼š{', '.join(required_columns)}")
                return

            st.success(f"âœ… æˆåŠŸåŠ è½½ {len(df)} æ¡æµ‹è¯•æ•°æ®ç”¨äºéšæœºæœç´¢")
            st.markdown("**æ•°æ®é¢„è§ˆï¼š**")
            st.dataframe(df.head(), use_container_width=True)

            st.session_state.random_search_custom_test_data_summarization = df.to_dict('records')

        except Exception as e:
            st.error(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{str(e)}")


def _render_random_search_manual_input_summarization():
    """æ¸²æŸ“æ‘˜è¦ä»»åŠ¡éšæœºæœç´¢æ‰‹åŠ¨è¾“å…¥ç•Œé¢"""
    st.markdown("**âœï¸ éšæœºæœç´¢æ‰‹åŠ¨è¾“å…¥æµ‹è¯•æ•°æ®ï¼ˆæ‘˜è¦ä»»åŠ¡ï¼‰**")

    manual_data = st.session_state.get('random_search_manual_test_data_summarization', [
        {"text": "", "expected": ""},
        {"text": "", "expected": ""},
        {"text": "", "expected": ""}
    ])

    st.markdown("æ·»åŠ ç”¨äºéšæœºæœç´¢çš„æµ‹è¯•æ ·æœ¬ï¼š")

    updated_data = []
    for i, item in enumerate(manual_data):
        col1, col2, col3 = st.columns([4, 4, 1])
        with col1:
            text = st.text_area(
                f"åŸæ–‡ {i+1}",
                value=item["text"],
                key=f"random_search_sum_manual_text_{i}",
                height=100,
                placeholder="è¾“å…¥å¾…æ‘˜è¦åŸæ–‡"
            )
        with col2:
            expected = st.text_area(
                f"å‚è€ƒæ‘˜è¦ {i+1}",
                value=item["expected"],
                key=f"random_search_sum_manual_expected_{i}",
                height=100,
                placeholder="è¾“å…¥å‚è€ƒæ‘˜è¦"
            )
        with col3:
            if st.button("ğŸ—‘ï¸", key=f"random_search_sum_delete_{i}", help=f"åˆ é™¤ç¬¬{i+1}è¡Œ"):
                continue

        if text.strip() or expected.strip():
            updated_data.append({"text": text, "expected": expected})

    if st.button("â• æ·»åŠ ä¸€è¡Œ", key="random_search_sum_add_manual_row"):
        updated_data.append({"text": "", "expected": ""})

    st.session_state.random_search_manual_test_data_summarization = updated_data

    valid_count = sum(1 for item in updated_data if item["text"].strip() and item["expected"].strip())
    st.info(f"å½“å‰æœ‰ {valid_count} æ¡æœ‰æ•ˆæµ‹è¯•æ•°æ®ç”¨äºéšæœºæœç´¢")


def _render_random_search_csv_upload_translation():
    """æ¸²æŸ“ç¿»è¯‘ä»»åŠ¡éšæœºæœç´¢CSVæ–‡ä»¶ä¸Šä¼ ç•Œé¢"""
    st.markdown("**ğŸ“ éšæœºæœç´¢CSVæ–‡ä»¶ä¸Šä¼ ï¼ˆç¿»è¯‘ä»»åŠ¡ï¼‰**")
    st.info("CSVæ–‡ä»¶åº”åŒ…å«ä¸¤åˆ—ï¼š'text'ï¼ˆåŸæ–‡ï¼‰å’Œ 'expected'ï¼ˆå‚è€ƒè¯‘æ–‡ï¼‰")

    uploaded_file = st.file_uploader(
        "é€‰æ‹©ç”¨äºéšæœºæœç´¢çš„CSVæ–‡ä»¶",
        type=["csv"],
        key="random_search_csv_upload_translation",
        help="ä¸Šä¼ åŒ…å«ç¿»è¯‘æµ‹è¯•æ•°æ®çš„CSVæ–‡ä»¶ç”¨äºéšæœºæœç´¢ä¼˜åŒ–"
    )

    if uploaded_file is not None:
        try:
            df = pd.read_csv(uploaded_file)

            required_columns = ["text", "expected"]
            if not all(col in df.columns for col in required_columns):
                st.error(f"âŒ CSVæ–‡ä»¶å¿…é¡»åŒ…å«ä»¥ä¸‹åˆ—ï¼š{', '.join(required_columns)}")
                return

            st.success(f"âœ… æˆåŠŸåŠ è½½ {len(df)} æ¡æµ‹è¯•æ•°æ®ç”¨äºéšæœºæœç´¢")
            st.markdown("**æ•°æ®é¢„è§ˆï¼š**")
            st.dataframe(df.head(), use_container_width=True)

            st.session_state.random_search_custom_test_data_translation = df.to_dict('records')

        except Exception as e:
            st.error(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{str(e)}")


def _render_random_search_manual_input_translation():
    """æ¸²æŸ“ç¿»è¯‘ä»»åŠ¡éšæœºæœç´¢æ‰‹åŠ¨è¾“å…¥ç•Œé¢"""
    st.markdown("**âœï¸ éšæœºæœç´¢æ‰‹åŠ¨è¾“å…¥æµ‹è¯•æ•°æ®ï¼ˆç¿»è¯‘ä»»åŠ¡ï¼‰**")

    manual_data = st.session_state.get('random_search_manual_test_data_translation', [
        {"text": "", "expected": ""},
        {"text": "", "expected": ""},
        {"text": "", "expected": ""}
    ])

    st.markdown("æ·»åŠ ç”¨äºéšæœºæœç´¢çš„æµ‹è¯•æ ·æœ¬ï¼š")

    updated_data = []
    for i, item in enumerate(manual_data):
        col1, col2, col3 = st.columns([4, 4, 1])
        with col1:
            text = st.text_area(
                f"åŸæ–‡ {i+1}",
                value=item["text"],
                key=f"random_search_trans_manual_text_{i}",
                height=100,
                placeholder="è¾“å…¥å¾…ç¿»è¯‘åŸæ–‡"
            )
        with col2:
            expected = st.text_area(
                f"å‚è€ƒè¯‘æ–‡ {i+1}",
                value=item["expected"],
                key=f"random_search_trans_manual_expected_{i}",
                height=100,
                placeholder="è¾“å…¥å‚è€ƒè¯‘æ–‡"
            )
        with col3:
            if st.button("ğŸ—‘ï¸", key=f"random_search_trans_delete_{i}", help=f"åˆ é™¤ç¬¬{i+1}è¡Œ"):
                continue

        if text.strip() or expected.strip():
            updated_data.append({"text": text, "expected": expected})

    if st.button("â• æ·»åŠ ä¸€è¡Œ", key="random_search_trans_add_manual_row"):
        updated_data.append({"text": "", "expected": ""})

    st.session_state.random_search_manual_test_data_translation = updated_data

    valid_count = sum(1 for item in updated_data if item["text"].strip() and item["expected"].strip())
    st.info(f"å½“å‰æœ‰ {valid_count} æ¡æœ‰æ•ˆæµ‹è¯•æ•°æ®ç”¨äºéšæœºæœç´¢")


def _get_random_search_config(task_type_label: str):
    """æ ¹æ®ä»»åŠ¡ç±»å‹è¿”å›éšæœºæœç´¢é…ç½®"""
    if task_type_label == "åˆ†ç±»ä»»åŠ¡":
        # è·å–ç”¨æˆ·è¾“å…¥çš„æ ‡ç­¾å’Œä»»åŠ¡æè¿°
        user_labels = st.session_state.get('user_labels', ["ç§¯æ", "æ¶ˆæ", "ä¸­ç«‹"])
        user_task_desc = st.session_state.get('user_task_description', get_default_value("classification", "task_description"))
        
        # åŠ¨æ€ç”Ÿæˆä»»åŠ¡æè¿°
        labels_str = ", ".join(user_labels)
        task_desc = f"{user_task_desc}ï¼Œåˆ¤æ–­ä¸º{labels_str}"
        
        # è·å–æµ‹è¯•æ•°æ®é›†ï¼ˆä½¿ç”¨éšæœºæœç´¢ä¸“ç”¨æ•°æ®é›†è·å–å‡½æ•°ï¼‰
        test_dataset = _get_random_search_test_dataset("classification")
        
        return (task_desc, "classification", test_dataset, {"labels": user_labels})
    if task_type_label == "æ‘˜è¦ä»»åŠ¡":
        # è·å–ç”¨æˆ·è¾“å…¥çš„ä»»åŠ¡æè¿°
        user_task_desc = st.session_state.get('user_task_description_summarization', get_default_value("summarization", "task_description"))
        
        # è·å–å…¶ä»–æ‘˜è¦é…ç½®
        source_type = st.session_state.get(
            'summarization_source_type',
            get_default_value("summarization", "source_type")
        )
        target_audience = st.session_state.get(
            'summarization_target_audience',
            get_default_value("summarization", "target_audience")
        )
        focus_points = st.session_state.get(
            'summarization_focus_points',
            get_default_value("summarization", "focus_points")
        )
        
        # åŠ¨æ€ç”Ÿæˆä»»åŠ¡æè¿°
        task_desc = user_task_desc
        
        test_dataset = _get_random_search_test_dataset("summarization")

        return (task_desc, "summarization", test_dataset, {
            "source_type": source_type,
            "target_audience": target_audience,
            "focus_points": focus_points
        })
    if task_type_label == "ç¿»è¯‘ä»»åŠ¡":
        # è·å–ç”¨æˆ·è¾“å…¥çš„ä»»åŠ¡æè¿°
        user_task_desc = st.session_state.get('user_task_description_translation', get_default_value("translation", "task_description"))
        
        # è·å–å…¶ä»–ç¿»è¯‘é…ç½®
        source_lang = st.session_state.get('translation_source_lang', 'è‹±æ–‡')
        target_lang = st.session_state.get('translation_target_lang', 'ä¸­æ–‡')
        domain = st.session_state.get(
            'translation_domain',
            get_default_value("translation", "domain")
        )
        tone = st.session_state.get(
            'translation_tone',
            get_default_value("translation", "tone")
        )
        
        # åŠ¨æ€ç”Ÿæˆä»»åŠ¡æè¿°
        task_desc = user_task_desc
        
        test_dataset = _get_random_search_test_dataset("translation")

        return (task_desc, "translation", test_dataset, {
            "source_lang": source_lang,
            "target_lang": target_lang,
            "domain": domain,
            "tone": tone
        })
    return None


# éšæœºæœç´¢ä¼˜åŒ–åŒºåŸŸ
if optimizer and task_type in ["åˆ†ç±»ä»»åŠ¡", "æ‘˜è¦ä»»åŠ¡", "ç¿»è¯‘ä»»åŠ¡"]:
    st.divider()
    st.subheader("ğŸ² éšæœºæœç´¢ä¼˜åŒ–ï¼ˆå®éªŒåŠŸèƒ½ï¼‰")
    st.markdown("*é€šè¿‡éšæœºé‡‡æ ·è§’è‰²/é£æ ¼/æŠ€å·§ç»„åˆï¼Œåœ¨å°å‹æµ‹è¯•é›†ä¸Šå¯»æ‰¾æ›´ä¼˜ Prompt ç»“æ„*" )

    # éšæœºæœç´¢æ•°æ®æ¥æºé€‰æ‹©ï¼ˆåˆ†ç±»/æ‘˜è¦/ç¿»è¯‘ï¼‰
    if task_type in ["åˆ†ç±»ä»»åŠ¡", "æ‘˜è¦ä»»åŠ¡", "ç¿»è¯‘ä»»åŠ¡"]:
        st.markdown("**ğŸ“Š éšæœºæœç´¢æ•°æ®æ¥æº**")

        if task_type == "åˆ†ç±»ä»»åŠ¡":
            random_search_data_source = st.radio(
                "é€‰æ‹©éšæœºæœç´¢ä½¿ç”¨çš„æ•°æ®æ¥æº",
                ["ä½¿ç”¨é»˜è®¤æ•°æ®", "ä¸Šä¼ CSVæ–‡ä»¶", "æ‰‹åŠ¨è¾“å…¥"],
                key="random_search_data_source",
                help="é€‰æ‹©ç”¨äºéšæœºæœç´¢ä¼˜åŒ–çš„æµ‹è¯•æ•°æ®æ¥æº",
                horizontal=True
            )

            if random_search_data_source == "ä¸Šä¼ CSVæ–‡ä»¶":
                _render_random_search_csv_upload()
            elif random_search_data_source == "æ‰‹åŠ¨è¾“å…¥":
                _render_random_search_manual_input()

        elif task_type == "æ‘˜è¦ä»»åŠ¡":
            random_search_data_source = st.radio(
                "é€‰æ‹©éšæœºæœç´¢ä½¿ç”¨çš„æ•°æ®æ¥æº",
                ["ä½¿ç”¨é»˜è®¤æ•°æ®", "ä¸Šä¼ CSVæ–‡ä»¶", "æ‰‹åŠ¨è¾“å…¥"],
                key="random_search_data_source_summarization",
                help="é€‰æ‹©ç”¨äºéšæœºæœç´¢ä¼˜åŒ–çš„æµ‹è¯•æ•°æ®æ¥æº",
                horizontal=True
            )

            if random_search_data_source == "ä¸Šä¼ CSVæ–‡ä»¶":
                _render_random_search_csv_upload_summarization()
            elif random_search_data_source == "æ‰‹åŠ¨è¾“å…¥":
                _render_random_search_manual_input_summarization()

        elif task_type == "ç¿»è¯‘ä»»åŠ¡":
            random_search_data_source = st.radio(
                "é€‰æ‹©éšæœºæœç´¢ä½¿ç”¨çš„æ•°æ®æ¥æº",
                ["ä½¿ç”¨é»˜è®¤æ•°æ®", "ä¸Šä¼ CSVæ–‡ä»¶", "æ‰‹åŠ¨è¾“å…¥"],
                key="random_search_data_source_translation",
                help="é€‰æ‹©ç”¨äºéšæœºæœç´¢ä¼˜åŒ–çš„æµ‹è¯•æ•°æ®æ¥æº",
                horizontal=True
            )

            if random_search_data_source == "ä¸Šä¼ CSVæ–‡ä»¶":
                _render_random_search_csv_upload_translation()
            elif random_search_data_source == "æ‰‹åŠ¨è¾“å…¥":
                _render_random_search_manual_input_translation()

    config = _get_random_search_config(task_type)
    if config:
        if len(config) == 4:
            task_desc, task_key, dataset, extra_config = config
        else:
            task_desc, task_key, dataset = config
            extra_config = {}

        col_a, col_b = st.columns([1, 2])
        with col_a:
            iterations = st.slider("è¿­ä»£æ¬¡æ•°", min_value=5, max_value=50, value=12, step=1)
        with col_b:
            st.caption("å»ºè®®ï¼šå¿«é€Ÿä½“éªŒ 5-10 æ¬¡ï¼›æœ‰æ•ˆä¼˜åŒ– 20-50 æ¬¡ï¼ˆæˆæœ¬æ›´é«˜ï¼‰ã€‚")

        if st.button("ğŸš€ è¿è¡Œéšæœºæœç´¢", type="primary", use_container_width=True, key="random_search_btn"):
            with st.spinner("â³ æ­£åœ¨ç”Ÿæˆæœç´¢ç©ºé—´å¹¶æ‰§è¡Œéšæœºæœç´¢..."):
                try:
                    search_space = optimizer.search_space_generator.generate(
                        task_description=task_desc,
                        task_type=task_key,
                        **extra_config
                    )

                    # æ˜¾ç¤ºç”Ÿæˆçš„æœç´¢ç©ºé—´
                    st.success("âœ… æœç´¢ç©ºé—´ç”Ÿæˆå®Œæˆï¼")
                    st.info("ğŸ’¡ ç³»ç»Ÿå°†ä»è¿™äº›é€‰é¡¹ä¸­éšæœºç»„åˆè¿›è¡Œæµ‹è¯•ï¼Œæ¯ä¸ªç»„åˆåŒ…å«ï¼š1ä¸ªè§’è‰² + 1ç§é£æ ¼ + 1ç§æŠ€å·§")
                    with st.expander("ğŸ” æŸ¥çœ‹ç”Ÿæˆçš„æœç´¢ç©ºé—´", expanded=True):
                        col1, col2, col3 = st.columns(3)

                        with col1:
                            st.markdown("**ğŸ­ è§’è‰²è®¾å®š (5ä¸ª)**")
                            for i, role in enumerate(search_space.roles, 1):
                                st.markdown(f"{i}. {role}")

                        with col2:
                            st.markdown("**ğŸ¨ å›ç­”é£æ ¼ (5ç§)**")
                            for i, style in enumerate(search_space.styles, 1):
                                st.markdown(f"{i}. {style}")

                        with col3:
                            st.markdown("**ğŸ› ï¸ æç¤ºæŠ€å·§ (3ç§)**")
                            for i, technique in enumerate(search_space.techniques, 1):
                                st.markdown(f"{i}. {technique}")

                    results, best = optimizer.random_search.run(
                        task_description=task_desc,
                        task_type=task_key,
                        test_dataset=dataset,
                        search_space=search_space,
                        iterations=iterations,
                        labels=st.session_state.get('user_labels') if task_type == "åˆ†ç±»ä»»åŠ¡" else None
                    )

                    st.session_state.random_search_results = results
                    st.session_state.random_search_best = best
                    st.session_state.random_search_space = search_space
                except Exception as e:
                    st.error(f"âŒ éšæœºæœç´¢å¤±è´¥ï¼š{str(e)}")

        if 'random_search_best' in st.session_state and st.session_state.random_search_best:
            best = st.session_state.random_search_best
            search_space = st.session_state.get('random_search_space')

            st.success(f"âœ… æœ€ä½³å¾—åˆ†ï¼š{best.avg_score:.2f}")
            st.markdown(f"**æœ€ä½³ç»„åˆï¼š** {best.role} + {best.style} + {best.technique}")
            st.text_area("æœ€ä½³ Prompt", value=best.full_prompt, height=200)

            # æ˜¾ç¤ºå®Œæ•´çš„æœç´¢ç©ºé—´
            if search_space:
                st.divider()
                st.markdown("### ğŸ” æœç´¢ç©ºé—´è¯¦æƒ…")
                with st.expander("æŸ¥çœ‹å®Œæ•´çš„æœç´¢ç©ºé—´", expanded=False):
                    col1, col2, col3 = st.columns(3)

                    with col1:
                        st.markdown("**ğŸ­ è§’è‰²è®¾å®š (5ä¸ª)**")
                        for i, role in enumerate(search_space.roles, 1):
                            if role == best.role:
                                st.markdown(f"**{i}. {role} â† æœ€ä½³é€‰æ‹©**")
                            else:
                                st.markdown(f"{i}. {role}")

                    with col2:
                        st.markdown("**ğŸ¨ å›ç­”é£æ ¼ (5ç§)**")
                        for i, style in enumerate(search_space.styles, 1):
                            if style == best.style:
                                st.markdown(f"**{i}. {style} â† æœ€ä½³é€‰æ‹©**")
                            else:
                                st.markdown(f"{i}. {style}")

                    with col3:
                        st.markdown("**ğŸ› ï¸ æç¤ºæŠ€å·§ (3ç§)**")
                        for i, technique in enumerate(search_space.techniques, 1):
                            if technique == best.technique:
                                st.markdown(f"**{i}. {technique} â† æœ€ä½³é€‰æ‹©**")
                            else:
                                st.markdown(f"{i}. {technique}")
