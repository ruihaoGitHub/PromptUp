"""
AI Prompt è‡ªåŠ¨ä¼˜åŒ–ç³»ç»Ÿ - Streamlit ä¸»ç•Œé¢
é‡æ„åçš„ç²¾ç®€ç‰ˆæœ¬ï¼Œä½¿ç”¨é¡µé¢æ¨¡å—å’Œ UI ç»„ä»¶ç³»ç»Ÿ
"""
import streamlit as st
import pandas as pd
from config.defaults import get_default_value, get_default_dataset
from config.defaults import get_default_value
from dotenv import load_dotenv
from optimizer import PromptOptimizer
from page_modules import (
    GenerationPage,
    ClassificationPage, 
    SummarizationPage,
    TranslationPage
)
from ui import apply_custom_styles, render_sidebar

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# åº”ç”¨è‡ªå®šä¹‰æ ·å¼ï¼ˆåŒ…å«é¡µé¢é…ç½®ï¼‰
apply_custom_styles()

# æ ‡é¢˜åŒºåŸŸ
st.markdown('<p class="main-header">ğŸš€ AI Prompt è‡ªåŠ¨ä¼˜åŒ–å¤§å¸ˆ</p>', unsafe_allow_html=True)

# æ¸²æŸ“ä¾§è¾¹æ å¹¶è·å–é…ç½®
config = render_sidebar()
task_type = config['task_type']
api_provider = config['api_provider']
api_key_input = config['api_key']
base_url = config['base_url']
model_choice = config['model']

# æ ¹æ®ä»»åŠ¡ç±»å‹æ˜¾ç¤ºä¸åŒçš„å‰¯æ ‡é¢˜
if task_type == "ç”Ÿæˆä»»åŠ¡":
    st.markdown('<p class="sub-header">è¾“å…¥ç®€å•çš„æƒ³æ³•ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨åˆ©ç”¨ <b>ç»“æ„åŒ–æ¨¡æ¿ã€è¯­ä¹‰æ‰©å±•ã€å…³é”®è¯å¢å¼º</b> æŠ€æœ¯ä¸ºæ‚¨ç”Ÿæˆä¸“å®¶çº§ Prompt</p>', unsafe_allow_html=True)
elif task_type == "åˆ†ç±»ä»»åŠ¡":
    st.markdown('<p class="sub-header">ç³»ç»Ÿå°†ä¸ºæ‚¨è®¾è®¡ä¸“ä¸šçš„åˆ†ç±»å™¨ Promptï¼Œè‡ªåŠ¨ç”Ÿæˆæœ€ä½³åˆ†ç±»ç­–ç•¥</p>', unsafe_allow_html=True)
elif task_type == "æ‘˜è¦ä»»åŠ¡":
    st.markdown('<p class="sub-header">ç³»ç»Ÿå°†ä¸ºæ‚¨è®¾è®¡æ™ºèƒ½çš„æ‘˜è¦å™¨ Promptï¼Œè‡ªåŠ¨ä¼˜åŒ– <b>ä¿¡æ¯æå–è§„åˆ™ã€å‹ç¼©ç­–ç•¥</b> å’Œè¾“å‡ºæ ¼å¼</p>', unsafe_allow_html=True)
elif task_type == "ç¿»è¯‘ä»»åŠ¡":
    st.markdown('<p class="sub-header">ç³»ç»Ÿå°†ä¸ºæ‚¨æ„å»ºä¸“ä¸šçš„ç¿»è¯‘å™¨ Promptï¼Œæ•´åˆ <b>æœ¯è¯­è¡¨ã€é£æ ¼æŒ‡å—</b> å’Œé¢†åŸŸçŸ¥è¯†åº“</p>', unsafe_allow_html=True)

# åˆ›å»ºä¼˜åŒ–å™¨å®ä¾‹ï¼ˆæ‰€æœ‰é¡µé¢å…±äº«ï¼‰
if api_key_input and api_key_input.strip():
    optimizer = PromptOptimizer(
        api_key=api_key_input,
        model=model_choice,
        base_url=base_url if base_url else None,
        provider=api_provider.lower()
    )
    
    # å°†é…ç½®ä¿å­˜åˆ° session_stateï¼Œä¾›é¡µé¢æ¨¡å—ä½¿ç”¨
    st.session_state.api_key_input = api_key_input
    st.session_state.api_provider = api_provider
    st.session_state.model_choice = model_choice
    st.session_state.base_url = base_url
else:
    optimizer = None

# æ ¹æ®ä»»åŠ¡ç±»å‹æ¸²æŸ“å¯¹åº”çš„é¡µé¢
if not optimizer:
    # å¦‚æœæ²¡æœ‰é…ç½® API Keyï¼Œæ˜¾ç¤ºæç¤º
    st.warning("âš ï¸ è¯·å…ˆåœ¨å·¦ä¾§è¾¹æ é…ç½® API Key")
elif task_type == "ç”Ÿæˆä»»åŠ¡":
    generation_page = GenerationPage(optimizer)
    generation_page.render()
elif task_type == "åˆ†ç±»ä»»åŠ¡":
    classification_page = ClassificationPage(optimizer)
    classification_page.render()
elif task_type == "æ‘˜è¦ä»»åŠ¡":
    summarization_page = SummarizationPage(optimizer)
    summarization_page.render()
elif task_type == "ç¿»è¯‘ä»»åŠ¡":
    translation_page = TranslationPage(optimizer)
    translation_page.render()


def _get_classification_test_dataset():
    """è·å–åˆ†ç±»ä»»åŠ¡çš„æµ‹è¯•æ•°æ®é›†ï¼Œä¼˜å…ˆä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰æ•°æ®"""
    # æ£€æŸ¥æ˜¯å¦æœ‰è‡ªå®šä¹‰CSVæ•°æ®
    if 'custom_test_data' in st.session_state and st.session_state.custom_test_data:
        custom_data = st.session_state.custom_test_data
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        return [{"input": item["text"], "ground_truth": item["expected"]} for item in custom_data]

    # æ£€æŸ¥æ˜¯å¦æœ‰æ‰‹åŠ¨è¾“å…¥æ•°æ®
    if 'manual_test_data' in st.session_state:
        manual_data = [item for item in st.session_state.manual_test_data
                      if item["text"].strip() and item["expected"].strip()]
        if manual_data:
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            return [{"input": item["text"], "ground_truth": item["expected"]} for item in manual_data]

    # ä½¿ç”¨é»˜è®¤æ•°æ®é›†
    return get_default_dataset("classification")


def _get_random_search_test_dataset(task_key: str):
    """è·å–éšæœºæœç´¢ä½¿ç”¨çš„æµ‹è¯•æ•°æ®é›†ï¼Œæ ¹æ®ç”¨æˆ·é€‰æ‹©çš„æ•°æ®æº"""
    if task_key == "classification":
        data_source_key = "random_search_data_source"
        custom_key = "random_search_custom_test_data"
        manual_key = "random_search_manual_test_data"
        default_dataset = get_default_dataset("classification")
    elif task_key == "summarization":
        data_source_key = "random_search_data_source_summarization"
        custom_key = "random_search_custom_test_data_summarization"
        manual_key = "random_search_manual_test_data_summarization"
        default_dataset = get_default_dataset("summarization")
    elif task_key == "translation":
        data_source_key = "random_search_data_source_translation"
        custom_key = "random_search_custom_test_data_translation"
        manual_key = "random_search_manual_test_data_translation"
        default_dataset = get_default_dataset("translation")
    else:
        return []

    data_source = st.session_state.get(data_source_key, 'ä½¿ç”¨é»˜è®¤æ•°æ®')

    if data_source == "ä½¿ç”¨é»˜è®¤æ•°æ®":
        # æ¸…é™¤éšæœºæœç´¢çš„è‡ªå®šä¹‰æ•°æ®
        if custom_key in st.session_state:
            del st.session_state[custom_key]
        if manual_key in st.session_state:
            del st.session_state[manual_key]
        return default_dataset

    elif data_source == "ä¸Šä¼ CSVæ–‡ä»¶":
        # è¿”å›éšæœºæœç´¢çš„CSVæ•°æ®ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›é»˜è®¤æ•°æ®
        if custom_key in st.session_state and st.session_state[custom_key]:
            custom_data = st.session_state[custom_key]
            return [{"input": item["text"], "ground_truth": item["expected"]} for item in custom_data]
        else:
            return default_dataset

    elif data_source == "æ‰‹åŠ¨è¾“å…¥":
        # è¿”å›éšæœºæœç´¢çš„æ‰‹åŠ¨è¾“å…¥æ•°æ®ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›é»˜è®¤æ•°æ®
        if manual_key in st.session_state:
            manual_data = [item for item in st.session_state[manual_key]
                          if item["text"].strip() and item["expected"].strip()]
            if manual_data:
                return [{"input": item["text"], "ground_truth": item["expected"]} for item in manual_data]
        return default_dataset

    # é»˜è®¤æƒ…å†µ
    return default_dataset


def _render_random_search_csv_upload():
    """æ¸²æŸ“éšæœºæœç´¢CSVæ–‡ä»¶ä¸Šä¼ ç•Œé¢"""
    st.markdown("**ğŸ“ éšæœºæœç´¢CSVæ–‡ä»¶ä¸Šä¼ **")
    st.info("CSVæ–‡ä»¶åº”åŒ…å«ä¸¤åˆ—ï¼š'text'ï¼ˆæ–‡æœ¬ï¼‰å’Œ 'expected'ï¼ˆé¢„æœŸæ ‡ç­¾ï¼‰")

    uploaded_file = st.file_uploader(
        "é€‰æ‹©ç”¨äºéšæœºæœç´¢çš„CSVæ–‡ä»¶",
        type=["csv"],
        key="random_search_csv_upload",
        help="ä¸Šä¼ åŒ…å«æµ‹è¯•æ•°æ®çš„CSVæ–‡ä»¶ç”¨äºéšæœºæœç´¢ä¼˜åŒ–"
    )

    if uploaded_file is not None:
        try:
            # è¯»å–CSVæ–‡ä»¶
            df = pd.read_csv(uploaded_file)

            # éªŒè¯åˆ—å
            required_columns = ["text", "expected"]
            if not all(col in df.columns for col in required_columns):
                st.error(f"âŒ CSVæ–‡ä»¶å¿…é¡»åŒ…å«ä»¥ä¸‹åˆ—ï¼š{', '.join(required_columns)}")
                return

            # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
            st.success(f"âœ… æˆåŠŸåŠ è½½ {len(df)} æ¡æµ‹è¯•æ•°æ®ç”¨äºéšæœºæœç´¢")
            st.markdown("**æ•°æ®é¢„è§ˆï¼š**")
            st.dataframe(df.head(), use_container_width=True)

            # ä¿å­˜åˆ°session_state
            st.session_state.random_search_custom_test_data = df.to_dict('records')

        except Exception as e:
            st.error(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{str(e)}")


def _render_random_search_manual_input():
    """æ¸²æŸ“éšæœºæœç´¢æ‰‹åŠ¨è¾“å…¥ç•Œé¢"""
    st.markdown("**âœï¸ éšæœºæœç´¢æ‰‹åŠ¨è¾“å…¥æµ‹è¯•æ•°æ®**")

    # è·å–å½“å‰çš„æ‰‹åŠ¨è¾“å…¥æ•°æ®
    manual_data = st.session_state.get('random_search_manual_test_data', [
        {"text": "", "expected": ""},
        {"text": "", "expected": ""},
        {"text": "", "expected": ""}
    ])

    st.markdown("æ·»åŠ ç”¨äºéšæœºæœç´¢çš„æµ‹è¯•æ ·æœ¬ï¼š")

    # æ˜¾ç¤ºç°æœ‰çš„è¾“å…¥æ¡†
    updated_data = []
    for i, item in enumerate(manual_data):
        col1, col2, col3 = st.columns([4, 2, 1])
        with col1:
            text = st.text_input(
                f"æ–‡æœ¬ {i+1}",
                value=item["text"],
                key=f"random_search_manual_text_{i}",
                placeholder="è¾“å…¥æµ‹è¯•æ–‡æœ¬"
            )
        with col2:
            expected = st.text_input(
                f"æ ‡ç­¾ {i+1}",
                value=item["expected"],
                key=f"random_search_manual_expected_{i}",
                placeholder="é¢„æœŸæ ‡ç­¾"
            )
        with col3:
            if st.button("ğŸ—‘ï¸", key=f"random_search_delete_{i}", help=f"åˆ é™¤ç¬¬{i+1}è¡Œ"):
                continue  # è·³è¿‡è¿™è¡Œï¼Œä¸æ·»åŠ åˆ°updated_dataä¸­

        if text.strip() or expected.strip():  # åªä¿å­˜éç©ºè¡Œ
            updated_data.append({"text": text, "expected": expected})

    # æ·»åŠ æ–°è¡Œçš„æŒ‰é’®
    if st.button("â• æ·»åŠ ä¸€è¡Œ", key="random_search_add_manual_row"):
        updated_data.append({"text": "", "expected": ""})

    # ä¿å­˜åˆ°session_state
    st.session_state.random_search_manual_test_data = updated_data

    # æ˜¾ç¤ºæœ‰æ•ˆæ•°æ®æ•°é‡
    valid_count = sum(1 for item in updated_data if item["text"].strip() and item["expected"].strip())
    st.info(f"å½“å‰æœ‰ {valid_count} æ¡æœ‰æ•ˆæµ‹è¯•æ•°æ®ç”¨äºéšæœºæœç´¢")


def _render_random_search_csv_upload_summarization():
    """æ¸²æŸ“æ‘˜è¦ä»»åŠ¡éšæœºæœç´¢CSVæ–‡ä»¶ä¸Šä¼ ç•Œé¢"""
    st.markdown("**ğŸ“ éšæœºæœç´¢CSVæ–‡ä»¶ä¸Šä¼ ï¼ˆæ‘˜è¦ä»»åŠ¡ï¼‰**")
    st.info("CSVæ–‡ä»¶åº”åŒ…å«ä¸¤åˆ—ï¼š'text'ï¼ˆåŸæ–‡ï¼‰å’Œ 'expected'ï¼ˆå‚è€ƒæ‘˜è¦ï¼‰")

    uploaded_file = st.file_uploader(
        "é€‰æ‹©ç”¨äºéšæœºæœç´¢çš„CSVæ–‡ä»¶",
        type=["csv"],
        key="random_search_csv_upload_summarization",
        help="ä¸Šä¼ åŒ…å«æ‘˜è¦æµ‹è¯•æ•°æ®çš„CSVæ–‡ä»¶ç”¨äºéšæœºæœç´¢ä¼˜åŒ–"
    )

    if uploaded_file is not None:
        try:
            df = pd.read_csv(uploaded_file)

            required_columns = ["text", "expected"]
            if not all(col in df.columns for col in required_columns):
                st.error(f"âŒ CSVæ–‡ä»¶å¿…é¡»åŒ…å«ä»¥ä¸‹åˆ—ï¼š{', '.join(required_columns)}")
                return

            st.success(f"âœ… æˆåŠŸåŠ è½½ {len(df)} æ¡æµ‹è¯•æ•°æ®ç”¨äºéšæœºæœç´¢")
            st.markdown("**æ•°æ®é¢„è§ˆï¼š**")
            st.dataframe(df.head(), use_container_width=True)

            st.session_state.random_search_custom_test_data_summarization = df.to_dict('records')

        except Exception as e:
            st.error(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{str(e)}")


def _render_random_search_manual_input_summarization():
    """æ¸²æŸ“æ‘˜è¦ä»»åŠ¡éšæœºæœç´¢æ‰‹åŠ¨è¾“å…¥ç•Œé¢"""
    st.markdown("**âœï¸ éšæœºæœç´¢æ‰‹åŠ¨è¾“å…¥æµ‹è¯•æ•°æ®ï¼ˆæ‘˜è¦ä»»åŠ¡ï¼‰**")

    manual_data = st.session_state.get('random_search_manual_test_data_summarization', [
        {"text": "", "expected": ""},
        {"text": "", "expected": ""},
        {"text": "", "expected": ""}
    ])

    st.markdown("æ·»åŠ ç”¨äºéšæœºæœç´¢çš„æµ‹è¯•æ ·æœ¬ï¼š")

    updated_data = []
    for i, item in enumerate(manual_data):
        col1, col2, col3 = st.columns([4, 4, 1])
        with col1:
            text = st.text_area(
                f"åŸæ–‡ {i+1}",
                value=item["text"],
                key=f"random_search_sum_manual_text_{i}",
                height=100,
                placeholder="è¾“å…¥å¾…æ‘˜è¦åŸæ–‡"
            )
        with col2:
            expected = st.text_area(
                f"å‚è€ƒæ‘˜è¦ {i+1}",
                value=item["expected"],
                key=f"random_search_sum_manual_expected_{i}",
                height=100,
                placeholder="è¾“å…¥å‚è€ƒæ‘˜è¦"
            )
        with col3:
            if st.button("ğŸ—‘ï¸", key=f"random_search_sum_delete_{i}", help=f"åˆ é™¤ç¬¬{i+1}è¡Œ"):
                continue

        if text.strip() or expected.strip():
            updated_data.append({"text": text, "expected": expected})

    if st.button("â• æ·»åŠ ä¸€è¡Œ", key="random_search_sum_add_manual_row"):
        updated_data.append({"text": "", "expected": ""})

    st.session_state.random_search_manual_test_data_summarization = updated_data

    valid_count = sum(1 for item in updated_data if item["text"].strip() and item["expected"].strip())
    st.info(f"å½“å‰æœ‰ {valid_count} æ¡æœ‰æ•ˆæµ‹è¯•æ•°æ®ç”¨äºéšæœºæœç´¢")


def _render_random_search_csv_upload_translation():
    """æ¸²æŸ“ç¿»è¯‘ä»»åŠ¡éšæœºæœç´¢CSVæ–‡ä»¶ä¸Šä¼ ç•Œé¢"""
    st.markdown("**ğŸ“ éšæœºæœç´¢CSVæ–‡ä»¶ä¸Šä¼ ï¼ˆç¿»è¯‘ä»»åŠ¡ï¼‰**")
    st.info("CSVæ–‡ä»¶åº”åŒ…å«ä¸¤åˆ—ï¼š'text'ï¼ˆåŸæ–‡ï¼‰å’Œ 'expected'ï¼ˆå‚è€ƒè¯‘æ–‡ï¼‰")

    uploaded_file = st.file_uploader(
        "é€‰æ‹©ç”¨äºéšæœºæœç´¢çš„CSVæ–‡ä»¶",
        type=["csv"],
        key="random_search_csv_upload_translation",
        help="ä¸Šä¼ åŒ…å«ç¿»è¯‘æµ‹è¯•æ•°æ®çš„CSVæ–‡ä»¶ç”¨äºéšæœºæœç´¢ä¼˜åŒ–"
    )

    if uploaded_file is not None:
        try:
            df = pd.read_csv(uploaded_file)

            required_columns = ["text", "expected"]
            if not all(col in df.columns for col in required_columns):
                st.error(f"âŒ CSVæ–‡ä»¶å¿…é¡»åŒ…å«ä»¥ä¸‹åˆ—ï¼š{', '.join(required_columns)}")
                return

            st.success(f"âœ… æˆåŠŸåŠ è½½ {len(df)} æ¡æµ‹è¯•æ•°æ®ç”¨äºéšæœºæœç´¢")
            st.markdown("**æ•°æ®é¢„è§ˆï¼š**")
            st.dataframe(df.head(), use_container_width=True)

            st.session_state.random_search_custom_test_data_translation = df.to_dict('records')

        except Exception as e:
            st.error(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{str(e)}")


def _render_random_search_manual_input_translation():
    """æ¸²æŸ“ç¿»è¯‘ä»»åŠ¡éšæœºæœç´¢æ‰‹åŠ¨è¾“å…¥ç•Œé¢"""
    st.markdown("**âœï¸ éšæœºæœç´¢æ‰‹åŠ¨è¾“å…¥æµ‹è¯•æ•°æ®ï¼ˆç¿»è¯‘ä»»åŠ¡ï¼‰**")

    manual_data = st.session_state.get('random_search_manual_test_data_translation', [
        {"text": "", "expected": ""},
        {"text": "", "expected": ""},
        {"text": "", "expected": ""}
    ])

    st.markdown("æ·»åŠ ç”¨äºéšæœºæœç´¢çš„æµ‹è¯•æ ·æœ¬ï¼š")

    updated_data = []
    for i, item in enumerate(manual_data):
        col1, col2, col3 = st.columns([4, 4, 1])
        with col1:
            text = st.text_area(
                f"åŸæ–‡ {i+1}",
                value=item["text"],
                key=f"random_search_trans_manual_text_{i}",
                height=100,
                placeholder="è¾“å…¥å¾…ç¿»è¯‘åŸæ–‡"
            )
        with col2:
            expected = st.text_area(
                f"å‚è€ƒè¯‘æ–‡ {i+1}",
                value=item["expected"],
                key=f"random_search_trans_manual_expected_{i}",
                height=100,
                placeholder="è¾“å…¥å‚è€ƒè¯‘æ–‡"
            )
        with col3:
            if st.button("ğŸ—‘ï¸", key=f"random_search_trans_delete_{i}", help=f"åˆ é™¤ç¬¬{i+1}è¡Œ"):
                continue

        if text.strip() or expected.strip():
            updated_data.append({"text": text, "expected": expected})

    if st.button("â• æ·»åŠ ä¸€è¡Œ", key="random_search_trans_add_manual_row"):
        updated_data.append({"text": "", "expected": ""})

    st.session_state.random_search_manual_test_data_translation = updated_data

    valid_count = sum(1 for item in updated_data if item["text"].strip() and item["expected"].strip())
    st.info(f"å½“å‰æœ‰ {valid_count} æ¡æœ‰æ•ˆæµ‹è¯•æ•°æ®ç”¨äºéšæœºæœç´¢")


def _get_random_search_config(task_type_label: str):
    """æ ¹æ®ä»»åŠ¡ç±»å‹è¿”å›éšæœºæœç´¢é…ç½®"""
    if task_type_label == "åˆ†ç±»ä»»åŠ¡":
        # è·å–ç”¨æˆ·è¾“å…¥çš„æ ‡ç­¾å’Œä»»åŠ¡æè¿°
        user_labels = st.session_state.get('user_labels', ["ç§¯æ", "æ¶ˆæ", "ä¸­ç«‹"])
        user_task_desc = st.session_state.get('user_task_description', get_default_value("classification", "task_description"))
        
        # åŠ¨æ€ç”Ÿæˆä»»åŠ¡æè¿°
        labels_str = ", ".join(user_labels)
        task_desc = f"{user_task_desc}ï¼Œåˆ¤æ–­ä¸º{labels_str}"
        
        # è·å–æµ‹è¯•æ•°æ®é›†ï¼ˆä½¿ç”¨éšæœºæœç´¢ä¸“ç”¨æ•°æ®é›†è·å–å‡½æ•°ï¼‰
        test_dataset = _get_random_search_test_dataset("classification")
        
        return (task_desc, "classification", test_dataset, {"labels": user_labels})
    if task_type_label == "æ‘˜è¦ä»»åŠ¡":
        # è·å–ç”¨æˆ·è¾“å…¥çš„ä»»åŠ¡æè¿°
        user_task_desc = st.session_state.get('user_task_description_summarization', get_default_value("summarization", "task_description"))
        
        # è·å–å…¶ä»–æ‘˜è¦é…ç½®
        source_type = st.session_state.get(
            'summarization_source_type',
            get_default_value("summarization", "source_type")
        )
        target_audience = st.session_state.get(
            'summarization_target_audience',
            get_default_value("summarization", "target_audience")
        )
        focus_points = st.session_state.get(
            'summarization_focus_points',
            get_default_value("summarization", "focus_points")
        )
        
        # åŠ¨æ€ç”Ÿæˆä»»åŠ¡æè¿°
        task_desc = user_task_desc
        
        test_dataset = _get_random_search_test_dataset("summarization")

        return (task_desc, "summarization", test_dataset, {
            "source_type": source_type,
            "target_audience": target_audience,
            "focus_points": focus_points
        })
    if task_type_label == "ç¿»è¯‘ä»»åŠ¡":
        # è·å–ç”¨æˆ·è¾“å…¥çš„ä»»åŠ¡æè¿°
        user_task_desc = st.session_state.get('user_task_description_translation', get_default_value("translation", "task_description"))
        
        # è·å–å…¶ä»–ç¿»è¯‘é…ç½®
        source_lang = st.session_state.get('translation_source_lang', 'è‹±æ–‡')
        target_lang = st.session_state.get('translation_target_lang', 'ä¸­æ–‡')
        domain = st.session_state.get(
            'translation_domain',
            get_default_value("translation", "domain")
        )
        tone = st.session_state.get(
            'translation_tone',
            get_default_value("translation", "tone")
        )
        
        # åŠ¨æ€ç”Ÿæˆä»»åŠ¡æè¿°
        task_desc = user_task_desc
        
        test_dataset = _get_random_search_test_dataset("translation")

        return (task_desc, "translation", test_dataset, {
            "source_lang": source_lang,
            "target_lang": target_lang,
            "domain": domain,
            "tone": tone
        })
    return None


