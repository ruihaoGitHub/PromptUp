"""æµ‹è¯• LLM è¿”å›æ ¼å¼"""
import os
os.environ['NVIDIA_API_KEY'] = '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'

from services import LLMService

llm = LLMService.create_llm(
    provider='nvidia',
    api_key='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',
    model='meta/llama-3.1-8b-instruct'
)

prompt = """è¯·ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½• Markdown æ ‡è®°ã€‚

è¦æ±‚ï¼šè¿”å›ä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- thinking_process: æ€è€ƒè¿‡ç¨‹
- improved_prompt: æ”¹è¿›åçš„æç¤ºè¯

ç¤ºä¾‹æ ¼å¼ï¼š
{
  "thinking_process": "åˆ†æ...",
  "improved_prompt": "è¯·..."
}

ç°åœ¨è¯·ä¼˜åŒ–è¿™ä¸ªæç¤ºè¯: å†™ä¸€ä¸ªå‹å¥½çš„é—®å€™è¯­
"""

print("ğŸ“¤ å‘é€è¯·æ±‚...")
response = llm.invoke(prompt)

print("\n" + "="*60)
print("ğŸ“¥ åŸå§‹å“åº”å†…å®¹")
print("="*60)
print(response.content)
print("\n" + "="*60)
print(f"ğŸ“ å“åº”é•¿åº¦: {len(response.content)} å­—ç¬¦")
print("="*60)
