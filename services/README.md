# Services æœåŠ¡å±‚æ¨¡å—

æœåŠ¡å±‚æ¨¡å—æä¾›æ ¸å¿ƒä¸šåŠ¡æœåŠ¡ï¼ŒåŒ…æ‹¬ LLM ç®¡ç†ã€å“åº”è§£æç­‰å¯å¤ç”¨çš„ä¸šåŠ¡é€»è¾‘ã€‚

## ğŸ“ æ¨¡å—ç»“æ„

```
services/
â”œâ”€â”€ __init__.py          # æ¨¡å—å¯¼å‡º
â”œâ”€â”€ llm_service.py       # LLM åˆå§‹åŒ–å’Œç®¡ç†æœåŠ¡
â”œâ”€â”€ response_parser.py   # å“åº”è§£æå’Œæ¸…ç†æœåŠ¡
â””â”€â”€ README.md            # æœ¬æ–‡æ¡£
```

## ğŸ¤– llm_service.py - LLM ç®¡ç†æœåŠ¡

### åŠŸèƒ½
- ç»Ÿä¸€çš„ LLM åˆå§‹åŒ–æ¥å£
- æ”¯æŒå¤šä¸ª API æä¾›å•†ï¼ˆOpenAIã€NVIDIAï¼‰
- è‡ªåŠ¨é…ç½® API Key å’Œå‚æ•°
- æä¾›å•†èƒ½åŠ›æ£€æµ‹ï¼ˆå¦‚ JSON mode æ”¯æŒï¼‰

### æ ¸å¿ƒç±»ï¼šLLMService

#### é™æ€æ–¹æ³•

**create_llm()**
```python
LLMService.create_llm(
    provider="nvidia",           # API æä¾›å•†
    api_key="your-api-key",      # API Key
    model="meta/llama-3.1-405b-instruct",
    base_url=None,               # å¯é€‰çš„ base URL
    temperature=0.7,
    top_p=0.7,
    max_tokens=2048
)
```
åˆ›å»ºå¹¶é…ç½® LLM å®ä¾‹ï¼Œè‡ªåŠ¨æ ¹æ® provider é€‰æ‹© ChatOpenAI æˆ– ChatNVIDIAã€‚

**supports_json_mode()**
```python
LLMService.supports_json_mode("openai")  # True
LLMService.supports_json_mode("nvidia")  # False
```
æ£€æŸ¥æŒ‡å®šæä¾›å•†æ˜¯å¦æ”¯æŒ JSON modeã€‚

### ä½¿ç”¨ç¤ºä¾‹

```python
from services import LLMService

# åˆ›å»º NVIDIA LLM
llm = LLMService.create_llm(
    provider="nvidia",
    api_key="nvapi-xxx",
    model="qwen/qwen3-235b-a22b"
)

# åˆ›å»º OpenAI LLM
llm = LLMService.create_llm(
    provider="openai",
    api_key="sk-xxx",
    model="gpt-4o"
)

# æ£€æŸ¥æ˜¯å¦æ”¯æŒ JSON mode
if LLMService.supports_json_mode("openai"):
    response = llm.invoke(
        messages,
        response_format={"type": "json_object"}
    )
```

## ğŸ“ response_parser.py - å“åº”è§£ææœåŠ¡

### åŠŸèƒ½
- ä»å“åº”ä¸­æå– JSONï¼ˆæ”¯æŒ Markdown ä»£ç å—ï¼‰
- è§£æ JSON å­—ç¬¦ä¸²ä¸ºå­—å…¸
- æ¸…ç† Prompt å­—æ®µï¼ˆç§»é™¤é”™è¯¯çš„ JSON åŒ…è£¹ï¼‰
- å‹å¥½çš„é”™è¯¯å¤„ç†å’Œæç¤º

### æ ¸å¿ƒç±»ï¼šResponseParser

#### é™æ€æ–¹æ³•

**extract_json_from_response()**
```python
ResponseParser.extract_json_from_response(content)
```
ä» LLM å“åº”ä¸­æå– JSONï¼Œæ”¯æŒï¼š
- çº¯ JSON æ–‡æœ¬
- Markdown JSON ä»£ç å— (\`\`\`json ... \`\`\`)
- æ™®é€šä»£ç å— (\`\`\` ... \`\`\`)

**parse_json()**
```python
ResponseParser.parse_json(json_string)
```
è§£æ JSON å­—ç¬¦ä¸²ä¸ºå­—å…¸ï¼Œå†…éƒ¨ä½¿ç”¨ utils çš„å®‰å…¨è§£æå‡½æ•°ã€‚

**clean_prompt_field()**
```python
cleaned_text, was_cleaned = ResponseParser.clean_prompt_field(prompt_text)
```
æ¸…ç† Prompt å­—æ®µï¼Œè¿”å›æ¸…ç†åçš„æ–‡æœ¬å’Œæ˜¯å¦è¿›è¡Œäº†æ¸…ç†çš„æ ‡å¿—ã€‚

**parse_optimization_response()**
```python
result_dict = ResponseParser.parse_optimization_response(response_content)
```
å®Œæ•´çš„å“åº”è§£ææµç¨‹ï¼šæå– JSON â†’ è§£æä¸ºå­—å…¸ã€‚

**handle_parsing_error()**
```python
error_message = ResponseParser.handle_parsing_error(error, response_content)
```
ç”Ÿæˆå‹å¥½çš„é”™è¯¯æ¶ˆæ¯ï¼ŒåŒ…å«è°ƒè¯•ä¿¡æ¯å’Œå»ºè®®ã€‚

### ä½¿ç”¨ç¤ºä¾‹

```python
from services import ResponseParser

# è§£æ LLM å“åº”
try:
    # å®Œæ•´æµç¨‹
    result_dict = ResponseParser.parse_optimization_response(response.content)
    
    # æ¸…ç† Prompt å­—æ®µ
    cleaned, was_cleaned = ResponseParser.clean_prompt_field(
        result_dict["improved_prompt"]
    )
    
except Exception as e:
    # ç”Ÿæˆå‹å¥½çš„é”™è¯¯æ¶ˆæ¯
    error_msg = ResponseParser.handle_parsing_error(e, response.content)
    print(error_msg)
```

## ğŸ“Š ä¼˜åŒ–æ•ˆæœ

### optimizer.py é‡æ„æ•ˆæœ
- **é‡æ„å‰**ï¼š509 è¡Œ
- **é‡æ„å**ï¼š456 è¡Œ
- **å‡å°‘**ï¼š53 è¡Œ (-10.4%)

### æå–çš„æœåŠ¡å±‚ä»£ç 
- **llm_service.py**ï¼š127 è¡Œ
- **response_parser.py**ï¼š147 è¡Œ
- **__init__.py**ï¼š8 è¡Œ
- **æ€»è®¡**ï¼š282 è¡Œ

### ä»£ç è´¨é‡æå‡
- âœ… **è§£è€¦**ï¼šLLM åˆå§‹åŒ–é€»è¾‘ä» optimizer.py æå–
- âœ… **å¤ç”¨**ï¼šæœåŠ¡å¯åœ¨å…¶ä»–æ¨¡å—ä¸­ä½¿ç”¨
- âœ… **æµ‹è¯•**ï¼šæœåŠ¡å¯ç‹¬ç«‹æµ‹è¯•
- âœ… **ç»´æŠ¤**ï¼šå•ä¸€èŒè´£ï¼Œæ˜“äºä¿®æ”¹å’Œæ‰©å±•

## ğŸ¯ è®¾è®¡åŸåˆ™

### å•ä¸€èŒè´£åŸåˆ™
- `LLMService` åªè´Ÿè´£ LLM çš„åˆ›å»ºå’Œé…ç½®
- `ResponseParser` åªè´Ÿè´£å“åº”çš„è§£æå’Œæ¸…ç†

### ä¾èµ–å€’ç½®åŸåˆ™
- optimizer.py ä¾èµ–æœåŠ¡å±‚æ¥å£ï¼Œè€Œä¸æ˜¯å…·ä½“å®ç°
- æœåŠ¡å±‚ä¸ä¾èµ– optimizer.pyï¼Œå¯ç‹¬ç«‹ä½¿ç”¨

### å¼€é—­åŸåˆ™
- æ·»åŠ æ–°çš„ LLM æä¾›å•†ï¼šæ‰©å±• LLMService
- æ·»åŠ æ–°çš„è§£æç­–ç•¥ï¼šæ‰©å±• ResponseParser
- ä¸éœ€è¦ä¿®æ”¹ç°æœ‰ä»£ç 

## ğŸ”§ æ‰©å±•æŒ‡å—

### æ·»åŠ æ–°çš„ LLM æä¾›å•†

åœ¨ `llm_service.py` ä¸­æ·»åŠ æ–°çš„ç§æœ‰æ–¹æ³•ï¼š

```python
@staticmethod
def _create_anthropic_llm(api_key, model, ...):
    """åˆ›å»º Anthropic LLM å®ä¾‹"""
    # å®ç°é€»è¾‘
    pass
```

ç„¶ååœ¨ `create_llm()` ä¸­æ·»åŠ åˆ†æ”¯ï¼š

```python
elif provider == "anthropic":
    return LLMService._create_anthropic_llm(...)
```

### æ·»åŠ æ–°çš„è§£æç­–ç•¥

åœ¨ `response_parser.py` ä¸­æ·»åŠ æ–°çš„é™æ€æ–¹æ³•ï¼š

```python
@staticmethod
def parse_xml_response(content: str) -> Dict:
    """è§£æ XML æ ¼å¼çš„å“åº”"""
    # å®ç°é€»è¾‘
    pass
```

## ğŸ’¡ æœ€ä½³å®è·µ

1. **ä½¿ç”¨æœåŠ¡å±‚è€Œä¸æ˜¯ç›´æ¥å®ä¾‹åŒ–**
   ```python
   # å¥½çš„åšæ³•
   llm = LLMService.create_llm(provider="nvidia", ...)
   
   # ä¸æ¨è
   llm = ChatNVIDIA(...)  # ç›´æ¥å®ä¾‹åŒ–
   ```

2. **ç»Ÿä¸€çš„é”™è¯¯å¤„ç†**
   ```python
   try:
       result = ResponseParser.parse_optimization_response(content)
   except Exception as e:
       error_msg = ResponseParser.handle_parsing_error(e, content)
       # å¤„ç†é”™è¯¯
   ```

3. **æ£€æŸ¥èƒ½åŠ›å†ä½¿ç”¨**
   ```python
   if LLMService.supports_json_mode(provider):
       # ä½¿ç”¨ JSON mode
   else:
       # ä½¿ç”¨æ ‡å‡†è°ƒç”¨
   ```

## ğŸš€ æœªæ¥æ‰©å±•

### è®¡åˆ’æ·»åŠ çš„æœåŠ¡
- **CacheService**ï¼šLLM å“åº”ç¼“å­˜
- **RateLimitService**ï¼šAPI è°ƒç”¨é¢‘ç‡æ§åˆ¶
- **ValidationService**ï¼šè¾“å…¥è¾“å‡ºéªŒè¯
- **MetricsService**ï¼šæ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡

### å¯èƒ½çš„ä¼˜åŒ–
- å¼‚æ­¥ LLM è°ƒç”¨æ”¯æŒ
- æ‰¹é‡è¯·æ±‚å¤„ç†
- è¯·æ±‚é‡è¯•å’Œé™çº§ç­–ç•¥
- å¤šæä¾›å•†è´Ÿè½½å‡è¡¡
